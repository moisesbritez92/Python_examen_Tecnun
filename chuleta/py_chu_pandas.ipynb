{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7028460a",
   "metadata": {},
   "source": [
    "PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68e721",
   "metadata": {},
   "source": [
    "## 1. Lectura y Exploración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca75975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Lectura CSV\n",
    "df = pd.read_csv('archivo.csv')\n",
    "\n",
    "# Info rapida\n",
    "df.shape                    # (filas, columnas)\n",
    "df.info()                   # tipos y NAs por columna\n",
    "df.describe()               # estadisticas numericas\n",
    "df.head(10)                 # primeras n filas\n",
    "df.tail()                   # ultimas filas\n",
    "df.columns                  # nombres de columnas\n",
    "df.dtypes                   # tipos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146f6c7",
   "metadata": {},
   "source": [
    "## 2. Limpieza de NAs - Por Columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb552052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar NAs por columna\n",
    "df.isna().sum()                          # cuenta NAs por columna\n",
    "df.isnull().sum()                        # alias de isna()\n",
    "\n",
    "# Porcentaje de NAs por columna (TIPICO EN EXAMENES)\n",
    "nrow = len(df)\n",
    "pct_nas = (df.isna().sum() / nrow) * 100\n",
    "print(pct_nas)\n",
    "\n",
    "# Eliminar columnas con >= 25% de NAs (PATRON EXAMEN 2024)\n",
    "threshold = 25\n",
    "columnas_validas = pct_nas[pct_nas < threshold].index\n",
    "df_clean = df[columnas_validas]\n",
    "\n",
    "# Alternativa: eliminar columnas con cualquier NA\n",
    "df_no_na_cols = df.dropna(axis=1)        # axis=1 -> columnas\n",
    "\n",
    "# Eliminar columna especifica\n",
    "df.drop('columna_name', axis=1, inplace=True)\n",
    "df.drop(['col1', 'col2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c585d",
   "metadata": {},
   "source": [
    "## 3. Limpieza de NAs - Por Fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con cualquier NA (PATRON EXAMENES)\n",
    "df_clean = df.dropna(axis=0)             # axis=0 -> filas\n",
    "df.dropna(inplace=True)                  # modifica el df original\n",
    "\n",
    "# Eliminar filas donde TODAS sean NA\n",
    "df_clean = df.dropna(how='all')\n",
    "\n",
    "# Eliminar filas con NA en columnas especificas\n",
    "df_clean = df.dropna(subset=['col1', 'col2'])\n",
    "\n",
    "# IMPORTANTE: reset index despues de dropna\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)  # drop=True: no guardar index viejo\n",
    "\n",
    "# Contar NAs por fila\n",
    "df.isna().sum(axis=1)                    # axis=1 -> suma por fila"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287df6d",
   "metadata": {},
   "source": [
    "## 2b. TRAMPA: NAs como Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUIDADO: valores como \"N/A\", \"NA\", \"nan\", \"None\" como STRINGS\n",
    "# No los detecta df.isna() porque son strings validos!\n",
    "\n",
    "# 1) Al leer CSV, especificar que valores son NA\n",
    "df = pd.read_csv('archivo.csv', na_values=['N/A', 'NA', 'nan', 'None', '', ' '])\n",
    "\n",
    "# 2) Si ya leiste el CSV, reemplazar strings por NaN real\n",
    "import numpy as np\n",
    "df.replace(['N/A', 'NA', 'nan', 'None', '', ' '], np.nan, inplace=True)\n",
    "\n",
    "# 3) Despues de reemplazar, AHORA si usar dropna()\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 4) Detectar estos valores string (antes de convertir)\n",
    "# Ver si hay valores sospechosos\n",
    "df['columna'].value_counts()  # revisar si aparece 'N/A', 'NA', etc.\n",
    "\n",
    "# 5) Eliminar filas con valores especificos\n",
    "valores_invalidos = ['N/A', 'NA', 'nan', 'None', '', 'Unkown', 'Unknown']\n",
    "df_clean = df[~df['columna'].isin(valores_invalidos)]\n",
    "\n",
    "# 6) Para TODAS las columnas tipo object (string)\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col].replace(['N/A', 'NA', 'nan', 'None', ''], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741b2a3",
   "metadata": {},
   "source": [
    "## 2c. Workflow Completo de Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKFLOW TIPICO DE EXAMEN (orden recomendado)\n",
    "\n",
    "# 1. Leer CSV con na_values\n",
    "df = pd.read_csv('archivo.csv', na_values=['N/A', 'NA', 'nan', 'None', ''])\n",
    "\n",
    "# 2. Seleccionar columnas de interes\n",
    "cols = ['product', 'issue', 'company', 'state', 'timely_response']\n",
    "df = df[cols]\n",
    "\n",
    "# 3. Convertir strings NA residuales a NaN real\n",
    "df.replace(['N/A', 'NA', 'Unknown', 'Unkown'], np.nan, inplace=True)\n",
    "\n",
    "# 4. Eliminar columnas con muchos NAs (>= 25%)\n",
    "pct_nas = (df.isna().sum() / len(df)) * 100\n",
    "cols_validas = pct_nas[pct_nas < 25].index\n",
    "df = df[cols_validas]\n",
    "\n",
    "# 5. Eliminar filas con NAs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 6. Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 7. Convertir booleanos a 0/1\n",
    "df.replace({True: 1, False: 0}, inplace=True)\n",
    "\n",
    "# 8. Verificar resultado\n",
    "print(f'Shape final: {df.shape}')\n",
    "print(f'NAs totales: {df.isna().sum().sum()}')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be356d5",
   "metadata": {},
   "source": [
    "## 4. Seleccion de Columnas por Tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo columnas numericas (PATRON EXAMEN 2024)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df_numeric = df[numeric_cols]\n",
    "\n",
    "# Alternativa con tipos especificos\n",
    "df_numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Seleccionar solo columnas de texto\n",
    "df_text = df.select_dtypes(include=['object'])\n",
    "\n",
    "# Excluir tipos especificos\n",
    "df_no_numeric = df.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Seleccion manual de columnas (PATRON EXAMEN 2023)\n",
    "cols_deseadas = ['product', 'issue', 'company', 'state']\n",
    "df_subset = df[cols_deseadas]\n",
    "\n",
    "# Lista de columnas\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1221cf",
   "metadata": {},
   "source": [
    "## 5. Reemplazo de Valores y Booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab767ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir booleanos a 0/1 (PATRON TODOS LOS EXAMENES)\n",
    "df.replace({True: 1, False: 0}, inplace=True)\n",
    "\n",
    "# Reemplazar valores especificos\n",
    "df['columna'].replace('valor_viejo', 'valor_nuevo', inplace=True)\n",
    "\n",
    "# Reemplazar multiples valores\n",
    "df.replace({'valor1': 'nuevo1', 'valor2': 'nuevo2'}, inplace=True)\n",
    "\n",
    "# Reemplazar por diccionario de columnas\n",
    "df.replace({\n",
    "    'col1': {'A': 1, 'B': 2},\n",
    "    'col2': {'X': 10, 'Y': 20}\n",
    "}, inplace=True)\n",
    "\n",
    "# Rellenar NAs\n",
    "df.fillna(0, inplace=True)               # con valor constante\n",
    "df['col'].fillna(df['col'].mean(), inplace=True)  # con media\n",
    "df.fillna(method='ffill')                # forward fill\n",
    "df.fillna(method='bfill')                # backward fill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aef40",
   "metadata": {},
   "source": [
    "## 6. Filtrado de Filas - Condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68428349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro simple\n",
    "df_filtrado = df[df['columna'] > 100]\n",
    "\n",
    "# Eliminar filas con valor especifico (PATRON EXAMEN 2022)\n",
    "df_clean = df[df['Total_Gross'] != 'Gross Unkown']\n",
    "\n",
    "# Multiples condiciones con & (AND) y | (OR)\n",
    "df_filtrado = df[(df['Year'] >= 1980) & (df['Year'] <= 2019)]\n",
    "df_filtrado = df[(df['col'] == 'A') | (df['col'] == 'B')]\n",
    "\n",
    "# Usar isin para filtrar por lista (PATRON TODOS LOS EXAMENES)\n",
    "valores_validos = ['Action', 'Drama', 'Comedy']\n",
    "df_filtrado = df[df['main_genre'].isin(valores_validos)]\n",
    "\n",
    "# Negacion con ~\n",
    "df_excluidos = df[~df['genre'].isin(['Horror', 'Thriller'])]\n",
    "\n",
    "# Reset index despues de filtrar\n",
    "df_filtrado.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1767c",
   "metadata": {},
   "source": [
    "## 7. Regex y Limpieza de Strings (re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Eliminar caracteres especificos con re.sub (PATRON EXAMEN 2022)\n",
    "# Formato: re.sub(patron, reemplazo, string)\n",
    "df['clean_col'] = df['col'].apply(lambda x: re.sub(r'\\$', '', x))\n",
    "df['clean_col'] = df['clean_col'].apply(lambda x: re.sub('M', '', x))\n",
    "\n",
    "# Ejemplo completo: '$125.5M' -> 125.5\n",
    "df['New_total_gross'] = (df['Total_Gross']\n",
    "                         .apply(lambda x: re.sub(r'\\$', '', x))\n",
    "                         .apply(lambda x: re.sub('M', '', x))\n",
    "                         .apply(lambda x: float(x)))\n",
    "\n",
    "# Metodos de string basicos\n",
    "df['col'].str.lower()                    # minusculas\n",
    "df['col'].str.upper()                    # mayusculas\n",
    "df['col'].str.strip()                    # eliminar espacios extremos\n",
    "df['col'].str.replace('viejo', 'nuevo')  # reemplazar sin regex\n",
    "\n",
    "# Buscar si contiene patron\n",
    "df[df['col'].str.contains('patron', na=False)]\n",
    "\n",
    "# Split de strings (EXAMEN 2022 - actores)\n",
    "re.split(', ', 'Actor1, Actor2, Actor3')  # devuelve lista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46b856",
   "metadata": {},
   "source": [
    "## 8. Valores Unicos y Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores unicos\n",
    "df['columna'].unique()                   # array de valores unicos\n",
    "df['columna'].nunique()                  # numero de valores unicos\n",
    "df['columna'].value_counts()             # conteo de cada valor\n",
    "\n",
    "# Conteo con sort\n",
    "df['columna'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Duplicados\n",
    "df.duplicated()                          # bool por fila\n",
    "df.duplicated().sum()                    # cuenta duplicados\n",
    "df.drop_duplicates(inplace=True)         # elimina duplicados\n",
    "\n",
    "# Duplicados basados en columnas especificas\n",
    "df.drop_duplicates(subset=['col1', 'col2'], inplace=True)\n",
    "\n",
    "# Ver valores unicos de multiples columnas\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].nunique()} valores unicos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05e892",
   "metadata": {},
   "source": [
    "## 9. Crear y Transformar Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columna nueva directamente\n",
    "df['nueva_col'] = 1                      # valor constante\n",
    "df['suma'] = df['col1'] + df['col2']     # operacion aritmetica\n",
    "\n",
    "# Crear columna desde suma de otras (PATRON EXAMEN 01MIAR)\n",
    "df['Relatives'] = df['SibSp'] + df['Parch']\n",
    "\n",
    "# Con apply y lambda (PATRON EXAMEN 2022 - decadas)\n",
    "def get_decade(year):\n",
    "    return (year // 10) * 10\n",
    "\n",
    "df['decade'] = df['Year'].apply(get_decade)\n",
    "# Alternativa con lambda\n",
    "df['decade'] = df['Year'].apply(lambda x: (x // 10) * 10)\n",
    "\n",
    "# Apply con multiples columnas\n",
    "df['ratio'] = df.apply(lambda row: row['col1'] / row['col2'], axis=1)\n",
    "\n",
    "# Apply con if/else\n",
    "df['categoria'] = df['valor'].apply(lambda x: 'Alto' if x > 100 else 'Bajo')\n",
    "\n",
    "# Transformacion tipo de dato\n",
    "df['col_numerica'] = df['col_str'].astype(float)\n",
    "df['col_str'] = df['col_num'].astype(str)\n",
    "df['col_int'] = df['col_float'].astype(int)\n",
    "\n",
    "# Renombrar columnas\n",
    "df.rename(columns={'old_name': 'new_name'}, inplace=True)\n",
    "df.columns = ['col1', 'col2', 'col3']    # renombrar todas\n",
    "\n",
    "# Crear columna con enumerate (agregar ID)\n",
    "df['ID'] = list(range(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677eb493",
   "metadata": {},
   "source": [
    "## 10. GroupBy y Agregaciones (CLAVE EXAMENES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy basico\n",
    "df_grouped = df.groupby('columna')['valor'].sum()\n",
    "df_grouped = df.groupby('columna')['valor'].mean()\n",
    "\n",
    "# GroupBy con as_index=False (devuelve DataFrame, no Series)\n",
    "df_agg = df.groupby('columna', as_index=False)['valor'].count()\n",
    "\n",
    "# Multiples columnas de agrupacion (PATRON EXAMENES)\n",
    "df_agg = df.groupby(['company', 'issue'], as_index=False)['product'].count()\n",
    "\n",
    "# === FUNCIONES DE AGREGACION ===\n",
    "\n",
    "# count() - cuenta valores NO nulos\n",
    "df.groupby('col')['valor'].count()\n",
    "\n",
    "# size() - cuenta TODAS las filas (incluye NAs)\n",
    "df.groupby('col').size()\n",
    "\n",
    "# sum() - suma de valores\n",
    "df.groupby('col')['ventas'].sum()\n",
    "\n",
    "# mean() - promedio (ignora NAs)\n",
    "df.groupby('col')['rating'].mean()\n",
    "\n",
    "# median() - mediana\n",
    "df.groupby('col')['price'].median()\n",
    "\n",
    "# std() - desviacion estandar\n",
    "df.groupby('col')['score'].std()\n",
    "\n",
    "# var() - varianza\n",
    "df.groupby('col')['score'].var()\n",
    "\n",
    "# min() / max() - minimo y maximo\n",
    "df.groupby('col')['age'].min()\n",
    "df.groupby('col')['age'].max()\n",
    "\n",
    "# first() / last() - primer y ultimo valor\n",
    "df.groupby('col')['name'].first()\n",
    "df.groupby('col')['name'].last()\n",
    "\n",
    "# nunique() - cuenta valores unicos\n",
    "df.groupby('company')['product'].nunique()\n",
    "\n",
    "# quantile() - percentiles\n",
    "df.groupby('col')['price'].quantile(0.75)  # percentil 75\n",
    "\n",
    "# agg() con multiples funciones\n",
    "df.groupby('col')['valor'].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "# Named aggregation (MEJOR OPCION - EXAMEN 2023)\n",
    "df_result = df.groupby(['company', 'issue'], as_index=False).agg(\n",
    "    total=('product', 'size'),           # cuenta total de filas\n",
    "    timely_sum=('timely_response', 'sum'),  # suma de 1s y 0s\n",
    "    dispute_sum=('consumer_disputed', 'sum'),\n",
    "    avg_rating=('rating', 'mean'),       # promedio\n",
    "    max_price=('price', 'max'),          # maximo\n",
    "    unique_products=('product', 'nunique')  # conteo de unicos\n",
    ")\n",
    "\n",
    "# Derivar columnas con assign despues de agg\n",
    "df_result = df_result.assign(\n",
    "    untimely_pct=lambda d: (d.total - d.timely_sum) / d.total * 100,\n",
    "    disputed_pct=lambda d: d.dispute_sum / d.total * 100\n",
    ")\n",
    "\n",
    "# Agregacion con lambdas personalizadas\n",
    "df.groupby('col').agg(\n",
    "    rango=('valor', lambda x: x.max() - x.min()),\n",
    "    p90=('valor', lambda x: x.quantile(0.9))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dff740",
   "metadata": {},
   "source": [
    "## 10b. Agregaciones Avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebde939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes agregaciones por columna\n",
    "df.groupby('categoria').agg({\n",
    "    'ventas': ['sum', 'mean'],\n",
    "    'precio': ['min', 'max'],\n",
    "    'cantidad': 'count'\n",
    "})\n",
    "\n",
    "# Aplanar MultiIndex columns despues de agg multiple\n",
    "df_agg = df.groupby('col').agg({'val1': ['sum', 'mean'], 'val2': 'count'})\n",
    "df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "df_agg.reset_index(inplace=True)\n",
    "\n",
    "# Filtrar grupos con .filter()\n",
    "# Solo grupos donde la media > 100\n",
    "df_filtered = df.groupby('categoria').filter(lambda x: x['valor'].mean() > 100)\n",
    "\n",
    "# Transformar (mantiene el tamaño original, difunde valores)\n",
    "# Asignar la media del grupo a cada fila\n",
    "df['media_grupo'] = df.groupby('categoria')['valor'].transform('mean')\n",
    "\n",
    "# Diferencia respecto a la media del grupo\n",
    "df['diff_media'] = df['valor'] - df.groupby('categoria')['valor'].transform('mean')\n",
    "\n",
    "# Ranking dentro de cada grupo\n",
    "df['rank'] = df.groupby('categoria')['valor'].rank(ascending=False)\n",
    "\n",
    "# Acumulado por grupo\n",
    "df['cumsum'] = df.groupby('categoria')['valor'].cumsum()\n",
    "\n",
    "# apply() para operaciones complejas (retorna agregado o transformado)\n",
    "df.groupby('col').apply(lambda x: x.nlargest(3, 'valor'))  # top 3 por grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39979252",
   "metadata": {},
   "source": [
    "## 10c. Casos Practicos de Agregacion (Examenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASO 1: Top 5 empresas con mas quejas (EXAMEN 2023)\n",
    "top_companies = (df.groupby('company', as_index=False)['issue']\n",
    "                   .count()\n",
    "                   .sort_values('issue', ascending=False)\n",
    "                   .head(5))\n",
    "\n",
    "# CASO 2: Top 3 issues por volumen (EXAMEN 2023)\n",
    "top_issues = (df.groupby('issue', as_index=False)['product']\n",
    "                .count()\n",
    "                .sort_values('product', ascending=False)\n",
    "                .head(3))\n",
    "\n",
    "# CASO 3: Filtrar por top issues\n",
    "df_filtered = df[df['issue'].isin(top_issues['issue'])]\n",
    "\n",
    "# CASO 4: Metricas por empresa e issue (EXAMEN 2023)\n",
    "# Calcular % de respuestas no oportunas y % disputas\n",
    "result = (df.groupby(['company', 'issue'], as_index=False)\n",
    "            .agg(\n",
    "                total=('product', 'size'),\n",
    "                timely_yes=('timely_response', 'sum'),  # suma de 1s\n",
    "                disputed=('consumer_disputed', 'sum')\n",
    "            )\n",
    "            .assign(\n",
    "                untimely_pct=lambda d: (1 - d.timely_yes/d.total) * 100,\n",
    "                disputed_pct=lambda d: d.disputed / d.total * 100\n",
    "            )\n",
    "            .drop(columns=['timely_yes', 'disputed']))\n",
    "\n",
    "# CASO 5: Contar peliculas por decada y genero (EXAMEN 2022)\n",
    "df['decade'] = (df['Year'] // 10) * 10\n",
    "counts = (df.groupby(['decade', 'main_genre'], as_index=False)\n",
    "            .size()\n",
    "            .rename(columns={'size': 'count'})\n",
    "            .sort_values(['decade', 'count'], ascending=False))\n",
    "\n",
    "# CASO 6: Top 3 generos por decada (EXAMEN 2022)\n",
    "top_by_decade = pd.DataFrame()\n",
    "for decade in df['decade'].unique():\n",
    "    top_3 = counts[counts['decade'] == decade].head(3)\n",
    "    top_by_decade = pd.concat([top_by_decade, top_3], ignore_index=True)\n",
    "\n",
    "# CASO 7: Generos con mas de 200 apariciones\n",
    "genre_counts = df.groupby('main_genre')['decade'].count()\n",
    "valid_genres = genre_counts[genre_counts > 200].index\n",
    "df_filtered = df[df['main_genre'].isin(valid_genres)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdc0ca",
   "metadata": {},
   "source": [
    "## 11. Sort y Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por una columna\n",
    "df_sorted = df.sort_values(by='columna', ascending=False)\n",
    "\n",
    "# Ordenar por multiples columnas (PATRON EXAMEN 2022)\n",
    "df_sorted = df.sort_values(by=['decade', 'count'], ascending=False)\n",
    "df_sorted = df.sort_values(by=['col1', 'col2'], ascending=[True, False])\n",
    "\n",
    "# Ordenar y reset index\n",
    "df.sort_values(by='col', ascending=False, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Top N valores (PATRON TODOS LOS EXAMENES)\n",
    "top_5 = df.sort_values(by='count', ascending=False).head(5)\n",
    "top_5 = df.sort_values(by='count', ascending=False)[:5]  # slice\n",
    "\n",
    "# Top N por grupo (EXAMEN 2023 - top 3 issues)\n",
    "top_issues = (df.groupby('issue', as_index=False)['product']\n",
    "              .count()\n",
    "              .sort_values('product', ascending=False)\n",
    "              .head(3))\n",
    "\n",
    "# nlargest / nsmallest (alternativa rapida)\n",
    "df.nlargest(5, 'columna')\n",
    "df.nsmallest(3, 'columna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef152d",
   "metadata": {},
   "source": [
    "## 12. Concatenar y Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeea86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar verticalmente (apilar filas - PATRON EXAMEN 2022)\n",
    "df_total = pd.DataFrame()\n",
    "for grupo in df['categoria'].unique():\n",
    "    df_subset = df[df['categoria'] == grupo].head(3)\n",
    "    df_total = pd.concat([df_total, df_subset], ignore_index=True)\n",
    "\n",
    "# Concatenar horizontalmente (columnas)\n",
    "df_combined = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Append (deprecado, usar concat)\n",
    "df_total = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Merge (join) - como SQL\n",
    "df_merged = pd.merge(df1, df2, on='key_column', how='inner')\n",
    "# how: 'inner', 'left', 'right', 'outer'\n",
    "\n",
    "# Merge con diferentes nombres de columna\n",
    "df_merged = pd.merge(df1, df2, left_on='col1', right_on='col2', how='left')\n",
    "\n",
    "# Merge por indice\n",
    "df_merged = pd.merge(df1, df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49fccb",
   "metadata": {},
   "source": [
    "## 13. Iteracion sobre Filas/Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre grupos (PATRON EXAMEN 2022 - por decada)\n",
    "for categoria, grupo_df in df.groupby('categoria'):\n",
    "    print(f'Procesando: {categoria}')\n",
    "    top_3 = grupo_df.nlargest(3, 'valor')\n",
    "    # hacer algo con top_3\n",
    "\n",
    "# Iterar sobre valores unicos\n",
    "for valor in df['columna'].unique():\n",
    "    df_subset = df[df['columna'] == valor]\n",
    "    # procesar df_subset\n",
    "\n",
    "# Iterar sobre filas (EVITAR si es posible, preferir apply)\n",
    "for index, row in df.iterrows():\n",
    "    print(row['columna1'], row['columna2'])\n",
    "\n",
    "# Iterar sobre columnas\n",
    "for col in df.columns:\n",
    "    print(f'{col}: tipo {df[col].dtype}')\n",
    "\n",
    "# List comprehension (mas eficiente)\n",
    "lista_valores = [valor * 2 for valor in df['columna']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa789176",
   "metadata": {},
   "source": [
    "## 14. Pivoting y Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table (formato ancho)\n",
    "pivot = df.pivot_table(\n",
    "    values='valor',           # columna a agregar\n",
    "    index='fila',             # columna para filas\n",
    "    columns='columna',        # columna para columnas\n",
    "    aggfunc='mean'            # funcion agregacion\n",
    ")\n",
    "\n",
    "# Crosstab (tabla de frecuencias)\n",
    "ct = pd.crosstab(df['col1'], df['col2'])\n",
    "\n",
    "# Melt (formato largo - opuesto a pivot)\n",
    "df_long = df.melt(\n",
    "    id_vars=['id', 'name'],   # columnas que no cambian\n",
    "    value_vars=['col1', 'col2'],  # columnas a transformar\n",
    "    var_name='variable',      # nombre para la nueva columna de variables\n",
    "    value_name='valor'        # nombre para la columna de valores\n",
    ")\n",
    "\n",
    "# Transpose\n",
    "df_T = df.T                   # intercambiar filas y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55cc81",
   "metadata": {},
   "source": [
    "## 15. Tips y Trucos Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39021bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar DataFrame (evitar referencias)\n",
    "df_copy = df.copy()           # copia profunda\n",
    "\n",
    "# Sampling\n",
    "df_sample = df.sample(n=100)  # 100 filas aleatorias\n",
    "df_sample = df.sample(frac=0.1)  # 10% de filas\n",
    "\n",
    "# Query (filtrado con strings)\n",
    "df_filtrado = df.query('Year >= 1980 and Year <= 2019')\n",
    "df_filtrado = df.query('main_genre in [\"Action\", \"Drama\"]')\n",
    "\n",
    "# Conditional assignment (np.where)\n",
    "df['categoria'] = np.where(df['valor'] > 100, 'Alto', 'Bajo')\n",
    "\n",
    "# Multiples condiciones con np.select\n",
    "conditions = [df['valor'] < 50, df['valor'] < 100, df['valor'] >= 100]\n",
    "choices = ['Bajo', 'Medio', 'Alto']\n",
    "df['categoria'] = np.select(conditions, choices, default='Otro')\n",
    "\n",
    "# Display settings (mostrar mas filas/columnas)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Memory usage\n",
    "df.memory_usage(deep=True).sum() / 1024**2  # MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fae46",
   "metadata": {},
   "source": [
    "## 16. Binning y Discretizacion (qcut, cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.qcut() - divide en grupos de IGUAL TAMAÑO (cuantiles)\n",
    "# PATRON EXAMEN 01MIAR - dividir por edad en grupos iguales\n",
    "df['AgeGroup'] = pd.qcut(df['Age'], q=5, labels=['E1','E2','E3','E4','E5'])\n",
    "\n",
    "# qcut con duplicates='drop' si hay valores repetidos\n",
    "df['AgeGroup'] = pd.qcut(df['Age'], q=5, labels=['E1','E2','E3','E4','E5'], \n",
    "                         duplicates='drop')\n",
    "\n",
    "# pd.cut() - divide por RANGOS especificos (intervalos fijos)\n",
    "bins = [0, 18, 30, 50, 100]\n",
    "labels = ['Menor', 'Joven', 'Adulto', 'Mayor']\n",
    "df['AgeCategory'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "\n",
    "# cut con right=False (incluye limite izquierdo, excluye derecho)\n",
    "df['Score_Cat'] = pd.cut(df['score'], bins=[0, 50, 75, 100], \n",
    "                         labels=['Bajo', 'Medio', 'Alto'], right=False)\n",
    "\n",
    "# Ejemplo completo: qcut para percentiles\n",
    "df['Quartile'] = pd.qcut(df['price'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Ver rangos creados\n",
    "df.groupby('AgeGroup')['Age'].agg(['min', 'max', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69051e",
   "metadata": {},
   "source": [
    "## 17. Correlaciones (EXAMEN 01MIAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlacion completa\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "# Correlacion entre dos columnas especificas\n",
    "corr_value = df['col1'].corr(df['col2'])\n",
    "\n",
    "# Correlaciones con una columna target\n",
    "correlations = df.corr()['target_col']\n",
    "print(correlations)\n",
    "\n",
    "# Verificar correlaciones > threshold (PATRON EXAMEN 01MIAR)\n",
    "def verify_corr(df, target):\n",
    "    corr_matrix = df.corr()\n",
    "    correlations = corr_matrix[target]\n",
    "    \n",
    "    result = {}\n",
    "    for col in df.columns:\n",
    "        if col != target:\n",
    "            # True si |correlacion| > 0.3\n",
    "            result[col] = abs(correlations[col]) > 0.3\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Uso\n",
    "corr_dict = verify_corr(df, 'Fare')\n",
    "print(corr_dict)  # {'Pclass': True, 'Age': False, 'Relatives': False}\n",
    "\n",
    "# Correlaciones solo de columnas numericas\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Encontrar correlaciones altas (excluyendo diagonal)\n",
    "# Correlaciones > 0.7 (positivas o negativas)\n",
    "high_corr = corr_matrix[(corr_matrix > 0.7) | (corr_matrix < -0.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017a866",
   "metadata": {},
   "source": [
    "## 18. Generadores con Pandas (EXAMEN 01MIAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de generadores para leer CSV (EXAMEN 01MIAR)\n",
    "file_name = \"archivo.csv\"\n",
    "lines = (line for line in open(file_name))           # generador de lineas\n",
    "list_line = (s.rstrip().split(\"|\") for s in lines)  # generador de listas\n",
    "cols = next(list_line)                               # primera linea = columnas\n",
    "passengers = (dict(zip(cols, data)) for data in list_line)  # generador de dicts\n",
    "\n",
    "# Consumir generador\n",
    "for passenger in passengers:\n",
    "    print(passenger)\n",
    "\n",
    "# Convertir generador a DataFrame\n",
    "passengers_list = list(passengers)\n",
    "df = pd.DataFrame(passengers_list)\n",
    "\n",
    "# Generador que divide por valores unicos (EXAMEN 01MIAR)\n",
    "def divide_by_uniques(df, column):\n",
    "    unique_values = df[column].unique()\n",
    "    for value in unique_values:\n",
    "        df_filtered = df[df[column] == value]\n",
    "        yield df_filtered\n",
    "\n",
    "# Uso del generador\n",
    "gen = divide_by_uniques(df, 'Pclass')\n",
    "for df_class in gen:\n",
    "    print(f\"Procesando clase: {df_class['Pclass'].iloc[0]}\")\n",
    "    print(f\"Filas: {len(df_class)}\")\n",
    "\n",
    "# Generador simple de DataFrames\n",
    "def generate_chunks(df, chunk_size=100):\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        yield df.iloc[start:start + chunk_size]\n",
    "\n",
    "# Uso\n",
    "for chunk in generate_chunks(df, 100):\n",
    "    # procesar cada chunk\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59313c1c",
   "metadata": {},
   "source": [
    "## 19. Guardar DataFrames por Grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame en CSV (PATRON EXAMEN 01MIAR)\n",
    "df.to_csv('output.csv', sep='|')                    # con separador |\n",
    "df.to_csv('output.csv', index=False)                # sin indice\n",
    "df.to_csv('output.csv', sep='|', index=False)\n",
    "\n",
    "# Dividir y guardar por grupos (EXAMEN 01MIAR - AgeGroups)\n",
    "groups = ['E1', 'E2', 'E3', 'E4', 'E5']\n",
    "df['AgeGroup'] = pd.qcut(df['Age'], q=len(groups), labels=groups, duplicates='drop')\n",
    "\n",
    "for group in groups:\n",
    "    df_group = df[df['AgeGroup'] == group]\n",
    "    df_group.to_csv(f\"{group}.csv\", sep='|')\n",
    "\n",
    "# Alternativa con groupby\n",
    "for name, group_df in df.groupby('AgeGroup'):\n",
    "    group_df.to_csv(f\"{name}.csv\", sep='|', index=False)\n",
    "\n",
    "# Guardar multiple formatos\n",
    "df.to_csv('data.csv')                               # CSV\n",
    "df.to_excel('data.xlsx', sheet_name='Sheet1')       # Excel\n",
    "df.to_json('data.json', orient='records')           # JSON\n",
    "df.to_parquet('data.parquet')                       # Parquet (mas eficiente)\n",
    "df.to_pickle('data.pkl')                            # Pickle (preserva tipos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcd588",
   "metadata": {},
   "source": [
    "## 20. Operaciones con Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f016bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index (IMPORTANTISIMO despues de filtros/dropna)\n",
    "df.reset_index(drop=True, inplace=True)  # drop=True: elimina index viejo\n",
    "df.reset_index(inplace=True)             # mantiene index viejo como columna\n",
    "\n",
    "# Set index (convertir columna en index)\n",
    "df.set_index('columna', inplace=True)\n",
    "df.set_index(['col1', 'col2'], inplace=True)  # multi-index\n",
    "\n",
    "# Acceder por index\n",
    "df.loc[0]                                # por label (index)\n",
    "df.iloc[0]                               # por posicion (0-based)\n",
    "df.loc[0:5]                              # slice por label (inclusivo)\n",
    "df.iloc[0:5]                             # slice por posicion (exclusivo final)\n",
    "\n",
    "# Acceder por index especifico (EXAMEN 01MIAR)\n",
    "df.iloc[id_max]                          # fila en posicion id_max\n",
    "df.iloc[id_max].Nombre                   # valor especifico\n",
    "\n",
    "# Argmax / Argmin - devuelve POSICION del max/min\n",
    "id_max = df['Estatura'].argmax()         # posicion del maximo\n",
    "id_min = df['Estatura'].argmin()         # posicion del minimo\n",
    "valor_max = df.iloc[id_max]['Estatura']\n",
    "\n",
    "# Reindexar\n",
    "df.index = range(len(df))                # nuevo index 0,1,2,...\n",
    "df.index = df.index + 1                  # empezar desde 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded495b",
   "metadata": {},
   "source": [
    "## 21. Lambda vs Funciones Clasicas (EXAMEN 01MIAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda en una linea (EXAMEN 01MIAR)\n",
    "import random\n",
    "oneline_function = lambda i: sum([random.randint(1,20) if n%2==0 else random.randint(-20,-1) for n in range(i)])\n",
    "\n",
    "# Convertir a funcion legible (EXAMEN 01MIAR)\n",
    "def readable_function(iterations):\n",
    "    list_a = []\n",
    "    for n in range(iterations):\n",
    "        if n % 2 == 0:\n",
    "            list_a.append(random.randint(1, 20))\n",
    "        else:\n",
    "            list_a.append(random.randint(-20, -1))\n",
    "    return sum(list_a)\n",
    "\n",
    "# Lambda con pandas apply\n",
    "df['categoria'] = df['valor'].apply(lambda x: 'Alto' if x > 100 else 'Bajo')\n",
    "\n",
    "# Equivalente con funcion\n",
    "def categorizar(valor):\n",
    "    if valor > 100:\n",
    "        return 'Alto'\n",
    "    else:\n",
    "        return 'Bajo'\n",
    "\n",
    "df['categoria'] = df['valor'].apply(categorizar)\n",
    "\n",
    "# List comprehension vs lambda\n",
    "# List comprehension (mas rapido)\n",
    "valores = [x * 2 for x in df['columna']]\n",
    "\n",
    "# Lambda con map\n",
    "valores = list(map(lambda x: x * 2, df['columna']))\n",
    "\n",
    "# Cuando usar cada uno:\n",
    "# - Lambda: operaciones simples en una linea\n",
    "# - Funcion: logica compleja, reutilizable, mas legible\n",
    "# - List comprehension: mejor rendimiento para listas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815937d",
   "metadata": {},
   "source": [
    "## 22. Patrones Comunes de Examenes - Workflow Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a336c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATRON 1: Leer con generador, calcular porcentajes por grupo ===\n",
    "# (EXAMEN 01MIAR - Ejercicio 03)\n",
    "def calc_percent_survived(passengers):\n",
    "    dict_list = list(passengers)                    # convertir generador\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df[\"Pclass\"] = df[\"Pclass\"].astype(int)\n",
    "    df[\"Survived\"] = df[\"Survived\"].astype(int)\n",
    "    \n",
    "    survived_by_class = df.groupby('Pclass')['Survived'].sum()\n",
    "    total_by_class = df.groupby('Pclass').size()\n",
    "    percent = (survived_by_class / total_by_class * 100).round().astype(int)\n",
    "    \n",
    "    return pd.DataFrame({'PClass': percent.index, 'PercentSurvived': percent.values})\n",
    "\n",
    "# === PATRON 2: Dividir en grupos iguales y guardar ===\n",
    "# (EXAMEN 01MIAR - Ejercicio 04)\n",
    "def split_dataframe(file_name, groups):\n",
    "    df = pd.read_csv(file_name, sep=\"|\")\n",
    "    df = df.dropna(subset=['Age'])\n",
    "    df = df.sort_values(by='Age').reset_index(drop=True)\n",
    "    df['AgeGroup'] = pd.qcut(df['Age'], q=len(groups), labels=groups, duplicates='drop')\n",
    "    \n",
    "    for group in groups:\n",
    "        df[df['AgeGroup'] == group].to_csv(f\"{group}.csv\", sep='|')\n",
    "\n",
    "# === PATRON 3: Crear columnas derivadas y verificar correlaciones ===\n",
    "# (EXAMEN 01MIAR - Ejercicio 05)\n",
    "def read_clean_df(file_name):\n",
    "    df = pd.read_csv(file_name, sep=\"|\")\n",
    "    df['Relatives'] = df['SibSp'] + df['Parch']\n",
    "    return df[['Pclass', 'Age', 'Relatives', 'Fare']]\n",
    "\n",
    "def verify_corr(df, target):\n",
    "    corr_matrix = df.corr()\n",
    "    return {col: abs(corr_matrix[target][col]) > 0.3 \n",
    "            for col in df.columns if col != target}\n",
    "\n",
    "def divide_by_uniques(df, column):\n",
    "    for value in df[column].unique():\n",
    "        yield df[df[column] == value]\n",
    "\n",
    "# === PATRON 4: Top N empresas, filtrar y metricas ===\n",
    "# (EXAMEN 2023)\n",
    "top_5 = df.groupby('company', as_index=False)['issue'].count().sort_values('issue', ascending=False).head(5)\n",
    "df_top = df[df['company'].isin(top_5['company'])]\n",
    "\n",
    "result = (df_top.groupby(['company', 'issue'], as_index=False)\n",
    "          .agg(total=('product', 'size'), timely=('timely_response', 'sum'))\n",
    "          .assign(untimely_pct=lambda d: (1 - d.timely/d.total) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
