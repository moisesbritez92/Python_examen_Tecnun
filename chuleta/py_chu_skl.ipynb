{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_0",
   "metadata": {},
   "source": [
    "SCIKIT-LEARN (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_1",
   "metadata": {},
   "source": [
    "## 1. Importaciones Basicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_3",
   "metadata": {},
   "source": [
    "## 2. Division de Datos - train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division basica 80-20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Division con estratificacion (mantiene proporcion de clases)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# Division en tres conjuntos: train, validation, test\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Parametros importantes:\n# test_size: proporcion del conjunto de prueba (0.2 = 20%)\n# random_state: semilla para reproducibilidad\n# stratify: mantiene la distribucion de clases en train y test\n# shuffle: mezcla los datos antes de dividir (True por defecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_5",
   "metadata": {},
   "source": [
    "## 3. Escalado y Normalizacion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler: media 0 y desviacion estandar 1\n# Formula: (x - media) / desviacion_estandar\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # ajusta y transforma train\nX_test_scaled = scaler.transform(X_test)        # solo transforma test\n\n# MinMaxScaler: escala entre 0 y 1\n# Formula: (x - min) / (max - min)\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# MinMaxScaler personalizado: escala en rango especifico\nscaler = MinMaxScaler(feature_range=(0, 10))  # escala entre 0 y 10\nX_train_scaled = scaler.fit_transform(X_train)\n\n# RobustScaler: robusto a outliers, usa mediana y rango intercuartil\n# Formula: (x - mediana) / IQR\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# IMPORTANTE: siempre fit solo en train, transform en train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_7",
   "metadata": {},
   "source": [
    "## 4. Codificacion de Variables Categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder: convierte categorias a numeros (0, 1, 2, ...)\n# Usar solo para variable objetivo (target) o variables ordinales\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\ny_original = le.inverse_transform(y_encoded)  # volver a categorias originales\nprint(le.classes_)  # ver las clases originales\n\n# OneHotEncoder: crea columnas binarias para cada categoria\n# Usar para variables nominales (sin orden) en features\nencoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' evita multicolinealidad\nX_encoded = encoder.fit_transform(X[['categoria1', 'categoria2']])\ncolumnas_nuevas = encoder.get_feature_names_out()  # nombres de nuevas columnas\n\n# OneHotEncoder manteniendo DataFrame\nencoder = OneHotEncoder(sparse_output=False)\nX_encoded = encoder.fit_transform(df[['columna_cat']])\ndf_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out())\n\n# OrdinalEncoder: convierte categorias a numeros respetando orden\n# Usar cuando las categorias tienen orden (bajo < medio < alto)\nencoder = OrdinalEncoder(categories=[['bajo', 'medio', 'alto']])\nX_encoded = encoder.fit_transform(X[['nivel']])\n\n# pd.get_dummies: alternativa de pandas para OneHotEncoding\ndf_encoded = pd.get_dummies(df, columns=['categoria'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_9",
   "metadata": {},
   "source": [
    "## 5. Modelos de Regresion - Predecir valores continuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n\n# Regresion Lineal simple\nmodelo = LinearRegression()\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\nprint(f'Coeficientes: {modelo.coef_}')\nprint(f'Intercepto: {modelo.intercept_}')\nprint(f'R2 Score: {modelo.score(X_test, y_test)}')\n\n# Ridge Regression: regularizacion L2, penaliza coeficientes grandes\n# Util cuando hay multicolinealidad\nmodelo = Ridge(alpha=1.0)  # alpha controla la regularizacion\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Lasso Regression: regularizacion L1, puede hacer coeficientes = 0\n# Util para seleccion de features\nmodelo = Lasso(alpha=1.0)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# ElasticNet: combina L1 y L2\nmodelo = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio controla mezcla L1/L2\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_11",
   "metadata": {},
   "source": [
    "## 6. Regresion Logistica - Clasificacion binaria y multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n\n# Clasificacion binaria\nmodelo = LogisticRegression(random_state=42)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\ny_pred_proba = modelo.predict_proba(X_test)  # probabilidades de cada clase\n\n# Clasificacion multiclase\n# solver: algoritmo de optimizacion\n# 'lbfgs': bueno para datasets peque\u00f1os\n# 'saga': bueno para datasets grandes\n# 'newton-cg': preciso pero lento\nmodelo = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Con regularizacion\nmodelo = LogisticRegression(penalty='l2', C=1.0, random_state=42)  # C es inverso de alpha\nmodelo.fit(X_train, y_train)\n\n# Parametros importantes:\n# C: inverso de la fuerza de regularizacion (menor C = mas regularizacion)\n# penalty: 'l1', 'l2', 'elasticnet', 'none'\n# max_iter: numero maximo de iteraciones (aumentar si no converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_13",
   "metadata": {},
   "source": [
    "## 7. Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\n# Clasificacion\nmodelo = DecisionTreeClassifier(\n    max_depth=5,              # profundidad maxima del arbol\n    min_samples_split=20,     # minimo de muestras para dividir un nodo\n    min_samples_leaf=10,      # minimo de muestras en hoja\n    random_state=42\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Feature importance: importancia de cada variable\nimportancias = modelo.feature_importances_\nfor i, imp in enumerate(importancias):\n    print(f'Feature {i}: {imp}')\n\n# Regresion\nmodelo = DecisionTreeRegressor(max_depth=5, random_state=42)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Visualizar el arbol\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,10))\nplot_tree(modelo, filled=True, feature_names=['feat1', 'feat2'])\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_15",
   "metadata": {},
   "source": [
    "## 8. Random Forest - Ensemble de arboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n# Clasificacion\nmodelo = RandomForestClassifier(\n    n_estimators=100,         # numero de arboles\n    max_depth=10,             # profundidad maxima de cada arbol\n    min_samples_split=20,\n    min_samples_leaf=10,\n    max_features='sqrt',      # numero de features aleatorias: 'sqrt', 'log2', int, float\n    random_state=42,\n    n_jobs=-1                 # usar todos los cores del CPU\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\ny_pred_proba = modelo.predict_proba(X_test)\n\n# Feature importance\nimportancias = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': modelo.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Regresion\nmodelo = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    random_state=42,\n    n_jobs=-1\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Out-of-bag score: estimacion de error sin validacion cruzada\nmodelo = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nmodelo.fit(X_train, y_train)\nprint(f'OOB Score: {modelo.oob_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_17",
   "metadata": {},
   "source": [
    "## 9. Gradient Boosting - Ensemble secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n\n# Clasificacion\nmodelo = GradientBoostingClassifier(\n    n_estimators=100,         # numero de arboles\n    learning_rate=0.1,        # tasa de aprendizaje (menor = mas conservador)\n    max_depth=3,              # profundidad maxima de cada arbol\n    min_samples_split=20,\n    min_samples_leaf=10,\n    subsample=0.8,            # proporcion de muestras para cada arbol\n    random_state=42\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\ny_pred_proba = modelo.predict_proba(X_test)\n\n# Regresion\nmodelo = GradientBoostingRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# Feature importance\nimportancias = modelo.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_19",
   "metadata": {},
   "source": [
    "## 10. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR\n\n# Clasificacion\nmodelo = SVC(\n    kernel='rbf',             # 'linear', 'poly', 'rbf', 'sigmoid'\n    C=1.0,                    # parametro de regularizacion\n    gamma='scale',            # coeficiente del kernel: 'scale', 'auto', float\n    random_state=42\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# SVC con probabilidades\nmodelo = SVC(kernel='rbf', probability=True, random_state=42)\nmodelo.fit(X_train, y_train)\ny_pred_proba = modelo.predict_proba(X_test)\n\n# Kernel lineal (mas rapido para datos linealmente separables)\nmodelo = SVC(kernel='linear', C=1.0, random_state=42)\nmodelo.fit(X_train, y_train)\n\n# Kernel polinomial\nmodelo = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\nmodelo.fit(X_train, y_train)\n\n# Regresion\nmodelo = SVR(kernel='rbf', C=1.0, epsilon=0.1)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# IMPORTANTE: SVM requiere datos escalados para funcionar bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_21",
   "metadata": {},
   "source": [
    "## 11. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\n# Clasificacion\nmodelo = KNeighborsClassifier(\n    n_neighbors=5,            # numero de vecinos\n    weights='uniform',        # 'uniform' o 'distance' (ponderacion por distancia)\n    metric='minkowski',       # metrica de distancia: 'euclidean', 'manhattan', 'minkowski'\n    p=2                       # p=1 Manhattan, p=2 Euclidean\n)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\ny_pred_proba = modelo.predict_proba(X_test)\n\n# KNN con ponderacion por distancia\nmodelo = KNeighborsClassifier(n_neighbors=5, weights='distance')\nmodelo.fit(X_train, y_train)\n\n# Regresion\nmodelo = KNeighborsRegressor(n_neighbors=5, weights='uniform')\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# IMPORTANTE: KNN requiere datos escalados para funcionar bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_23",
   "metadata": {},
   "source": [
    "## 12. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n\n# GaussianNB: para features continuas con distribucion normal\nmodelo = GaussianNB()\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\ny_pred_proba = modelo.predict_proba(X_test)\n\n# MultinomialNB: para features de conteo (texto, frecuencias)\n# Requiere valores no negativos\nmodelo = MultinomialNB(alpha=1.0)  # alpha: parametro de suavizado\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)\n\n# BernoulliNB: para features binarias (0/1)\nmodelo = BernoulliNB(alpha=1.0)\nmodelo.fit(X_train, y_train)\ny_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_25",
   "metadata": {},
   "source": [
    "## 13. Clustering - Aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\n# KMeans: particiona datos en K clusters\nmodelo = KMeans(\n    n_clusters=3,             # numero de clusters\n    init='k-means++',         # metodo de inicializacion\n    n_init=10,                # numero de veces que se ejecuta con diferentes centroides\n    max_iter=300,\n    random_state=42\n)\nclusters = modelo.fit_predict(X)\ncentroides = modelo.cluster_centers_\n\n# Evaluar calidad del clustering\ninercia = modelo.inertia_  # suma de distancias al cuadrado al centroide mas cercano\nsilhouette = silhouette_score(X, clusters)  # entre -1 y 1, mayor es mejor\n\n# Metodo del codo para encontrar K optimo\ninertias = []\nfor k in range(1, 11):\n    km = KMeans(n_clusters=k, random_state=42)\n    km.fit(X)\n    inertias.append(km.inertia_)\n\n# DBSCAN: clustering basado en densidad, no requiere especificar K\nmodelo = DBSCAN(\n    eps=0.5,                  # distancia maxima entre puntos del mismo cluster\n    min_samples=5             # minimo de puntos para formar un cluster\n)\nclusters = modelo.fit_predict(X)\n# cluster -1 son outliers\n\n# Hierarchical Clustering\nmodelo = AgglomerativeClustering(\n    n_clusters=3,             # numero de clusters\n    linkage='ward'            # 'ward', 'complete', 'average', 'single'\n)\nclusters = modelo.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_27",
   "metadata": {},
   "source": [
    "## 14. Reduccion de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# PCA: Principal Component Analysis\n# Reduce dimensiones manteniendo maxima varianza\npca = PCA(n_components=2)  # reducir a 2 componentes\nX_pca = pca.fit_transform(X)\n\n# Ver varianza explicada por cada componente\nprint(f'Varianza explicada: {pca.explained_variance_ratio_}')\nprint(f'Varianza acumulada: {pca.explained_variance_ratio_.cumsum()}')\n\n# PCA manteniendo X% de varianza\npca = PCA(n_components=0.95)  # mantener 95% de varianza\nX_pca = pca.fit_transform(X)\nprint(f'Numero de componentes: {pca.n_components_}')\n\n# Cargar componentes principales (loadings)\nloadings = pca.components_\n\n# t-SNE: para visualizacion (no lineal)\n# Mas lento que PCA pero mejor para visualizar clusters\ntsne = TSNE(\n    n_components=2,\n    perplexity=30,            # balance entre local y global\n    n_iter=1000,\n    random_state=42\n)\nX_tsne = tsne.fit_transform(X)\n\n# IMPORTANTE: aplicar PCA despues de escalar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_29",
   "metadata": {},
   "source": [
    "## 15. Metricas de Evaluacion - Clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\n# Metricas basicas\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')  # average: 'binary', 'weighted', 'macro', 'micro'\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {f1}')\n\n# Matriz de confusion\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n# Formato: [[TN, FP],\n#           [FN, TP]]\n\n# Reporte de clasificacion completo\nprint(classification_report(y_test, y_pred))\n\n# ROC AUC Score (requiere probabilidades)\n# Para clasificacion binaria\nauc = roc_auc_score(y_test, y_pred_proba[:, 1])\n\n# Para clasificacion multiclase\nauc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n\n# Curva ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n\n# Visualizar matriz de confusion\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_31",
   "metadata": {},
   "source": [
    "## 16. Metricas de Evaluacion - Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n\n# Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)  # Root Mean Squared Error\n\n# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\n\n# R2 Score (coeficiente de determinacion)\n# 1 = perfecto, 0 = modelo no mejor que la media\nr2 = r2_score(y_test, y_pred)\n\n# Mean Absolute Percentage Error (MAPE)\nmape = mean_absolute_percentage_error(y_test, y_pred)\n\nprint(f'MSE: {mse}')\nprint(f'RMSE: {rmse}')\nprint(f'MAE: {mae}')\nprint(f'R2: {r2}')\nprint(f'MAPE: {mape}')\n\n# Residuales\nresiduales = y_test - y_pred\n\n# Grafico de residuales\nplt.scatter(y_pred, residuales)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_33",
   "metadata": {},
   "source": [
    "## 17. Validacion Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, StratifiedKFold\n\n# Cross validation simple (5 folds por defecto)\nscores = cross_val_score(modelo, X, y, cv=5, scoring='accuracy')\nprint(f'Scores: {scores}')\nprint(f'Media: {scores.mean()}')\nprint(f'Desviacion: {scores.std()}')\n\n# Cross validation con KFold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(modelo, X, y, cv=kfold, scoring='accuracy')\n\n# StratifiedKFold: mantiene proporcion de clases en cada fold\nskfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(modelo, X, y, cv=skfold, scoring='accuracy')\n\n# Cross validation con multiples metricas\nscoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\nscores = cross_validate(modelo, X, y, cv=5, scoring=scoring, return_train_score=True)\nprint(f'Test Accuracy: {scores[\"test_accuracy\"].mean()}')\nprint(f'Test Precision: {scores[\"test_precision_weighted\"].mean()}')\n\n# Scoring options para clasificacion:\n# 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'\n# 'precision_weighted', 'recall_weighted', 'f1_weighted'\n\n# Scoring options para regresion:\n# 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_35",
   "metadata": {},
   "source": [
    "## 18. Busqueda de Hiperparametros - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n\n# Definir grid de parametros\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid Search\ngrid_search = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_grid=param_grid,\n    cv=5,                     # numero de folds\n    scoring='accuracy',\n    n_jobs=-1,                # usar todos los cores\n    verbose=1,                # mostrar progreso\n    return_train_score=True\n)\ngrid_search.fit(X_train, y_train)\n\n# Mejores parametros y score\nprint(f'Mejores parametros: {grid_search.best_params_}')\nprint(f'Mejor score: {grid_search.best_score_}')\n\n# Mejor modelo\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Resultados detallados\nresultados = pd.DataFrame(grid_search.cv_results_)\nprint(resultados[['params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_37",
   "metadata": {},
   "source": [
    "## 19. Busqueda de Hiperparametros - Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Definir distribuciones de parametros\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(5, 50),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': uniform(0.1, 0.9)\n}\n\n# Randomized Search (mas rapido que Grid Search)\nrandom_search = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_distributions=param_dist,\n    n_iter=100,               # numero de combinaciones a probar\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\nrandom_search.fit(X_train, y_train)\n\n# Mejores parametros y score\nprint(f'Mejores parametros: {random_search.best_params_}')\nprint(f'Mejor score: {random_search.best_score_}')\n\n# Mejor modelo\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_39",
   "metadata": {},
   "source": [
    "## 20. Pipelines - Concatenar transformaciones y modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Pipeline simple\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('modelo', LogisticRegression(random_state=42))\n])\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# Pipeline con make_pipeline (nombres automaticos)\npipeline = make_pipeline(\n    StandardScaler(),\n    PCA(n_components=10),\n    RandomForestClassifier(random_state=42)\n)\npipeline.fit(X_train, y_train)\n\n# ColumnTransformer: aplicar diferentes transformaciones a diferentes columnas\ncolumnas_numericas = ['edad', 'salario', 'experiencia']\ncolumnas_categoricas = ['ciudad', 'departamento']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), columnas_numericas),\n        ('cat', OneHotEncoder(drop='first'), columnas_categoricas)\n    ])\n\n# Pipeline completo con ColumnTransformer\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# Grid Search con Pipeline\nparam_grid = {\n    'preprocessor__num__with_mean': [True, False],\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [5, 10, None]\n}\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_41",
   "metadata": {},
   "source": [
    "## 21. Seleccion de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, chi2\nfrom sklearn.feature_selection import RFE, SelectFromModel\n\n# SelectKBest: selecciona K mejores features\n# Para clasificacion\nselector = SelectKBest(score_func=f_classif, k=10)  # seleccionar 10 mejores\nX_selected = selector.fit_transform(X_train, y_train)\nselected_features = selector.get_support(indices=True)\n\n# Para regresion\nselector = SelectKBest(score_func=f_regression, k=10)\nX_selected = selector.fit_transform(X_train, y_train)\n\n# Chi2: para features no negativas\nselector = SelectKBest(score_func=chi2, k=10)\nX_selected = selector.fit_transform(X_train, y_train)\n\n# RFE: Recursive Feature Elimination\n# Elimina recursivamente las peores features\nselector = RFE(\n    estimator=RandomForestClassifier(random_state=42),\n    n_features_to_select=10,\n    step=1                    # numero de features a eliminar en cada paso\n)\nselector.fit(X_train, y_train)\nX_selected = selector.transform(X_train)\nselected_features = selector.support_\nranking = selector.ranking_\n\n# SelectFromModel: selecciona basado en importancia del modelo\nselector = SelectFromModel(\n    estimator=RandomForestClassifier(random_state=42),\n    threshold='median'        # umbral de importancia: 'mean', 'median', float\n)\nselector.fit(X_train, y_train)\nX_selected = selector.transform(X_train)\nselected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_43",
   "metadata": {},
   "source": [
    "## 22. Manejo de Datos Desbalanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n\n# Class weights: penalizar mas errores en clase minoritaria\n# Opcion 1: calcular manualmente\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\n# Opcion 2: usar 'balanced' automaticamente\nmodelo = LogisticRegression(class_weight='balanced', random_state=42)\nmodelo.fit(X_train, y_train)\n\n# Para Random Forest\nmodelo = RandomForestClassifier(class_weight='balanced', random_state=42)\nmodelo.fit(X_train, y_train)\n\n# Para SVM\nmodelo = SVC(class_weight='balanced', random_state=42)\nmodelo.fit(X_train, y_train)\n\n# Ajustar threshold de decision\ny_pred_proba = modelo.predict_proba(X_test)[:, 1]\nthreshold = 0.3  # reducir threshold para detectar mas positivos\ny_pred_adjusted = (y_pred_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_45",
   "metadata": {},
   "source": [
    "## 23. Guardar y Cargar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\nimport pickle\n\n# Guardar modelo con joblib (recomendado)\njoblib.dump(modelo, 'modelo.pkl')\n\n# Cargar modelo con joblib\nmodelo_cargado = joblib.load('modelo.pkl')\ny_pred = modelo_cargado.predict(X_test)\n\n# Guardar con pickle\nwith open('modelo.pkl', 'wb') as file:\n    pickle.dump(modelo, file)\n\n# Cargar con pickle\nwith open('modelo.pkl', 'rb') as file:\n    modelo_cargado = pickle.load(file)\n\n# Guardar pipeline completo\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\npipeline.fit(X_train, y_train)\njoblib.dump(pipeline, 'pipeline_completo.pkl')\n\n# Cargar pipeline\npipeline_cargado = joblib.load('pipeline_completo.pkl')\ny_pred = pipeline_cargado.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_47",
   "metadata": {},
   "source": [
    "## 24. Metodos Ensemble Adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, VotingRegressor, BaggingClassifier, AdaBoostClassifier\n\n# Voting Classifier: combina multiples modelos\nmodelo1 = LogisticRegression(random_state=42)\nmodelo2 = RandomForestClassifier(random_state=42)\nmodelo3 = SVC(probability=True, random_state=42)\n\n# Voting hard: voto por mayoria\nvoting = VotingClassifier(\n    estimators=[('lr', modelo1), ('rf', modelo2), ('svc', modelo3)],\n    voting='hard'\n)\nvoting.fit(X_train, y_train)\ny_pred = voting.predict(X_test)\n\n# Voting soft: promedio de probabilidades\nvoting = VotingClassifier(\n    estimators=[('lr', modelo1), ('rf', modelo2), ('svc', modelo3)],\n    voting='soft',\n    weights=[1, 2, 1]         # pesos para cada modelo\n)\nvoting.fit(X_train, y_train)\ny_pred = voting.predict(X_test)\n\n# Bagging: multiples modelos del mismo tipo con muestras aleatorias\nbagging = BaggingClassifier(\n    estimator=DecisionTreeClassifier(),\n    n_estimators=100,\n    max_samples=0.8,          # proporcion de muestras para cada modelo\n    max_features=0.8,         # proporcion de features para cada modelo\n    random_state=42\n)\nbagging.fit(X_train, y_train)\ny_pred = bagging.predict(X_test)\n\n# AdaBoost: ensemble secuencial con pesos adaptativos\nadaboost = AdaBoostClassifier(\n    estimator=DecisionTreeClassifier(max_depth=1),\n    n_estimators=100,\n    learning_rate=1.0,\n    random_state=42\n)\nadaboost.fit(X_train, y_train)\ny_pred = adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_49",
   "metadata": {},
   "source": [
    "## 25. Feature Engineering con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer\nfrom sklearn.preprocessing import Binarizer, KBinsDiscretizer\n\n# PolynomialFeatures: crear features polinomiales\n# Ejemplo: [a, b] -> [1, a, b, a^2, ab, b^2]\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\nfeature_names = poly.get_feature_names_out()\n\n# Binarizer: convertir a binario segun umbral\nbinarizer = Binarizer(threshold=0.5)\nX_binary = binarizer.transform(X)\n\n# KBinsDiscretizer: discretizar variables continuas en bins\ndiscretizer = KBinsDiscretizer(\n    n_bins=5,                 # numero de bins\n    encode='ordinal',         # 'ordinal', 'onehot', 'onehot-dense'\n    strategy='quantile'       # 'uniform', 'quantile', 'kmeans'\n)\nX_discretized = discretizer.fit_transform(X)\n\n# FunctionTransformer: aplicar funcion personalizada\ndef log_transform(X):\n    return np.log1p(X)  # log(1 + X)\n\ntransformer = FunctionTransformer(log_transform)\nX_log = transformer.fit_transform(X)\n\n# Interacciones personalizadas en Pipeline\ndef crear_interacciones(X):\n    df = pd.DataFrame(X)\n    df['interaccion_1_2'] = df[0] * df[1]\n    df['ratio_1_2'] = df[0] / (df[1] + 1)\n    return df\n\ntransformer = FunctionTransformer(crear_interacciones)\npipeline = make_pipeline(transformer, StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_51",
   "metadata": {},
   "source": [
    "## 26. Manejo de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n\n# SimpleImputer: imputacion simple\n# Estrategias: 'mean', 'median', 'most_frequent', 'constant'\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X_train)\n\n# Imputar con mediana\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X_train)\n\n# Imputar con moda (para categoricas)\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = imputer.fit_transform(X_train)\n\n# Imputar con constante\nimputer = SimpleImputer(strategy='constant', fill_value=0)\nX_imputed = imputer.fit_transform(X_train)\n\n# KNNImputer: imputacion basada en K vecinos mas cercanos\nimputer = KNNImputer(n_neighbors=5)\nX_imputed = imputer.fit_transform(X_train)\n\n# Uso en Pipeline\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ('model', RandomForestClassifier())\n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_53",
   "metadata": {},
   "source": [
    "## 27. Inspeccion y Analisis de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n\n# Permutation Importance: importancia por permutacion\n# Mas preciso que feature_importances_ para Random Forest\nresultado = permutation_importance(modelo, X_test, y_test, n_repeats=10, random_state=42)\nimportancias = resultado.importances_mean\n\nfor i, imp in enumerate(importancias):\n    print(f'Feature {i}: {imp}')\n\n# Partial Dependence: efecto de una feature en prediccion\n# Muestra como cambia la prediccion al variar una feature\nfrom sklearn.inspection import PartialDependenceDisplay\nfeatures = [0, 1, (0, 1)]  # features individuales y par de features\nPartialDependenceDisplay.from_estimator(modelo, X, features)\n\n# Learning curves: diagnosticar overfitting/underfitting\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    modelo, X, y,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    cv=5,\n    scoring='accuracy'\n)\n\ntrain_mean = train_scores.mean(axis=1)\nval_mean = val_scores.mean(axis=1)\n\nplt.plot(train_sizes, train_mean, label='Training score')\nplt.plot(train_sizes, val_mean, label='Validation score')\nplt.xlabel('Training size')\nplt.ylabel('Score')\nplt.legend()\nplt.show()\n\n# Validation curves: evaluar efecto de un hiperparametro\nfrom sklearn.model_selection import validation_curve\n\nparam_range = [10, 50, 100, 200, 500]\ntrain_scores, val_scores = validation_curve(\n    RandomForestClassifier(random_state=42), X, y,\n    param_name='n_estimators',\n    param_range=param_range,\n    cv=5,\n    scoring='accuracy'\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_55",
   "metadata": {},
   "source": [
    "## 28. Calibracion de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\n# Calibrar probabilidades de un modelo\n# Util cuando las probabilidades predichas no son confiables\nmodelo_base = RandomForestClassifier(random_state=42)\nmodelo_calibrado = CalibratedClassifierCV(modelo_base, method='sigmoid', cv=5)\nmodelo_calibrado.fit(X_train, y_train)\ny_pred_proba = modelo_calibrado.predict_proba(X_test)\n\n# method='sigmoid': Platt scaling (bueno para modelos tipo SVM)\n# method='isotonic': regresion isotonica (mas flexible, requiere mas datos)\n\n# Evaluar calibracion\nprob_true, prob_pred = calibration_curve(y_test, y_pred_proba[:, 1], n_bins=10)\n\nplt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\nplt.plot(prob_pred, prob_true, marker='o', label='Model')\nplt.xlabel('Mean predicted probability')\nplt.ylabel('Fraction of positives')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_57",
   "metadata": {},
   "source": [
    "## 29. Deteccion de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\n\n# Isolation Forest\niso_forest = IsolationForest(\n    contamination=0.1,        # proporcion esperada de outliers\n    random_state=42\n)\noutliers = iso_forest.fit_predict(X)  # -1 para outliers, 1 para inliers\n\n# Local Outlier Factor\nlof = LocalOutlierFactor(\n    n_neighbors=20,\n    contamination=0.1\n)\noutliers = lof.fit_predict(X)\n\n# Elliptic Envelope: asume distribucion gaussiana\nenvelope = EllipticEnvelope(contamination=0.1, random_state=42)\noutliers = envelope.fit_predict(X)\n\n# Usar outlier detection para limpiar datos\nmask_inliers = outliers == 1\nX_clean = X[mask_inliers]\ny_clean = y[mask_inliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_59",
   "metadata": {},
   "source": [
    "## 30. Tips y Mejores Practicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SIEMPRE escalar datos para SVM, KNN, regresion regularizada\n# NO es necesario para arboles y Random Forest\n\n# 2. SIEMPRE usar random_state para reproducibilidad\nmodelo = RandomForestClassifier(random_state=42)\n\n# 3. SIEMPRE fit scaler solo en train, transform en train y test\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. Usar Pipeline para evitar data leakage\npipeline = make_pipeline(StandardScaler(), LogisticRegression())\npipeline.fit(X_train, y_train)  # scaling se aplica solo a train\n\n# 5. Usar stratify en train_test_split para datos desbalanceados\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\n# 6. Cross validation es mejor que una sola division train/test\nscores = cross_val_score(modelo, X, y, cv=5)\n\n# 7. Para datasets grandes, usar n_jobs=-1 para paralelizar\nmodelo = RandomForestClassifier(n_jobs=-1)\n\n# 8. GridSearch puede ser muy lento, considerar RandomizedSearch\n# GridSearch: prueba todas las combinaciones\n# RandomizedSearch: prueba N combinaciones aleatorias\n\n# 9. Verificar distribucion de clases en clasificacion\nprint(pd.Series(y).value_counts())\n\n# 10. Para modelos interpretables: Regresion Logistica, Decision Trees\n# Para mejor performance: Random Forest, Gradient Boosting, XGBoost\n\n# 11. Siempre verificar el rango de features antes de modelar\nprint(X.describe())\n\n# 12. Documentar los hiperparametros elegidos y por que\n# Usar nombres descriptivos para pipelines y modelos\n\n# 13. Guardar el modelo final con su pipeline completo\njoblib.dump(pipeline, 'modelo_completo.pkl')\n\n# 14. Monitorear metricas en train y test para detectar overfitting\ntrain_score = modelo.score(X_train, y_train)\ntest_score = modelo.score(X_test, y_test)\nprint(f'Train: {train_score}, Test: {test_score}')\n\n# 15. Para datos con muchas features, considerar PCA o feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}