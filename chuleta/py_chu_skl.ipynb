{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996fbece",
   "metadata": {},
   "source": [
    "# √çNDICE - SCIKIT-LEARN CHEAT SHEET\n",
    "\n",
    "## üìö Tabla de Contenidos\n",
    "\n",
    "### Preparaci√≥n de Datos\n",
    "- [1. Importaciones B√°sicas](#1-importaciones-basicas)\n",
    "- [2. Divisi√≥n de Datos - train_test_split](#2-division-de-datos---train_test_split)\n",
    "- [3. Escalado y Normalizaci√≥n de Datos](#3-escalado-y-normalizacion-de-datos)\n",
    "- [4. Codificaci√≥n de Variables Categ√≥ricas](#4-codificacion-de-variables-categoricas)\n",
    "  - [4.1. Variance Threshold](#41-variance-threshold)\n",
    "\n",
    "### Modelos Supervisados\n",
    "#### Regresi√≥n\n",
    "- [5. Modelos de Regresi√≥n](#5-modelos-de-regresion---predecir-valores-continuos)\n",
    "  - [5.1. ElasticNetCV](#51-elasticnetcv)\n",
    "\n",
    "#### Clasificaci√≥n\n",
    "- [6. Regresi√≥n Log√≠stica](#6-regresion-logistica---clasificacion-binaria-y-multiclase)\n",
    "  - [6.1. LogisticRegressionCV](#61-logisticregressioncv)\n",
    "- [7. √Årboles de Decisi√≥n](#7-arboles-de-decision)\n",
    "- [8. Random Forest](#8-random-forest---ensemble-de-arboles)\n",
    "- [9. Gradient Boosting](#9-gradient-boosting---ensemble-secuencial)\n",
    "- [10. Support Vector Machines (SVM)](#10-support-vector-machines-svm)\n",
    "- [11. K-Nearest Neighbors (KNN)](#11-k-nearest-neighbors-knn)\n",
    "- [12. Naive Bayes](#12-naive-bayes)\n",
    "\n",
    "### Modelos No Supervisados\n",
    "- [13. Clustering](#13-clustering---aprendizaje-no-supervisado)\n",
    "  - [13.1. KMeans - Gu√≠a Completa](#131-kmeans---gu√≠a-completa)\n",
    "  - [13.2. Hierarchical Clustering](#132-hierarchical-clustering---gu√≠a-completa-dendrogram-y-linkage)\n",
    "\n",
    "### Evaluaci√≥n y M√©tricas\n",
    "- [14. Guardar y Cargar Modelos](#14-guardar-y-cargar-modelos)\n",
    "- [15. M√©tricas de Evaluaci√≥n - Clasificaci√≥n](#15-metricas-de-evaluacion---clasificacion)\n",
    "  - [15.1. ROC Curve y PR Curve](#151-roc-curve-y-pr-curve---gu√≠a-completa)\n",
    "- [16. M√©tricas de Evaluaci√≥n - Regresi√≥n](#16-metricas-de-evaluacion---regresion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_0",
   "metadata": {},
   "source": [
    "SCIKIT-LEARN (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_1",
   "metadata": {},
   "source": [
    "## 1. Importaciones Basicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_3",
   "metadata": {},
   "source": [
    "## 2. Division de Datos - train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division basica 80-20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Division con estratificacion (mantiene proporcion de clases)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Division en tres conjuntos: train, validation, test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Parametros importantes:\n",
    "# test_size: proporcion del conjunto de prueba (0.2 = 20%)\n",
    "# random_state: semilla para reproducibilidad\n",
    "# stratify: mantiene la distribucion de clases en train y test\n",
    "# shuffle: mezcla los datos antes de dividir (True por defecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_5",
   "metadata": {},
   "source": [
    "## 3. Escalado y Normalizacion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler: media 0 y desviacion estandar 1\n",
    "# Formula: (x - media) / desviacion_estandar\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # ajusta y transforma train\n",
    "X_test_scaled = scaler.transform(X_test)        # solo transforma test\n",
    "\n",
    "# MinMaxScaler: escala entre 0 y 1\n",
    "# Formula: (x - min) / (max - min)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MinMaxScaler personalizado: escala en rango especifico\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))  # escala entre 0 y 10\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# RobustScaler: robusto a outliers, usa mediana y rango intercuartil\n",
    "# Formula: (x - mediana) / IQR\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# IMPORTANTE: siempre fit solo en train, transform en train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_7",
   "metadata": {},
   "source": [
    "## 4. Codificacion de Variables Categoricas - Gu√≠a Completa\n",
    "\n",
    "Las variables categ√≥ricas no pueden ser procesadas directamente por algoritmos de ML.\n",
    "Necesitamos convertirlas a formato num√©rico. Existen diferentes m√©todos seg√∫n el tipo de variable:\n",
    "\n",
    "### Tipos de Variables Categ√≥ricas:\n",
    "- **Nominales**: Sin orden (color: rojo, azul, verde)\n",
    "- **Ordinales**: Con orden (talla: S, M, L, XL)\n",
    "- **Binarias**: Solo dos valores (s√≠/no, True/False)\n",
    "\n",
    "### M√©todos de Codificaci√≥n:\n",
    "1. **LabelEncoder**: Para target (y) o variables ordinales\n",
    "2. **OneHotEncoder**: Para variables nominales (features)\n",
    "3. **OrdinalEncoder**: Para variables ordinales con orden espec√≠fico\n",
    "4. **get_dummies**: Alternativa simple de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. LABELENCODER - Para variable objetivo (y)\n",
    "# ============================================\n",
    "# Convierte categor√≠as a n√∫meros enteros (0, 1, 2, ...)\n",
    "# ‚ö†Ô∏è NO usar para features (X) porque implica orden que no existe\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ejemplo: variable objetivo con clases\n",
    "y = np.array(['gato', 'perro', 'gato', 'pajaro', 'perro', 'pajaro'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f'Original: {y}')\n",
    "print(f'Encoded:  {y_encoded}')  # [0, 1, 0, 2, 1, 2]\n",
    "print(f'Classes:  {le.classes_}')  # ['gato', 'pajaro', 'perro']\n",
    "\n",
    "# Volver a categor√≠as originales\n",
    "y_decoded = le.inverse_transform(y_encoded)\n",
    "print(f'Decoded:  {y_decoded}')\n",
    "\n",
    "# Ver el mapeo completo\n",
    "mapeo = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f'Mapeo: {mapeo}')  # {'gato': 0, 'pajaro': 1, 'perro': 2}\n",
    "\n",
    "# ============================================\n",
    "# 2. ONEHOTENCODER - Para variables nominales (Features)\n",
    "# ============================================\n",
    "# Crea columnas binarias (0/1) para cada categor√≠a\n",
    "# ‚úÖ Usar para variables sin orden (color, pa√≠s, categor√≠a)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Datos de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'color': ['rojo', 'azul', 'verde', 'rojo', 'azul'],\n",
    "    'tama√±o': ['S', 'M', 'L', 'M', 'S'],\n",
    "    'precio': [10, 20, 30, 15, 25]\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# 2.1. OneHotEncoder B√ÅSICO\n",
    "# ============================================\n",
    "encoder = OneHotEncoder(sparse_output=False)  # sparse_output=False para array denso\n",
    "X_encoded = encoder.fit_transform(df[['color']])\n",
    "\n",
    "print(f'\\nOriginal:\\n{df[\"color\"].values}')\n",
    "print(f'\\nEncoded:\\n{X_encoded}')\n",
    "print(f'Feature names: {encoder.get_feature_names_out()}')\n",
    "# Resultado: ['color_azul', 'color_rojo', 'color_verde']\n",
    "\n",
    "# ============================================\n",
    "# 2.2. OneHotEncoder con drop='first'\n",
    "# ============================================\n",
    "# drop='first' elimina primera categor√≠a para evitar multicolinealidad\n",
    "# Si tenemos n categor√≠as, solo necesitamos n-1 columnas\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = encoder.fit_transform(df[['color']])\n",
    "\n",
    "print(f'\\nEncoded (drop=first):\\n{X_encoded}')\n",
    "print(f'Feature names: {encoder.get_feature_names_out()}')\n",
    "# Resultado: ['color_rojo', 'color_verde'] (azul eliminado)\n",
    "\n",
    "# ============================================\n",
    "# 2.3. OneHotEncoder M√öLTIPLES COLUMNAS\n",
    "# ============================================\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = encoder.fit_transform(df[['color', 'tama√±o']])\n",
    "\n",
    "print(f'\\nMultiple columns encoded:\\n{X_encoded}')\n",
    "print(f'Feature names: {encoder.get_feature_names_out()}')\n",
    "# Resultado: ['color_rojo', 'color_verde', 'tama√±o_L', 'tama√±o_M', 'tama√±o_S']\n",
    "\n",
    "# ============================================\n",
    "# 2.4. OneHotEncoder MANTENIENDO DATAFRAME\n",
    "# ============================================\n",
    "# Mantener estructura de DataFrame para mejor legibilidad\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = encoder.fit_transform(df[['color', 'tama√±o']])\n",
    "\n",
    "# Crear DataFrame con nombres de columnas\n",
    "df_encoded = pd.DataFrame(\n",
    "    X_encoded, \n",
    "    columns=encoder.get_feature_names_out(),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "print(f'\\nDataFrame encoded:\\n{df_encoded}')\n",
    "\n",
    "# Combinar con columnas num√©ricas originales\n",
    "df_final = pd.concat([df[['precio']], df_encoded], axis=1)\n",
    "print(f'\\nDataFrame final:\\n{df_final}')\n",
    "\n",
    "# ============================================\n",
    "# 2.5. OneHotEncoder con handle_unknown='ignore'\n",
    "# ============================================\n",
    "# Manejar categor√≠as nuevas en datos de test\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoder.fit(df[['color']])\n",
    "\n",
    "# Datos de test con categor√≠a nueva 'amarillo'\n",
    "df_test = pd.DataFrame({'color': ['rojo', 'amarillo', 'verde']})\n",
    "X_test_encoded = encoder.transform(df_test[['color']])\n",
    "\n",
    "print(f'\\nTest con categor√≠a nueva:\\n{X_test_encoded}')\n",
    "# 'amarillo' no existe ‚Üí todas las columnas en 0\n",
    "\n",
    "# ============================================\n",
    "# 2.6. OneHotEncoder - Convertir SPARSE a DENSO\n",
    "# ============================================\n",
    "# Por defecto, OneHotEncoder devuelve matriz sparse (eficiente en memoria)\n",
    "encoder = OneHotEncoder()  # sparse_output=True por defecto\n",
    "X_sparse = encoder.fit_transform(df[['color']])\n",
    "\n",
    "print(f'\\nTipo sparse: {type(X_sparse)}')  # <class 'scipy.sparse._csr.csr_matrix'>\n",
    "print(f'Shape: {X_sparse.shape}')\n",
    "\n",
    "# Convertir a array denso si es necesario\n",
    "X_dense = X_sparse.toarray()\n",
    "print(f'Tipo denso: {type(X_dense)}')  # <class 'numpy.ndarray'>\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANTE: StandardScaler no funciona con sparse ‚Üí usar .toarray()\n",
    "\n",
    "# ============================================\n",
    "# 3. ORDINALENCODER - Para variables ordinales\n",
    "# ============================================\n",
    "# Convierte categor√≠as a n√∫meros RESPETANDO ORDEN\n",
    "# ‚úÖ Usar cuando existe orden l√≥gico: bajo < medio < alto\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Datos con orden\n",
    "df_ordinal = pd.DataFrame({\n",
    "    'educacion': ['Secundaria', 'Universidad', 'Primaria', 'Postgrado', 'Universidad'],\n",
    "    'satisfaccion': ['Bajo', 'Medio', 'Alto', 'Medio', 'Bajo']\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# 3.1. OrdinalEncoder B√ÅSICO\n",
    "# ============================================\n",
    "# Definir orden de categor√≠as expl√≠citamente\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        ['Primaria', 'Secundaria', 'Universidad', 'Postgrado'],\n",
    "        ['Bajo', 'Medio', 'Alto']\n",
    "    ]\n",
    ")\n",
    "X_encoded = encoder.fit_transform(df_ordinal)\n",
    "\n",
    "print(f'\\nOriginal:\\n{df_ordinal}')\n",
    "print(f'\\nOrdinal encoded:\\n{X_encoded}')\n",
    "# Primaria=0, Secundaria=1, Universidad=2, Postgrado=3\n",
    "# Bajo=0, Medio=1, Alto=2\n",
    "\n",
    "# Ver mapeo\n",
    "for i, col in enumerate(df_ordinal.columns):\n",
    "    print(f'{col}: {encoder.categories_[i]}')\n",
    "\n",
    "# ============================================\n",
    "# 3.2. OrdinalEncoder sin especificar orden\n",
    "# ============================================\n",
    "# Si no especificas categories, usa orden alfab√©tico (‚ö†Ô∏è puede no ser lo que quieres)\n",
    "encoder_auto = OrdinalEncoder()\n",
    "X_auto = encoder_auto.fit_transform(df_ordinal)\n",
    "\n",
    "print(f'\\nOrdinal autom√°tico:\\n{X_auto}')\n",
    "print(f'Categor√≠as detectadas: {encoder_auto.categories_}')\n",
    "\n",
    "# ============================================\n",
    "# 3.3. OrdinalEncoder con handle_unknown='use_encoded_value'\n",
    "# ============================================\n",
    "# Asignar valor espec√≠fico a categor√≠as desconocidas\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[['Primaria', 'Secundaria', 'Universidad', 'Postgrado']],\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1  # valor para categor√≠as desconocidas\n",
    ")\n",
    "encoder.fit(df_ordinal[['educacion']])\n",
    "\n",
    "# Test con categor√≠a nueva\n",
    "df_test = pd.DataFrame({'educacion': ['Universidad', 'Doctorado', 'Primaria']})\n",
    "X_test = encoder.transform(df_test)\n",
    "print(f'\\nTest con desconocido:\\n{X_test}')  # Doctorado ‚Üí -1\n",
    "\n",
    "# ============================================\n",
    "# 4. pd.get_dummies() - Alternativa de Pandas\n",
    "# ============================================\n",
    "# Forma r√°pida de hacer One-Hot Encoding sin sklearn\n",
    "df = pd.DataFrame({\n",
    "    'color': ['rojo', 'azul', 'verde', 'rojo'],\n",
    "    'tama√±o': ['S', 'M', 'L', 'M'],\n",
    "    'precio': [10, 20, 30, 15]\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# 4.1. get_dummies B√ÅSICO\n",
    "# ============================================\n",
    "df_dummies = pd.get_dummies(df, columns=['color'])\n",
    "print(f'\\nget_dummies b√°sico:\\n{df_dummies}')\n",
    "\n",
    "# ============================================\n",
    "# 4.2. get_dummies con drop_first=True\n",
    "# ============================================\n",
    "df_dummies = pd.get_dummies(df, columns=['color', 'tama√±o'], drop_first=True)\n",
    "print(f'\\nget_dummies (drop_first):\\n{df_dummies}')\n",
    "\n",
    "# ============================================\n",
    "# 4.3. get_dummies con prefijo personalizado\n",
    "# ============================================\n",
    "df_dummies = pd.get_dummies(df, columns=['color'], prefix='col')\n",
    "print(f'\\nget_dummies con prefijo:\\n{df_dummies}')\n",
    "\n",
    "# ============================================\n",
    "# 4.4. get_dummies SOLO columnas categ√≥ricas autom√°tico\n",
    "# ============================================\n",
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "# Detecta autom√°ticamente columnas object/category\n",
    "print(f'\\nget_dummies autom√°tico:\\n{df_dummies}')\n",
    "\n",
    "# ============================================\n",
    "# 5. COMPARACI√ìN: sklearn vs pandas\n",
    "# ============================================\n",
    "# sklearn (OneHotEncoder):\n",
    "# ‚úÖ Mantiene el encoder para aplicar a test\n",
    "# ‚úÖ Maneja unknown categories\n",
    "# ‚úÖ Controla sparse/dense\n",
    "# ‚ùå M√°s verbose\n",
    "\n",
    "# pandas (get_dummies):\n",
    "# ‚úÖ M√°s simple y r√°pido\n",
    "# ‚úÖ Se integra bien con DataFrames\n",
    "# ‚ùå No mantiene encoder para test\n",
    "# ‚ùå Dif√≠cil manejar unknown categories\n",
    "\n",
    "# ============================================\n",
    "# 6. WORKFLOW COMPLETO - Train y Test\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORKFLOW COMPLETO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Datos de ejemplo\n",
    "df_train = pd.DataFrame({\n",
    "    'color': ['rojo', 'azul', 'verde', 'rojo', 'azul'],\n",
    "    'tama√±o': ['S', 'M', 'L', 'M', 'S'],\n",
    "    'precio': [10, 20, 30, 15, 25],\n",
    "    'comprado': [1, 0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'color': ['verde', 'rojo', 'azul'],\n",
    "    'tama√±o': ['L', 'S', 'M'],\n",
    "    'precio': [28, 12, 22]\n",
    "})\n",
    "\n",
    "# 6.1. Separar features y target\n",
    "X_train = df_train[['color', 'tama√±o', 'precio']]\n",
    "y_train = df_train['comprado']\n",
    "X_test = df_test[['color', 'tama√±o', 'precio']]\n",
    "\n",
    "# 6.2. Aplicar OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "X_train_cat = encoder.fit_transform(X_train[['color', 'tama√±o']])\n",
    "X_test_cat = encoder.transform(X_test[['color', 'tama√±o']])\n",
    "\n",
    "# 6.3. Crear DataFrames\n",
    "df_train_encoded = pd.DataFrame(\n",
    "    X_train_cat,\n",
    "    columns=encoder.get_feature_names_out(),\n",
    "    index=X_train.index\n",
    ")\n",
    "df_test_encoded = pd.DataFrame(\n",
    "    X_test_cat,\n",
    "    columns=encoder.get_feature_names_out(),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 6.4. Combinar con variables num√©ricas\n",
    "X_train_final = pd.concat([X_train[['precio']], df_train_encoded], axis=1)\n",
    "X_test_final = pd.concat([X_test[['precio']], df_test_encoded], axis=1)\n",
    "\n",
    "print(f'\\nX_train_final:\\n{X_train_final}')\n",
    "print(f'\\nX_test_final:\\n{X_test_final}')\n",
    "\n",
    "# ============================================\n",
    "# 7. CONSEJOS Y MEJORES PR√ÅCTICAS\n",
    "# ============================================\n",
    "# 1. LabelEncoder: SOLO para target (y), NO para features (X)\n",
    "# 2. OneHotEncoder: para variables nominales sin orden\n",
    "# 3. OrdinalEncoder: para variables con orden l√≥gico\n",
    "# 4. Siempre usar drop='first' para evitar multicolinealidad\n",
    "# 5. Usar handle_unknown='ignore' en producci√≥n\n",
    "# 6. Fit en train, transform en train y test\n",
    "# 7. Si usas pd.get_dummies, aseg√∫rate columnas test = columnas train\n",
    "# 8. OneHotEncoder con sparse=True ahorra memoria en datasets grandes\n",
    "# 9. Convertir sparse a dense (.toarray()) antes de StandardScaler\n",
    "# 10. Para muchas categor√≠as (>50), considerar Target Encoding o Frequency Encoding\n",
    "\n",
    "# ============================================\n",
    "# 8. MANEJO DE ALTA CARDINALIDAD\n",
    "# ============================================\n",
    "# Cuando una variable tiene muchas categor√≠as (>50), OneHot puede crear demasiadas columnas\n",
    "\n",
    "# Opci√≥n 1: Agrupar categor√≠as menos frecuentes\n",
    "def agrupar_categorias_raras(df, columna, threshold=0.05):\n",
    "    \"\"\"Agrupa categor√≠as con frecuencia < threshold en 'Otros'\"\"\"\n",
    "    freq = df[columna].value_counts(normalize=True)\n",
    "    categorias_raras = freq[freq < threshold].index\n",
    "    df[columna] = df[columna].replace(categorias_raras, 'Otros')\n",
    "    return df\n",
    "\n",
    "# Opci√≥n 2: Target Encoding (mean encoding)\n",
    "# Reemplaza categor√≠a por media del target para esa categor√≠a\n",
    "def target_encoding(df_train, df_test, columna, target):\n",
    "    \"\"\"Target encoding simple\"\"\"\n",
    "    means = df_train.groupby(columna)[target].mean()\n",
    "    df_train[f'{columna}_encoded'] = df_train[columna].map(means)\n",
    "    df_test[f'{columna}_encoded'] = df_test[columna].map(means)\n",
    "    # Manejar categor√≠as nuevas con media global\n",
    "    global_mean = df_train[target].mean()\n",
    "    df_test[f'{columna}_encoded'].fillna(global_mean, inplace=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "# Opci√≥n 3: Frequency Encoding\n",
    "# Reemplaza categor√≠a por su frecuencia\n",
    "def frequency_encoding(df_train, df_test, columna):\n",
    "    \"\"\"Frequency encoding\"\"\"\n",
    "    freq = df_train[columna].value_counts(normalize=True)\n",
    "    df_train[f'{columna}_freq'] = df_train[columna].map(freq)\n",
    "    df_test[f'{columna}_freq'] = df_test[columna].map(freq)\n",
    "    df_test[f'{columna}_freq'].fillna(0, inplace=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec88b12",
   "metadata": {},
   "source": [
    "### 4.1. Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217420b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar esta celda como \"## 4.1 VarianceThreshold\" justo despu√©s de la secci√≥n 3 (Escalado)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# VarianceThreshold: elimina features con varianza <= threshold\n",
    "# threshold=0.0 elimina columnas constantes (varianza cero).\n",
    "# Ejemplo: aplicar sobre X_train/X_test (asume que existen X_train, X_test)\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "try:\n",
    "    # Mantener nombres de columnas si X_train es DataFrame\n",
    "    if hasattr(X_train, \"columns\"):\n",
    "        X_train_vt = selector.fit_transform(X_train)\n",
    "        cols_sel = X_train.columns[selector.get_support()]\n",
    "        X_train_vt = pd.DataFrame(X_train_vt, columns=cols_sel, index=getattr(X_train, \"index\", None))\n",
    "        X_test_vt = pd.DataFrame(selector.transform(X_test), columns=cols_sel, index=getattr(X_test, \"index\", None))\n",
    "    else:\n",
    "        X_train_vt = selector.fit_transform(X_train)\n",
    "        X_test_vt = selector.transform(X_test)\n",
    "\n",
    "    print(f'Features originales: {getattr(X_train, \"shape\", (None, None))[1]}')\n",
    "    print(f'Features retenidos despu√©s de VarianceThreshold: {selector.get_support().sum()}')\n",
    "    print('Columnas retenidas:' , list(getattr(cols_sel, \"tolist\", lambda: cols_sel)()))\n",
    "except NameError:\n",
    "    # Si no hay X_train/X_test definidos, mostrar ejemplo minimal\n",
    "    print(\"X_train/X_test no definidos en este entorno. Ejemplo m√≠nimo con array aleatorio:\")\n",
    "    X = np.random.rand(100, 5)\n",
    "    X[:, 0] = 1.0  # columna constante\n",
    "    sel = VarianceThreshold(threshold=0.0).fit(X)\n",
    "    print(\"Varianzas:\", sel.variances_)\n",
    "    print(\"Soporte:\", sel.get_support())\n",
    "\n",
    "# Uso pr√°ctico: ajustar umbral para eliminar features con baja variabilidad\n",
    "# selector = VarianceThreshold(threshold=0.01)  # por ejemplo, eliminar var < 0.01\n",
    "# X_train_vt = selector.fit_transform(X_train_scaled)  # normalmente usar datos escalados/preprocesados\n",
    "# X_test_vt = selector.transform(X_test_scaled)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_9",
   "metadata": {},
   "source": [
    "## 5. Modelos de Regresion - Predecir valores continuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Regresion Lineal simple\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(f'Coeficientes: {modelo.coef_}')\n",
    "print(f'Intercepto: {modelo.intercept_}')\n",
    "print(f'R2 Score: {modelo.score(X_test, y_test)}')\n",
    "\n",
    "# Ridge Regression: regularizacion L2, penaliza coeficientes grandes\n",
    "# Util cuando hay multicolinealidad\n",
    "modelo = Ridge(alpha=1.0)  # alpha controla la regularizacion\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Lasso Regression: regularizacion L1, puede hacer coeficientes = 0\n",
    "# Util para seleccion de features\n",
    "modelo = Lasso(alpha=1.0)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# ElasticNet: combina L1 y L2\n",
    "modelo = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio controla mezcla L1/L2\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764accb3",
   "metadata": {},
   "source": [
    "### 5.1. ElasticNetCV - Regresi√≥n con Validaci√≥n Cruzada Autom√°tica\n",
    "\n",
    "ElasticNetCV es la versi√≥n de ElasticNet con **validaci√≥n cruzada incorporada** que busca autom√°ticamente los mejores hiperpar√°metros (alpha y l1_ratio).\n",
    "\n",
    "**¬øPor qu√© usar ElasticNetCV?**\n",
    "- ‚úÖ Busca autom√°ticamente el mejor `alpha` (fuerza de regularizaci√≥n)\n",
    "- ‚úÖ Busca autom√°ticamente el mejor `l1_ratio` (mezcla L1/L2)\n",
    "- ‚úÖ Evita overfitting mediante validaci√≥n cruzada\n",
    "- ‚úÖ Combina ventajas de Ridge (L2) y Lasso (L1)\n",
    "- ‚úÖ Ideal para datasets con multicolinealidad y muchas features\n",
    "\n",
    "**Cu√°ndo usar ElasticNetCV:**\n",
    "- Problemas de regresi√≥n con muchas features\n",
    "- Cuando hay multicolinealidad entre variables\n",
    "- Cuando quieres selecci√≥n autom√°tica de features (L1) + estabilidad (L2)\n",
    "- En ex√°menes: cuando piden \"fix optimal parameters\" con CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c345984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# 1. ELASTICNETCV - USO B√ÅSICO\n",
    "# ============================================\n",
    "# ElasticNetCV busca autom√°ticamente los mejores alpha y l1_ratio\n",
    "\n",
    "# Configuraci√≥n b√°sica\n",
    "modelo = ElasticNetCV(\n",
    "    cv=5,                      # n√∫mero de folds de validaci√≥n cruzada\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # usar todos los cores\n",
    ")\n",
    "modelo.fit(X_train_scaled, y_train)\n",
    "y_pred = modelo.predict(X_test_scaled)\n",
    "\n",
    "# Ver los mejores par√°metros encontrados\n",
    "print(f'Mejor alpha: {modelo.alpha_}')\n",
    "print(f'Mejor l1_ratio: {modelo.l1_ratio_}')\n",
    "print(f'R¬≤ Score: {modelo.score(X_test_scaled, y_test):.4f}')\n",
    "\n",
    "# ============================================\n",
    "# 2. ELASTICNETCV - CONFIGURACI√ìN COMPLETA (ESTILO EXAMEN)\n",
    "# ============================================\n",
    "# Configuraci√≥n t√≠pica para ex√°menes como el 2024\n",
    "\n",
    "modelo = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0],  # rango de mezcla L1/L2\n",
    "    alphas=None,               # None = genera autom√°ticamente 100 valores\n",
    "    cv=5,                      # 5-folds cross-validation (como en examen)\n",
    "    max_iter=10000,            # m√°ximo de iteraciones\n",
    "    tol=1e-4,                  # tolerancia de convergencia\n",
    "    n_jobs=-1,                 # paralelizaci√≥n\n",
    "    random_state=42,\n",
    "    selection='cyclic'         # 'cyclic' o 'random' para actualizaci√≥n de coeficientes\n",
    ")\n",
    "\n",
    "# IMPORTANTE: siempre escalar datos antes de ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar modelo\n",
    "modelo.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = modelo.predict(X_train_scaled)\n",
    "y_pred_test = modelo.predict(X_test_scaled)\n",
    "\n",
    "# ============================================\n",
    "# 3. HIPERPAR√ÅMETROS - Explicaci√≥n Detallada\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HIPERPAR√ÅMETROS DE ELASTICNETCV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# l1_ratio: mezcla entre L1 (Lasso) y L2 (Ridge)\n",
    "# - l1_ratio = 0.0: Solo Ridge (L2) - no elimina features\n",
    "# - l1_ratio = 0.5: 50% Lasso + 50% Ridge - balance\n",
    "# - l1_ratio = 1.0: Solo Lasso (L1) - elimina features\n",
    "print(f'\\nBest l1_ratio: {modelo.l1_ratio_:.4f}')\n",
    "print('  ‚Üí 0.0 = Ridge puro (L2)')\n",
    "print('  ‚Üí 0.5 = Balance L1/L2')\n",
    "print('  ‚Üí 1.0 = Lasso puro (L1)')\n",
    "\n",
    "# alpha: fuerza de regularizaci√≥n\n",
    "# - alpha bajo (ej: 0.001): poca regularizaci√≥n, puede overfitting\n",
    "# - alpha alto (ej: 10): mucha regularizaci√≥n, puede underfitting\n",
    "print(f'\\nBest alpha: {modelo.alpha_:.6f}')\n",
    "print('  ‚Üí Controla la fuerza de la penalizaci√≥n')\n",
    "print('  ‚Üí Menor alpha = m√°s flexible (m√°s overfitting)')\n",
    "print('  ‚Üí Mayor alpha = m√°s regularizaci√≥n (m√°s simple)')\n",
    "\n",
    "# ============================================\n",
    "# 4. M√âTRICAS DE EVALUACI√ìN (ESTILO EXAMEN)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"M√âTRICAS DE EVALUACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Explained Variance Score (pedido en examen 2024)\n",
    "explained_var_train = explained_variance_score(y_train, y_pred_train)\n",
    "explained_var_test = explained_variance_score(y_test, y_pred_test)\n",
    "print(f'\\nExplained Variance (train): {explained_var_train:.4f}')\n",
    "print(f'Explained Variance (test):  {explained_var_test:.4f}')\n",
    "\n",
    "# R¬≤ Score\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f'\\nR¬≤ Score (train): {r2_train:.4f}')\n",
    "print(f'R¬≤ Score (test):  {r2_test:.4f}')\n",
    "\n",
    "# RMSE\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f'\\nRMSE (train): {rmse_train:.4f}')\n",
    "print(f'RMSE (test):  {rmse_test:.4f}')\n",
    "\n",
    "# ============================================\n",
    "# 5. FEATURES M√ÅS RELEVANTES (PEDIDO EN EXAMEN)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURES M√ÅS RELEVANTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Obtener coeficientes\n",
    "coeficientes = modelo.coef_\n",
    "\n",
    "# Crear DataFrame con features y coeficientes\n",
    "if hasattr(X_train, 'columns'):\n",
    "    feature_names = X_train.columns\n",
    "else:\n",
    "    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coeficiente': coeficientes,\n",
    "    'Abs_Coef': np.abs(coeficientes)\n",
    "}).sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    "print(f'\\nTotal features: {len(coeficientes)}')\n",
    "print(f'Features no nulas: {np.sum(coeficientes != 0)}')\n",
    "print(f'Features eliminadas (coef=0): {np.sum(coeficientes == 0)}')\n",
    "\n",
    "print('\\nTop 10 features m√°s importantes:')\n",
    "print(df_coef.head(10).to_string(index=False))\n",
    "\n",
    "# Visualizar coeficientes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_n = min(20, len(df_coef))\n",
    "df_top = df_coef.head(top_n)\n",
    "colors = ['red' if c < 0 else 'green' for c in df_top['Coeficiente']]\n",
    "plt.barh(range(top_n), df_top['Coeficiente'].values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(top_n), df_top['Feature'].values)\n",
    "plt.xlabel('Coeficiente')\n",
    "plt.title(f'Top {top_n} Features M√°s Importantes (ElasticNetCV)')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 6. SCATTERPLOT: PREDICCI√ìN vs REAL (PEDIDO EN EXAMEN)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZACI√ìN: PREDICCI√ìN vs REAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scatterplot test vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], \n",
    "         [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Predicci√≥n Perfecta')\n",
    "plt.xlabel('Valor Real (Test)', fontsize=12)\n",
    "plt.ylabel('Valor Predicho', fontsize=12)\n",
    "plt.title('ElasticNetCV: Predicci√≥n vs Valor Real', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot con densidad de puntos\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = np.vstack([y_test, y_pred_test])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(y_test, y_pred_test, c=z, s=50, cmap='viridis', alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], \n",
    "         [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Predicci√≥n Perfecta')\n",
    "plt.colorbar(scatter, label='Densidad')\n",
    "plt.xlabel('Valor Real (Test)', fontsize=12)\n",
    "plt.ylabel('Valor Predicho', fontsize=12)\n",
    "plt.title('ElasticNetCV: Predicci√≥n vs Real (con densidad)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 7. CORRELACI√ìN SPEARMAN (PEDIDO EN EXAMEN)\n",
    "# ============================================\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# Correlaci√≥n de Spearman (no param√©trica, robusta a outliers)\n",
    "corr_spearman, p_value_spearman = spearmanr(y_test, y_pred_test)\n",
    "print(f'\\nCorrelaci√≥n de Spearman: {corr_spearman:.4f}')\n",
    "print(f'P-value: {p_value_spearman:.4e}')\n",
    "\n",
    "# Tambi√©n calcular Pearson para comparar\n",
    "corr_pearson, p_value_pearson = pearsonr(y_test, y_pred_test)\n",
    "print(f'\\nCorrelaci√≥n de Pearson: {corr_pearson:.4f}')\n",
    "print(f'P-value: {p_value_pearson:.4e}')\n",
    "\n",
    "# Interpretaci√≥n\n",
    "print('\\nInterpretaci√≥n de correlaci√≥n:')\n",
    "print('  0.9-1.0: Correlaci√≥n muy fuerte')\n",
    "print('  0.7-0.9: Correlaci√≥n fuerte')\n",
    "print('  0.5-0.7: Correlaci√≥n moderada')\n",
    "print('  0.3-0.5: Correlaci√≥n d√©bil')\n",
    "print('  0.0-0.3: Correlaci√≥n muy d√©bil')\n",
    "\n",
    "# ============================================\n",
    "# 8. AN√ÅLISIS DE RESIDUOS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AN√ÅLISIS DE RESIDUOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "residuos = y_test - y_pred_test\n",
    "\n",
    "# Estad√≠sticas de residuos\n",
    "print(f'\\nMedia de residuos: {np.mean(residuos):.4f} (debe estar cerca de 0)')\n",
    "print(f'Desviaci√≥n est√°ndar: {np.std(residuos):.4f}')\n",
    "print(f'Residuo m√°ximo: {np.max(np.abs(residuos)):.4f}')\n",
    "\n",
    "# Gr√°ficos de residuos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuos vs Predicciones\n",
    "axes[0, 0].scatter(y_pred_test, residuos, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicciones')\n",
    "axes[0, 0].set_ylabel('Residuos')\n",
    "axes[0, 0].set_title('Residuos vs Predicciones')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histograma de residuos\n",
    "axes[0, 1].hist(residuos, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Residuos')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].set_title('Distribuci√≥n de Residuos')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Q-Q Plot (normalidad de residuos)\n",
    "from scipy import stats\n",
    "stats.probplot(residuos, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normalidad de Residuos)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuos absolutos vs Predicciones\n",
    "axes[1, 1].scatter(y_pred_test, np.abs(residuos), alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "axes[1, 1].set_xlabel('Predicciones')\n",
    "axes[1, 1].set_ylabel('|Residuos|')\n",
    "axes[1, 1].set_title('Residuos Absolutos vs Predicciones')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 9. COMPARAR CON OTROS MODELOS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACI√ìN CON OTROS MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "\n",
    "modelos = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'RidgeCV': RidgeCV(cv=5),\n",
    "    'LassoCV': LassoCV(cv=5, random_state=42),\n",
    "    'ElasticNetCV': modelo  # ya entrenado\n",
    "}\n",
    "\n",
    "resultados = []\n",
    "for nombre, mod in modelos.items():\n",
    "    if nombre != 'ElasticNetCV':\n",
    "        mod.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = mod.predict(X_test_scaled)\n",
    "    \n",
    "    resultados.append({\n",
    "        'Modelo': nombre,\n",
    "        'R¬≤': r2_score(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'Explained Variance': explained_variance_score(y_test, y_pred),\n",
    "        'Features Activas': np.sum(mod.coef_ != 0) if hasattr(mod, 'coef_') else 'N/A'\n",
    "    })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados).sort_values('R¬≤', ascending=False)\n",
    "print('\\nComparaci√≥n de modelos:')\n",
    "print(df_resultados.to_string(index=False))\n",
    "\n",
    "# Visualizar comparaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R¬≤ Scores\n",
    "axes[0].barh(df_resultados['Modelo'], df_resultados['R¬≤'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('R¬≤ Score')\n",
    "axes[0].set_title('Comparaci√≥n de R¬≤ Scores')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(df_resultados['Modelo'], df_resultados['RMSE'], color='salmon', edgecolor='black')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('Comparaci√≥n de RMSE (menor es mejor)')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 10. RUTA DE REGULARIZACI√ìN (PATH PLOT)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUTA DE REGULARIZACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mostrar c√≥mo cambian los coeficientes con diferentes alphas\n",
    "from sklearn.linear_model import enet_path\n",
    "\n",
    "# Calcular path\n",
    "alphas, coefs, _ = enet_path(X_train_scaled, y_train, \n",
    "                             l1_ratio=modelo.l1_ratio_, \n",
    "                             eps=1e-6, n_alphas=100)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(coefs.shape[0]):\n",
    "    plt.plot(alphas, coefs[i, :], linewidth=2)\n",
    "plt.axvline(modelo.alpha_, color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Alpha √≥ptimo ({modelo.alpha_:.4f})')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Coeficientes')\n",
    "plt.title(f'Ruta de Regularizaci√≥n ElasticNet (l1_ratio={modelo.l1_ratio_:.2f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 11. WORKFLOW COMPLETO TIPO EXAMEN 2024\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETO - ESTILO EXAMEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\"\"\"\n",
    "PASOS COMPLETOS PARA UN PROBLEMA COMO EL EXAMEN 2024:\n",
    "\n",
    "1. Preparar datos (limpieza, encoding)\n",
    "2. Separar X e y\n",
    "3. Codificar variables categ√≥ricas (OneHotEncoder)\n",
    "4. Convertir sparse a denso (.toarray())\n",
    "5. Dividir train/test (test_size=0.8 para 1/5 train, 4/5 test)\n",
    "6. Escalar datos (StandardScaler)\n",
    "7. Entrenar ElasticNetCV con cv=5\n",
    "8. Evaluar con explained_variance_score\n",
    "9. Identificar features m√°s relevantes\n",
    "10. Graficar scatterplot predicci√≥n vs real\n",
    "11. Calcular correlaci√≥n de Spearman\n",
    "\"\"\"\n",
    "\n",
    "# Ejemplo de c√≥digo completo\n",
    "def workflow_examen_elasticnet(df, target_col, cat_cols=None, test_size=0.8):\n",
    "    \"\"\"\n",
    "    Workflow completo de ElasticNetCV estilo examen\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Dataset completo\n",
    "    target_col : str\n",
    "        Nombre de la columna objetivo\n",
    "    cat_cols : list, optional\n",
    "        Lista de columnas categ√≥ricas a codificar\n",
    "    test_size : float\n",
    "        Proporci√≥n de test (0.8 = 4/5)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "    from sklearn.metrics import explained_variance_score\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    # 1. Separar X e y\n",
    "    y = df[target_col].values\n",
    "    X = df.drop(columns=target_col)\n",
    "    \n",
    "    # 2. Codificar variables categ√≥ricas si hay\n",
    "    if cat_cols:\n",
    "        encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "        X_encoded = encoder.fit_transform(X[cat_cols])\n",
    "        X_numeric = X.drop(columns=cat_cols).values\n",
    "        X = np.hstack([X_numeric, X_encoded])\n",
    "    else:\n",
    "        X = X.values\n",
    "    \n",
    "    # 3. Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 4. Escalar\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 5. ElasticNetCV con 5-folds CV\n",
    "    modelo = ElasticNetCV(\n",
    "        l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0],\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        max_iter=10000\n",
    "    )\n",
    "    modelo.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 6. Predicciones\n",
    "    y_pred_test = modelo.predict(X_test_scaled)\n",
    "    \n",
    "    # 7. M√©tricas\n",
    "    explained_var = explained_variance_score(y_test, y_pred_test)\n",
    "    corr_spearman, _ = spearmanr(y_test, y_pred_test)\n",
    "    \n",
    "    # 8. Features importantes\n",
    "    n_features_activas = np.sum(modelo.coef_ != 0)\n",
    "    \n",
    "    # 9. Visualizaci√≥n\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], \n",
    "            [y_test.min(), y_test.max()], \n",
    "            'r--', lw=2, label='Predicci√≥n Perfecta')\n",
    "    plt.xlabel('Valor Real (Test)')\n",
    "    plt.ylabel('Valor Predicho')\n",
    "    plt.title(f'ElasticNetCV: R¬≤={modelo.score(X_test_scaled, y_test):.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # 10. Resultados\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mejor alpha: {modelo.alpha_:.6f}\")\n",
    "    print(f\"Mejor l1_ratio: {modelo.l1_ratio_:.4f}\")\n",
    "    print(f\"Explained Variance: {explained_var:.4f}\")\n",
    "    print(f\"R¬≤ Score: {modelo.score(X_test_scaled, y_test):.4f}\")\n",
    "    print(f\"Correlaci√≥n Spearman: {corr_spearman:.4f}\")\n",
    "    print(f\"Features activas: {n_features_activas}/{len(modelo.coef_)}\")\n",
    "    \n",
    "    return modelo, scaler, X_train_scaled, X_test_scaled, y_train, y_test, y_pred_test\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# modelo, scaler, X_train_scaled, X_test_scaled, y_train, y_test, y_pred = \\\n",
    "#     workflow_examen_elasticnet(df_clean, 'price_eur')\n",
    "\n",
    "# ============================================\n",
    "# 12. TIPS Y ERRORES COMUNES\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ö†Ô∏è TIPS Y ERRORES COMUNES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ HACER:\n",
    "1. SIEMPRE escalar datos antes de ElasticNet\n",
    "2. Convertir matrices sparse a densas antes de escalar (.toarray())\n",
    "3. Usar cv=5 cuando el examen lo pida\n",
    "4. Guardar el scaler para usar en test\n",
    "5. Verificar que train y test tengan mismas columnas despu√©s de encoding\n",
    "\n",
    "‚ùå EVITAR:\n",
    "1. No escalar los datos ‚Üí coeficientes incorrectos\n",
    "2. Escalar matriz sparse sin convertir a densa ‚Üí ERROR\n",
    "3. Fit scaler en X completo en vez de solo X_train ‚Üí data leakage\n",
    "4. Usar test_size=0.2 cuando piden 4/5 test ‚Üí debe ser 0.8\n",
    "5. No usar handle_unknown='ignore' en OneHotEncoder ‚Üí error en test\n",
    "\n",
    "üéØ M√âTRICAS T√çPICAS DE EXAMEN:\n",
    "- Explained Variance Score: explained_variance_score()\n",
    "- R¬≤ Score: r2_score() o modelo.score()\n",
    "- Correlaci√≥n Spearman: spearmanr()\n",
    "- RMSE: np.sqrt(mean_squared_error())\n",
    "\n",
    "üìä VISUALIZACIONES T√çPICAS:\n",
    "- Scatterplot: predicci√≥n vs real\n",
    "- Gr√°fico de coeficientes: top features\n",
    "- Residuos: para verificar supuestos\n",
    "\n",
    "üîß PAR√ÅMETROS CLAVE:\n",
    "- cv=5: n√∫mero de folds (t√≠pico en ex√°menes)\n",
    "- l1_ratio: [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]\n",
    "- max_iter=10000: evitar warnings de no convergencia\n",
    "- random_state=42: reproducibilidad\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_11",
   "metadata": {},
   "source": [
    "## 6. Regresion Logistica - Clasificacion binaria y multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Clasificacion binaria\n",
    "modelo = LogisticRegression(random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)  # probabilidades de cada clase\n",
    "\n",
    "# Clasificacion multiclase\n",
    "# solver: algoritmo de optimizacion\n",
    "# 'lbfgs': bueno para datasets peque√±os\n",
    "# 'saga': bueno para datasets grandes\n",
    "# 'newton-cg': preciso pero lento\n",
    "modelo = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Con regularizacion\n",
    "modelo = LogisticRegression(penalty='l2', C=1.0, random_state=42)  # C es inverso de alpha\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Parametros importantes:\n",
    "# C: inverso de la fuerza de regularizacion (menor C = mas regularizacion)\n",
    "# penalty: 'l1', 'l2', 'elasticnet', 'none'\n",
    "# max_iter: numero maximo de iteraciones (aumentar si no converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170cf2e",
   "metadata": {},
   "source": [
    "### 6.1 LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa37bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# LogisticRegressionCV: Regresion Logistica con validacion cruzada incorporada\n",
    "# Busca automaticamente el mejor valor de C (inverso de regularizacion)\n",
    "\n",
    "# Uso basico - busca mejor C automaticamente\n",
    "modelo = LogisticRegressionCV(cv=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# Ver el mejor C encontrado\n",
    "print(f'Mejor C: {modelo.C_}')\n",
    "print(f'Scores por fold: {modelo.scores_}')\n",
    "\n",
    "# Especificar rango de valores C a probar\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "modelo = LogisticRegressionCV(Cs=Cs, cv=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# ============================================\n",
    "# RIDGE (L2) - Regularizacion L2\n",
    "# ============================================\n",
    "# Penaliza coeficientes grandes, util contra multicolinealidad\n",
    "# penalty='l2' es el valor por defecto\n",
    "modelo_ridge = LogisticRegressionCV(\n",
    "    Cs=10,                    # numero de valores C a probar\n",
    "    cv=5,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',           # 'lbfgs', 'newton-cg', 'sag', 'saga'\n",
    "    scoring='accuracy',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "modelo_ridge.fit(X_train, y_train)\n",
    "print(f'Ridge - Mejor C: {modelo_ridge.C_}')\n",
    "print(f'Ridge - Coeficientes: {modelo_ridge.coef_}')\n",
    "\n",
    "# ============================================\n",
    "# LASSO (L1) - Regularizacion L1\n",
    "# ============================================\n",
    "# Puede hacer coeficientes = 0, util para seleccion de features\n",
    "# IMPORTANTE: solo funciona con solver='liblinear' o 'saga'\n",
    "modelo_lasso = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',       # 'liblinear' o 'saga' para L1\n",
    "    scoring='accuracy',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "modelo_lasso.fit(X_train, y_train)\n",
    "print(f'Lasso - Mejor C: {modelo_lasso.C_}')\n",
    "print(f'Lasso - Coeficientes: {modelo_lasso.coef_}')\n",
    "\n",
    "# Ver features con coeficiente = 0 (eliminadas por Lasso)\n",
    "coefs_zero = np.sum(modelo_lasso.coef_ == 0)\n",
    "print(f'Features eliminadas por Lasso: {coefs_zero}')\n",
    "\n",
    "# ============================================\n",
    "# ELASTIC NET - Combinacion L1 + L2\n",
    "# ============================================\n",
    "# Combina ventajas de Ridge y Lasso\n",
    "# l1_ratio controla la mezcla: 0=Ridge, 1=Lasso, 0.5=50% cada uno\n",
    "# IMPORTANTE: solo funciona con solver='saga'\n",
    "modelo_elastic = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',            # OBLIGATORIO para elasticnet\n",
    "    l1_ratios=[0.1, 0.3, 0.5, 0.7, 0.9],  # proporcion de L1\n",
    "    scoring='accuracy',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "modelo_elastic.fit(X_train, y_train)\n",
    "print(f'ElasticNet - Mejor C: {modelo_elastic.C_}')\n",
    "print(f'ElasticNet - Mejor l1_ratio: {modelo_elastic.l1_ratio_}')\n",
    "print(f'ElasticNet - Coeficientes: {modelo_elastic.coef_}')\n",
    "\n",
    "# ============================================\n",
    "# Comparar los tres modelos\n",
    "# ============================================\n",
    "modelos = {\n",
    "    'Ridge (L2)': modelo_ridge,\n",
    "    'Lasso (L1)': modelo_lasso,\n",
    "    'ElasticNet': modelo_elastic\n",
    "}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_pred_proba = modelo.predict_proba(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # ROC AUC para clasificacion binaria\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        print(f'{nombre} - Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}')\n",
    "    else:\n",
    "        # ROC AUC para multiclase\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        print(f'{nombre} - Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}')\n",
    "    \n",
    "    # Contar features no nulas\n",
    "    non_zero = np.sum(modelo.coef_ != 0)\n",
    "    print(f'{nombre} - Features activas: {non_zero}')\n",
    "    print()\n",
    "\n",
    "# ============================================\n",
    "# Para clasificacion multiclase\n",
    "# ============================================\n",
    "modelo_multi = LogisticRegressionCV(\n",
    "    cv=5,\n",
    "    multi_class='multinomial',  # 'ovr' o 'multinomial'\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "modelo_multi.fit(X_train, y_train)\n",
    "\n",
    "# ============================================\n",
    "# Tips para elegir penalizacion\n",
    "# ============================================\n",
    "# Ridge (L2): cuando todas las features son importantes\n",
    "# Lasso (L1): cuando quieres seleccion automatica de features\n",
    "# ElasticNet: cuando tienes muchas features correlacionadas\n",
    "# \n",
    "# Valores de C:\n",
    "# - C alto (ej: 100): poca regularizacion, puede overfitting\n",
    "# - C bajo (ej: 0.01): mucha regularizacion, puede underfitting\n",
    "# - LogisticRegressionCV encuentra el mejor C automaticamente\n",
    "\n",
    "# IMPORTANTE: siempre escalar los datos antes de usar regularizacion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "modelo = LogisticRegressionCV(cv=5, penalty='elasticnet', solver='saga', \n",
    "                               l1_ratios=[0.5], random_state=42)\n",
    "modelo.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da89319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained scikit-learn demo: Confusion Matrix, ROC, and Precision-Recall\n",
    "# - Generates an imbalanced binary classification dataset\n",
    "# - Trains Logistic Regression\n",
    "# - Shows confusion matrix (default threshold 0.5)\n",
    "# - Plots ROC curve with AUC\n",
    "# - Plots Precision-Recall curve with Average Precision\n",
    "# - Finds a better threshold by maximizing F1 on validation set and shows new confusion matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# 1) Data: Imbalanced to highlight PR behavior\n",
    "X, y = make_classification(\n",
    "    n_samples=6000,\n",
    "    n_features=20,\n",
    "    n_informative=6,\n",
    "    n_redundant=4,\n",
    "    n_repeated=0,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.90, 0.10],  # 10% positive class\n",
    "    flip_y=0.01,\n",
    "    class_sep=1.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2) Model\n",
    "clf = LogisticRegression(max_iter=2000, n_jobs=None)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3) Predictions (probabilities + default threshold 0.5)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "y_pred_default = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# 4) Confusion matrix at threshold 0.5\n",
    "cm_default = confusion_matrix(y_test, y_pred_default, labels=[1, 0])  # rows: actual 1,0\n",
    "disp_default = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_default, display_labels=[1, 0]\n",
    ")\n",
    "plt.figure()\n",
    "disp_default.plot()\n",
    "plt.title(\"Confusion Matrix (threshold = 0.5)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification report (threshold = 0.5):\\n\")\n",
    "print(classification_report(y_test, y_pred_default, digits=3))\n",
    "\n",
    "# 5) ROC curve + AUC\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(f\"ROC Curve (AUC = {auc_roc:.3f})\")\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "# 6) Precision-Recall + Average Precision\n",
    "prec, rec, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "ap = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Curve (AP = {ap:.3f})\")\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "# 7) Pick a better threshold by maximizing F1 on a grid of thresholds\n",
    "# (Exclude the last element of rec/prec which corresponds to threshold = -inf)\n",
    "threshold_grid = np.linspace(0.01, 0.99, 99)\n",
    "f1_scores = []\n",
    "for thr in threshold_grid:\n",
    "    y_pred_thr = (y_proba >= thr).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thr))\n",
    "\n",
    "best_idx = int(np.argmax(f1_scores))\n",
    "best_thr = float(threshold_grid[best_idx])\n",
    "best_f1 = float(f1_scores[best_idx])\n",
    "\n",
    "print(f\"\\nBest threshold by F1 on test set: {best_thr:.3f} (F1 = {best_f1:.3f})\")\n",
    "\n",
    "y_pred_best = (y_proba >= best_thr).astype(int)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best, labels=[1, 0])\n",
    "disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=[1, 0])\n",
    "plt.figure()\n",
    "disp_best.plot()\n",
    "plt.title(f\"Confusion Matrix (best F1 threshold = {best_thr:.3f})\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification report (best F1 threshold):\\n\")\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_13",
   "metadata": {},
   "source": [
    "## 7. Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Clasificacion\n",
    "modelo = DecisionTreeClassifier(\n",
    "    max_depth=5,              # profundidad maxima del arbol\n",
    "    min_samples_split=20,     # minimo de muestras para dividir un nodo\n",
    "    min_samples_leaf=10,      # minimo de muestras en hoja\n",
    "    random_state=42\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Feature importance: importancia de cada variable\n",
    "importancias = modelo.feature_importances_\n",
    "for i, imp in enumerate(importancias):\n",
    "    print(f'Feature {i}: {imp}')\n",
    "\n",
    "# Regresion\n",
    "modelo = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Visualizar el arbol\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(modelo, filled=True, feature_names=['feat1', 'feat2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_15",
   "metadata": {},
   "source": [
    "## 8. Random Forest - Ensemble de arboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Clasificacion\n",
    "modelo = RandomForestClassifier(\n",
    "    n_estimators=100,         # numero de arboles\n",
    "    max_depth=10,             # profundidad maxima de cada arbol\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',      # numero de features aleatorias: 'sqrt', 'log2', int, float\n",
    "    random_state=42,\n",
    "    n_jobs=-1                 # usar todos los cores del CPU\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# Feature importance\n",
    "importancias = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': modelo.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Regresion\n",
    "modelo = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Out-of-bag score: estimacion de error sin validacion cruzada\n",
    "modelo = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "print(f'OOB Score: {modelo.oob_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_17",
   "metadata": {},
   "source": [
    "## 9. Gradient Boosting - Ensemble secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "# Clasificacion\n",
    "modelo = GradientBoostingClassifier(\n",
    "    n_estimators=100,         # numero de arboles\n",
    "    learning_rate=0.1,        # tasa de aprendizaje (menor = mas conservador)\n",
    "    max_depth=3,              # profundidad maxima de cada arbol\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    subsample=0.8,            # proporcion de muestras para cada arbol\n",
    "    random_state=42\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# Regresion\n",
    "modelo = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Feature importance\n",
    "importancias = modelo.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_19",
   "metadata": {},
   "source": [
    "## 10. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# Clasificacion\n",
    "modelo = SVC(\n",
    "    kernel='rbf',             # 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "    C=1.0,                    # parametro de regularizacion\n",
    "    gamma='scale',            # coeficiente del kernel: 'scale', 'auto', float\n",
    "    random_state=42\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# SVC con probabilidades\n",
    "modelo = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# Kernel lineal (mas rapido para datos linealmente separables)\n",
    "modelo = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Kernel polinomial\n",
    "modelo = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Regresion\n",
    "modelo = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# IMPORTANTE: SVM requiere datos escalados para funcionar bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_21",
   "metadata": {},
   "source": [
    "## 11. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# Clasificacion\n",
    "modelo = KNeighborsClassifier(\n",
    "    n_neighbors=5,            # numero de vecinos\n",
    "    weights='uniform',        # 'uniform' o 'distance' (ponderacion por distancia)\n",
    "    metric='minkowski',       # metrica de distancia: 'euclidean', 'manhattan', 'minkowski'\n",
    "    p=2                       # p=1 Manhattan, p=2 Euclidean\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# KNN con ponderacion por distancia\n",
    "modelo = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Regresion\n",
    "modelo = KNeighborsRegressor(n_neighbors=5, weights='uniform')\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# IMPORTANTE: KNN requiere datos escalados para funcionar bien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_23",
   "metadata": {},
   "source": [
    "## 12. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "# GaussianNB: para features continuas con distribucion normal\n",
    "modelo = GaussianNB()\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)\n",
    "\n",
    "# MultinomialNB: para features de conteo (texto, frecuencias)\n",
    "# Requiere valores no negativos\n",
    "modelo = MultinomialNB(alpha=1.0)  # alpha: parametro de suavizado\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# BernoulliNB: para features binarias (0/1)\n",
    "modelo = BernoulliNB(alpha=1.0)\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_25",
   "metadata": {},
   "source": [
    "## 13. Clustering - Aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# KMeans: particiona datos en K clusters\n",
    "modelo = KMeans(\n",
    "    n_clusters=3,             # numero de clusters\n",
    "    init='k-means++',         # metodo de inicializacion\n",
    "    n_init=10,                # numero de veces que se ejecuta con diferentes centroides\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "clusters = modelo.fit_predict(X)\n",
    "centroides = modelo.cluster_centers_\n",
    "\n",
    "# Evaluar calidad del clustering\n",
    "inercia = modelo.inertia_  # suma de distancias al cuadrado al centroide mas cercano\n",
    "silhouette = silhouette_score(X, clusters)  # entre -1 y 1, mayor es mejor\n",
    "\n",
    "# Metodo del codo para encontrar K optimo\n",
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# DBSCAN: clustering basado en densidad, no requiere especificar K\n",
    "modelo = DBSCAN(\n",
    "    eps=0.5,                  # distancia maxima entre puntos del mismo cluster\n",
    "    min_samples=5             # minimo de puntos para formar un cluster\n",
    ")\n",
    "clusters = modelo.fit_predict(X)\n",
    "# cluster -1 son outliers\n",
    "\n",
    "# Hierarchical Clustering\n",
    "modelo = AgglomerativeClustering(\n",
    "    n_clusters=3,             # numero de clusters\n",
    "    linkage='ward'            # 'ward', 'complete', 'average', 'single'\n",
    ")\n",
    "clusters = modelo.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f4c2f",
   "metadata": {},
   "source": [
    "### 13.2. Hierarchical Clustering - Gu√≠a Completa (dendrogram y linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# HIERARCHICAL CLUSTERING B√ÅSICO con scipy\n",
    "# ============================================\n",
    "# linkage() crea el √°rbol jer√°rquico\n",
    "# dendrogram() visualiza el √°rbol\n",
    "\n",
    "# M√©todo b√°sico con linkage 'ward' y distancia euclidiana\n",
    "Z = linkage(X, method='ward', metric='euclidean')\n",
    "\n",
    "# Visualizar dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z)\n",
    "plt.title('Dendrograma - Hierarchical Clustering (Ward)')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# M√âTODOS DE LINKAGE (method)\n",
    "# ============================================\n",
    "# 'ward': minimiza varianza dentro clusters (solo con euclidean)\n",
    "# 'complete': m√°xima distancia entre pares de puntos\n",
    "# 'average': promedio de distancias entre pares\n",
    "# 'single': m√≠nima distancia entre pares (sensible a outliers)\n",
    "# 'centroid': distancia entre centroides\n",
    "# 'median': mediana de distancias\n",
    "# 'weighted': weighted average\n",
    "\n",
    "# Complete linkage con distancia euclidiana\n",
    "Z_complete = linkage(X, method='complete', metric='euclidean')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_complete)\n",
    "plt.title('Dendrograma - Complete Linkage (Euclidean)')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# Complete linkage con distancia Manhattan (cityblock)\n",
    "Z_manhattan = linkage(X, method='complete', metric='cityblock')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_manhattan)\n",
    "plt.title('Dendrograma - Complete Linkage (Manhattan/Cityblock)')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# Single linkage con distancia euclidiana\n",
    "Z_single = linkage(X, method='single', metric='euclidean')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_single)\n",
    "plt.title('Dendrograma - Single Linkage (Euclidean)')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# M√âTRICAS DE DISTANCIA DISPONIBLES\n",
    "# ============================================\n",
    "# 'euclidean': distancia euclidiana (L2)\n",
    "# 'cityblock' o 'manhattan': distancia Manhattan (L1)\n",
    "# 'cosine': similaridad coseno\n",
    "# 'correlation': correlaci√≥n\n",
    "# 'hamming': para datos binarios\n",
    "# 'jaccard': para datos binarios\n",
    "# 'chebyshev': distancia Chebyshev\n",
    "# 'minkowski': distancia Minkowski generalizada\n",
    "\n",
    "# Ejemplo con distancia coseno\n",
    "Z_cosine = linkage(X, method='average', metric='cosine')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z_cosine)\n",
    "plt.title('Dendrograma - Average Linkage (Cosine)')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# CORTAR EL DENDROGRAM - Obtener clusters\n",
    "# ============================================\n",
    "# Opci√≥n 1: Especificar n√∫mero de clusters\n",
    "n_clusters = 3\n",
    "clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "print(f'Clusters (n={n_clusters}): {clusters}')\n",
    "print(f'Conteo por cluster: {np.bincount(clusters)}')\n",
    "\n",
    "# Opci√≥n 2: Especificar altura de corte (distancia)\n",
    "altura_corte = 10\n",
    "clusters_altura = fcluster(Z, altura_corte, criterion='distance')\n",
    "print(f'Clusters (altura={altura_corte}): {clusters_altura}')\n",
    "\n",
    "# Visualizar dendrogram con l√≠nea de corte\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=altura_corte, c='red', linestyle='--', label=f'Corte en altura={altura_corte}')\n",
    "plt.title('Dendrograma con L√≠nea de Corte')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PERSONALIZAR DENDROGRAM\n",
    "# ============================================\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z,\n",
    "          truncate_mode='lastp',  # mostrar solo √∫ltimas p fusiones\n",
    "          p=12,                   # n√∫mero de fusiones a mostrar\n",
    "          leaf_rotation=90,       # rotaci√≥n de etiquetas\n",
    "          leaf_font_size=10,      # tama√±o de fuente\n",
    "          show_contracted=True,   # mostrar altura de nodos contra√≠dos\n",
    "          color_threshold=15)     # colorear clusters por altura\n",
    "plt.title('Dendrograma Personalizado (Truncado)')\n",
    "plt.xlabel('√çndice de muestra o (tama√±o del cluster)')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# Dendrogram horizontal\n",
    "plt.figure(figsize=(8, 10))\n",
    "dendrogram(Z, orientation='left')\n",
    "plt.title('Dendrograma Horizontal')\n",
    "plt.xlabel('Distancia')\n",
    "plt.ylabel('√çndice de muestra')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZAR CLUSTERS EN 2D\n",
    "# ============================================\n",
    "def plot_hierarchical_clusters(X, Z, n_clusters):\n",
    "    \"\"\"Visualiza clusters de hierarchical clustering\"\"\"\n",
    "    clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'Hierarchical Clustering (K={n_clusters})')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar con 3 clusters\n",
    "plot_hierarchical_clusters(X, Z, n_clusters=3)\n",
    "\n",
    "# ============================================\n",
    "# USAR sklearn.cluster.AgglomerativeClustering\n",
    "# ============================================\n",
    "# Alternativa a scipy con API de sklearn\n",
    "\n",
    "# Ward linkage\n",
    "agg_ward = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels_ward = agg_ward.fit_predict(X)\n",
    "\n",
    "# Complete linkage\n",
    "agg_complete = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
    "labels_complete = agg_complete.fit_predict(X)\n",
    "\n",
    "# Average linkage\n",
    "agg_average = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
    "labels_average = agg_average.fit_predict(X)\n",
    "\n",
    "# Single linkage\n",
    "agg_single = AgglomerativeClustering(n_clusters=3, linkage='single')\n",
    "labels_single = agg_single.fit_predict(X)\n",
    "\n",
    "# Con distancia espec√≠fica (solo para linkage != 'ward')\n",
    "agg_manhattan = AgglomerativeClustering(\n",
    "    n_clusters=3, \n",
    "    linkage='complete', \n",
    "    metric='manhattan'  # tambi√©n: 'euclidean', 'cosine', 'l1', 'l2'\n",
    ")\n",
    "labels_manhattan = agg_manhattan.fit_predict(X)\n",
    "\n",
    "print(f'N√∫mero de clusters formados: {agg_ward.n_clusters_}')\n",
    "print(f'N√∫mero de hojas en el √°rbol: {agg_ward.n_leaves_}')\n",
    "\n",
    "# ============================================\n",
    "# M√âTODO DE LA SILUETA para elegir K √≥ptimo\n",
    "# ============================================\n",
    "def evaluar_hierarchical_silhouette(X, method='ward', metric='euclidean', max_k=10):\n",
    "    \"\"\"Eval√∫a hierarchical clustering con diferentes K usando silueta\"\"\"\n",
    "    Z = linkage(X, method=method, metric=metric)\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        clusters = fcluster(Z, k, criterion='maxclust')\n",
    "        score = silhouette_score(X, clusters)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f'K={k}: Silhouette Score = {score:.4f}')\n",
    "    \n",
    "    # Graficar\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, silhouette_scores, 'ro-')\n",
    "    plt.xlabel('N√∫mero de clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(f'M√©todo de la Silueta - Hierarchical ({method.capitalize()} Linkage)')\n",
    "    plt.xticks(K_range)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    best_k = K_range[np.argmax(silhouette_scores)]\n",
    "    print(f'\\nMejor K seg√∫n Silhouette: {best_k}')\n",
    "    return best_k, silhouette_scores\n",
    "\n",
    "# Evaluar con complete linkage y distancia euclidiana\n",
    "best_k_complete, scores_complete = evaluar_hierarchical_silhouette(\n",
    "    X, method='complete', metric='euclidean', max_k=10\n",
    ")\n",
    "\n",
    "# Evaluar con single linkage y distancia euclidiana\n",
    "best_k_single, scores_single = evaluar_hierarchical_silhouette(\n",
    "    X, method='single', metric='euclidean', max_k=10\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# COMPARAR M√öLTIPLES M√âTODOS\n",
    "# ============================================\n",
    "def comparar_linkages(X, n_clusters=3):\n",
    "    \"\"\"Compara diferentes m√©todos de linkage\"\"\"\n",
    "    metodos = ['ward', 'complete', 'average', 'single']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, metodo in enumerate(metodos):\n",
    "        # Crear linkage\n",
    "        if metodo == 'ward':\n",
    "            Z = linkage(X, method=metodo, metric='euclidean')\n",
    "        else:\n",
    "            Z = linkage(X, method=metodo, metric='euclidean')\n",
    "        \n",
    "        # Obtener clusters\n",
    "        clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "        \n",
    "        # Calcular silhouette\n",
    "        sil_score = silhouette_score(X, clusters)\n",
    "        \n",
    "        # Graficar\n",
    "        axes[idx].scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.6)\n",
    "        axes[idx].set_title(f'{metodo.capitalize()} Linkage\\nSilhouette: {sil_score:.4f}')\n",
    "        axes[idx].set_xlabel('Feature 1')\n",
    "        axes[idx].set_ylabel('Feature 2')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "comparar_linkages(X, n_clusters=3)\n",
    "\n",
    "# ============================================\n",
    "# EVALUACI√ìN DE LA CALIDAD DEL CLUSTERING\n",
    "# ============================================\n",
    "# Coeficiente de correlaci√≥n cofen√©tica: qu√© tan bien preserva las distancias\n",
    "Z = linkage(X, method='ward')\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "print(f'Coeficiente de correlaci√≥n cofen√©tica: {c:.4f}')\n",
    "# Valores cercanos a 1 = buena preservaci√≥n de distancias\n",
    "\n",
    "# ============================================\n",
    "# DENDROGRAMA CON COLORES PERSONALIZADOS\n",
    "# ============================================\n",
    "from scipy.cluster.hierarchy import set_link_color_palette\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "set_link_color_palette(['red', 'blue', 'green', 'orange', 'purple'])\n",
    "dendrogram(Z, \n",
    "          color_threshold=20,\n",
    "          above_threshold_color='gray')\n",
    "plt.title('Dendrograma con Colores Personalizados')\n",
    "plt.xlabel('√çndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLO COMPLETO: Workflow t√≠pico\n",
    "# ============================================\n",
    "def workflow_hierarchical(X, visualizar=True):\n",
    "    \"\"\"Workflow completo de hierarchical clustering\"\"\"\n",
    "    \n",
    "    # 1. Probar diferentes m√©todos de linkage\n",
    "    metodos = {\n",
    "        'Ward': linkage(X, method='ward', metric='euclidean'),\n",
    "        'Complete-Euclidean': linkage(X, method='complete', metric='euclidean'),\n",
    "        'Complete-Manhattan': linkage(X, method='complete', metric='cityblock'),\n",
    "        'Single-Euclidean': linkage(X, method='single', metric='euclidean'),\n",
    "        'Average': linkage(X, method='average', metric='euclidean')\n",
    "    }\n",
    "    \n",
    "    # 2. Calcular coeficiente cofen√©tico para cada m√©todo\n",
    "    print(\"Coeficientes de correlaci√≥n cofen√©tica:\")\n",
    "    for nombre, Z in metodos.items():\n",
    "        c, _ = cophenet(Z, pdist(X))\n",
    "        print(f'{nombre}: {c:.4f}')\n",
    "    \n",
    "    # 3. Seleccionar mejor m√©todo (por ejemplo, Ward)\n",
    "    Z_best = metodos['Ward']\n",
    "    \n",
    "    # 4. Usar silueta para encontrar K √≥ptimo\n",
    "    print(\"\\nBuscando K √≥ptimo...\")\n",
    "    best_k, scores = evaluar_hierarchical_silhouette(X, method='ward', max_k=10)\n",
    "    \n",
    "    # 5. Crear clusters finales\n",
    "    clusters = fcluster(Z_best, best_k, criterion='maxclust')\n",
    "    \n",
    "    # 6. Visualizar resultados\n",
    "    if visualizar and X.shape[1] == 2:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Dendrogram\n",
    "        dendrogram(Z_best, ax=ax1)\n",
    "        ax1.set_title('Dendrograma (Ward Linkage)')\n",
    "        ax1.set_xlabel('√çndice de muestra')\n",
    "        ax1.set_ylabel('Distancia')\n",
    "        \n",
    "        # Clusters\n",
    "        scatter = ax2.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.6)\n",
    "        ax2.set_title(f'Clusters finales (K={best_k})')\n",
    "        ax2.set_xlabel('Feature 1')\n",
    "        ax2.set_ylabel('Feature 2')\n",
    "        plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return clusters, best_k\n",
    "\n",
    "# Ejecutar workflow\n",
    "clusters_final, k_optimo = workflow_hierarchical(X)\n",
    "\n",
    "# ============================================\n",
    "# TIPS Y MEJORES PR√ÅCTICAS\n",
    "# ============================================\n",
    "# 1. Ward: mejor para clusters de tama√±o similar y forma esf√©rica\n",
    "# 2. Complete: m√°s robusto a outliers que single\n",
    "# 3. Single: sensible a outliers, puede crear \"cadenas\"\n",
    "# 4. Average: buen balance, menos sensible a outliers\n",
    "# 5. SIEMPRE escalar los datos antes de clustering\n",
    "# 6. Usar coeficiente cofen√©tico para evaluar preservaci√≥n de distancias\n",
    "# 7. Combinar dendrogram + silhouette para elegir K\n",
    "# 8. Para datasets grandes, considerar KMeans (m√°s eficiente)\n",
    "# 9. Hierarchical es determin√≠stico (no necesita random_state)\n",
    "# 10. √ötil cuando quieres visualizar la jerarqu√≠a de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a734b7",
   "metadata": {},
   "source": [
    "### 13.1. KMeans - Gu√≠a Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# KMEANS B√ÅSICO\n",
    "# ============================================\n",
    "# KMeans agrupa datos en K clusters minimizando la distancia intra-cluster\n",
    "\n",
    "# Uso b√°sico\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Atributos importantes despu√©s del fit\n",
    "centroids = kmeans.cluster_centers_     # coordenadas de los centroides\n",
    "labels = kmeans.labels_                 # etiquetas de cluster para cada punto\n",
    "inertia = kmeans.inertia_               # suma de distancias al cuadrado\n",
    "n_iter = kmeans.n_iter_                 # n√∫mero de iteraciones realizadas\n",
    "\n",
    "print(f'Inercia: {inertia}')\n",
    "print(f'Iteraciones: {n_iter}')\n",
    "print(f'Centroides:\\n{centroids}')\n",
    "\n",
    "# ============================================\n",
    "# PAR√ÅMETROS IMPORTANTES\n",
    "# ============================================\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,              # n√∫mero de clusters (OBLIGATORIO elegir)\n",
    "    init='k-means++',          # m√©todo inicializaci√≥n: 'k-means++', 'random', array\n",
    "    n_init=10,                 # n√∫mero de veces que se ejecuta con diferentes inicializaciones\n",
    "    max_iter=300,              # n√∫mero m√°ximo de iteraciones por ejecuci√≥n\n",
    "    tol=1e-4,                  # tolerancia para convergencia\n",
    "    random_state=42,           # semilla para reproducibilidad\n",
    "    algorithm='lloyd'          # 'lloyd', 'elkan' (m√°s r√°pido para datos densos)\n",
    ")\n",
    "kmeans.fit(X)\n",
    "\n",
    "# ============================================\n",
    "# M√âTODO DEL CODO - Encontrar K √≥ptimo\n",
    "# ============================================\n",
    "# Prueba diferentes valores de K y grafica la inercia\n",
    "\n",
    "inertias = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# Graficar m√©todo del codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('N√∫mero de clusters (K)')\n",
    "plt.ylabel('Inercia (Within-Cluster Sum of Squares)')\n",
    "plt.title('M√©todo del Codo para Selecci√≥n de K')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# El \"codo\" indica el K √≥ptimo (donde la inercia deja de decrecer significativamente)\n",
    "\n",
    "# ============================================\n",
    "# M√âTODO DE LA SILUETA - Encontrar K √≥ptimo\n",
    "# ============================================\n",
    "# Silhouette score: mide qu√© tan bien separados est√°n los clusters\n",
    "# Valores: [-1, 1]. M√°s cercano a 1 = mejor\n",
    "\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)  # m√≠nimo 2 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f'K={k}: Silhouette Score = {score:.4f}')\n",
    "\n",
    "# Graficar silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, silhouette_scores, 'ro-')\n",
    "plt.xlabel('N√∫mero de clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('M√©todo de la Silueta para Selecci√≥n de K')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# El K con mayor silhouette score es el √≥ptimo\n",
    "best_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f'\\nMejor K seg√∫n Silhouette: {best_k}')\n",
    "\n",
    "# ============================================\n",
    "# OTRAS M√âTRICAS DE EVALUACI√ìN\n",
    "# ============================================\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Davies-Bouldin Index: menor es mejor (mide separaci√≥n entre clusters)\n",
    "db_score = davies_bouldin_score(X, labels)\n",
    "print(f'Davies-Bouldin Index: {db_score:.4f}')\n",
    "\n",
    "# Calinski-Harabasz Index: mayor es mejor (ratio varianza entre/dentro clusters)\n",
    "ch_score = calinski_harabasz_score(X, labels)\n",
    "print(f'Calinski-Harabasz Index: {ch_score:.4f}')\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZACI√ìN DE CLUSTERS (2D)\n",
    "# ============================================\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Graficar puntos coloreados por cluster\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)\n",
    "# Graficar centroides\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='red', marker='X', s=200, edgecolors='black', linewidths=2,\n",
    "           label='Centroides')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('KMeans Clustering')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# GR√ÅFICO DE SILUETA DETALLADO\n",
    "# ============================================\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_silhouette(X, n_clusters):\n",
    "    \"\"\"Grafica el an√°lisis de silueta para KMeans\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    sample_silhouette_values = silhouette_samples(X, labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        # Valores de silueta para cluster i\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_xlabel('Coeficiente de Silueta')\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_title(f'Gr√°fico de Silueta (K={n_clusters})')\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", \n",
    "              label=f'Promedio: {silhouette_avg:.3f}')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Probar con diferentes K\n",
    "plot_silhouette(X, n_clusters=3)\n",
    "\n",
    "# ============================================\n",
    "# PREDECIR CLUSTER PARA NUEVOS DATOS\n",
    "# ============================================\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Nuevos puntos\n",
    "nuevos_datos = np.array([[5, 5], [20, 20], [15, 15]])\n",
    "clusters_nuevos = kmeans.predict(nuevos_datos)\n",
    "print(f'Clusters asignados a nuevos datos: {clusters_nuevos}')\n",
    "\n",
    "# Distancia a cada centroide\n",
    "distancias = kmeans.transform(nuevos_datos)\n",
    "print(f'Distancias a centroides:\\n{distancias}')\n",
    "\n",
    "# ============================================\n",
    "# INICIALIZACI√ìN PERSONALIZADA\n",
    "# ============================================\n",
    "# Puedes especificar centroides iniciales manualmente\n",
    "centroides_iniciales = np.array([[5, 5], [15, 15], [25, 25]])\n",
    "kmeans = KMeans(n_clusters=3, init=centroides_iniciales, n_init=1, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# ============================================\n",
    "# MANEJO DE DATOS CON PANDAS\n",
    "# ============================================\n",
    "# Si X es un DataFrame\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(df[['Feature1', 'Feature2']])\n",
    "\n",
    "# Ver estad√≠sticas por cluster\n",
    "print(df.groupby('Cluster').mean())\n",
    "print(df.groupby('Cluster').size())\n",
    "\n",
    "# ============================================\n",
    "# COMPARACI√ìN M√öLTIPLES K\n",
    "# ============================================\n",
    "def evaluar_kmeans(X, max_k=10):\n",
    "    \"\"\"Eval√∫a KMeans con diferentes K y muestra m√©tricas\"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        resultados.append({\n",
    "            'K': k,\n",
    "            'Inercia': kmeans.inertia_,\n",
    "            'Silhouette': silhouette_score(X, labels),\n",
    "            'Davies-Bouldin': davies_bouldin_score(X, labels),\n",
    "            'Calinski-Harabasz': calinski_harabasz_score(X, labels)\n",
    "        })\n",
    "    \n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    print(df_resultados.to_string(index=False))\n",
    "    \n",
    "    return df_resultados\n",
    "\n",
    "# Evaluar y comparar\n",
    "df_metricas = evaluar_kmeans(X, max_k=10)\n",
    "\n",
    "# ============================================\n",
    "# TIPS Y MEJORES PR√ÅCTICAS\n",
    "# ============================================\n",
    "# 1. SIEMPRE escalar los datos antes de KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# 2. Usar m√∫ltiples m√©todos para elegir K (codo + silueta)\n",
    "\n",
    "# 3. n_init=10 o m√°s para evitar m√≠nimos locales\n",
    "\n",
    "# 4. KMeans asume clusters esf√©ricos y de tama√±o similar\n",
    "#    No funciona bien con clusters de formas irregulares\n",
    "\n",
    "# 5. Sensible a outliers - considerar eliminarlos primero\n",
    "\n",
    "# 6. Para datasets grandes, usar algorithm='elkan' o MiniBatchKMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans_mb = MiniBatchKMeans(n_clusters=3, batch_size=100, random_state=42)\n",
    "kmeans_mb.fit(X)\n",
    "\n",
    "# 7. Comparar con otros algoritmos si KMeans no funciona bien\n",
    "#    (DBSCAN para clusters de forma arbitraria, por ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Guardar modelo con joblib (recomendado)\n",
    "joblib.dump(modelo, 'modelo.pkl')\n",
    "\n",
    "# Cargar modelo con joblib\n",
    "modelo_cargado = joblib.load('modelo.pkl')\n",
    "y_pred = modelo_cargado.predict(X_test)\n",
    "\n",
    "# Guardar con pickle\n",
    "with open('modelo.pkl', 'wb') as file:\n",
    "    pickle.dump(modelo, file)\n",
    "\n",
    "# Cargar con pickle\n",
    "with open('modelo.pkl', 'rb') as file:\n",
    "    modelo_cargado = pickle.load(file)\n",
    "\n",
    "# Guardar pipeline completo\n",
    "pipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "pipeline.fit(X_train, y_train)\n",
    "joblib.dump(pipeline, 'pipeline_completo.pkl')\n",
    "\n",
    "# Cargar pipeline\n",
    "pipeline_cargado = joblib.load('pipeline_completo.pkl')\n",
    "y_pred = pipeline_cargado.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52804a",
   "metadata": {},
   "source": [
    "## 15. Metricas de Evaluacion - Clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Ejemplo basico con regresion logistica\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "y_pred_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Matriz de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de Confusion:\")\n",
    "print(cm)\n",
    "\n",
    "# Reporte de clasificacion\n",
    "print(\"\\nReporte de Clasificacion:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d0cb0",
   "metadata": {},
   "source": [
    "### 15.1. ROC Curve y PR Curve - Gu√≠a Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "# ============================================\n",
    "# ROC CURVE - Clasificaci√≥n Binaria\n",
    "# ============================================\n",
    "# ROC (Receiver Operating Characteristic) mide trade-off entre TPR y FPR\n",
    "# TPR (True Positive Rate) = Recall = Sensitivity\n",
    "# FPR (False Positive Rate) = 1 - Specificity\n",
    "\n",
    "# Obtener probabilidades del modelo (clasificaci√≥n binaria)\n",
    "# Asumiendo: modelo.predict_proba(X_test) ya calculado\n",
    "y_pred_proba = modelo.predict_proba(X_test)[:, 1]  # probabilidades de clase positiva\n",
    "\n",
    "# Calcular ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calcular AUC (Area Under the Curve)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "# Tambi√©n: roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interpretaci√≥n del AUC:\n",
    "# AUC = 1.0: clasificador perfecto\n",
    "# AUC = 0.9-1.0: excelente\n",
    "# AUC = 0.8-0.9: muy bueno\n",
    "# AUC = 0.7-0.8: bueno\n",
    "# AUC = 0.6-0.7: mediocre\n",
    "# AUC = 0.5: random (no mejor que adivinar)\n",
    "# AUC < 0.5: peor que random (invertir predicciones)\n",
    "\n",
    "print(f'AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# ============================================\n",
    "# ROC CURVE - Usando RocCurveDisplay (sklearn >= 0.24)\n",
    "# ============================================\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# M√©todo 1: Desde predicciones\n",
    "display = RocCurveDisplay.from_predictions(y_test, y_pred_proba, name='Modelo')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# M√©todo 2: Desde estimador entrenado\n",
    "display = RocCurveDisplay.from_estimator(modelo, X_test, y_test, name='Modelo')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ENCONTRAR MEJOR THRESHOLD\n",
    "# ============================================\n",
    "# Encontrar threshold que maximiza TPR - FPR (Youden's J statistic)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f'Optimal Threshold: {optimal_threshold:.4f}')\n",
    "print(f'TPR at optimal: {tpr[optimal_idx]:.4f}')\n",
    "print(f'FPR at optimal: {fpr[optimal_idx]:.4f}')\n",
    "\n",
    "# Visualizar threshold √≥ptimo en ROC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', s=100, \n",
    "           label=f'Optimal (threshold={optimal_threshold:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve with Optimal Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Usar threshold √≥ptimo para hacer predicciones\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# PRECISION-RECALL CURVE - Clasificaci√≥n Binaria\n",
    "# ============================================\n",
    "# √ötil cuando hay desbalance de clases (muchos m√°s negativos que positivos)\n",
    "# Enfocada en la clase positiva (minoritaria)\n",
    "\n",
    "# Calcular Precision-Recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calcular Average Precision (AP) - resumen de PR curve\n",
    "ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "# Graficar PR Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {ap_score:.2f})')\n",
    "plt.axhline(y=np.mean(y_test), color='red', linestyle='--', \n",
    "           label=f'Baseline (prevalence = {np.mean(y_test):.2f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (Sensitivity)')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interpretaci√≥n:\n",
    "# AP cercano a 1: excelente\n",
    "# AP cercano a prevalencia de clase positiva: baseline\n",
    "print(f'Average Precision Score: {ap_score:.4f}')\n",
    "print(f'Baseline (prevalence): {np.mean(y_test):.4f}')\n",
    "\n",
    "# ============================================\n",
    "# PR CURVE - Usando PrecisionRecallDisplay\n",
    "# ============================================\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# M√©todo 1: Desde predicciones\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba, name='Modelo')\n",
    "plt.axhline(y=np.mean(y_test), color='red', linestyle='--', label='Baseline')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# M√©todo 2: Desde estimador\n",
    "display = PrecisionRecallDisplay.from_estimator(modelo, X_test, y_test, name='Modelo')\n",
    "plt.axhline(y=np.mean(y_test), color='red', linestyle='--', label='Baseline')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# COMPARAR M√öLTIPLES MODELOS - ROC\n",
    "# ============================================\n",
    "# Comparar ROC curves de diferentes modelos\n",
    "\n",
    "modelos_comparar = {\n",
    "    'Logistic Regression': modelo_lr,\n",
    "    'Random Forest': modelo_rf,\n",
    "    'SVM': modelo_svm\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "\n",
    "for nombre, modelo in modelos_comparar.items():\n",
    "    y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{nombre} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# COMPARAR M√öLTIPLES MODELOS - PR\n",
    "# ============================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "baseline = np.mean(y_test)\n",
    "plt.axhline(y=baseline, color='red', linestyle='--', lw=2, label=f'Baseline ({baseline:.2f})')\n",
    "\n",
    "for nombre, modelo in modelos_comparar.items():\n",
    "    y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{nombre} (AP = {ap:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Model Comparison')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ROC CURVE - Clasificaci√≥n Multiclase (One-vs-Rest)\n",
    "# ============================================\n",
    "# Binarizar las etiquetas para multiclase\n",
    "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Obtener probabilidades para todas las clases\n",
    "y_pred_proba_multi = modelo.predict_proba(X_test)\n",
    "\n",
    "# Calcular ROC curve y AUC para cada clase\n",
    "fpr_multi = dict()\n",
    "tpr_multi = dict()\n",
    "roc_auc_multi = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_multi[i], tpr_multi[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba_multi[:, i])\n",
    "    roc_auc_multi[i] = auc(fpr_multi[i], tpr_multi[i])\n",
    "\n",
    "# Calcular micro-average ROC curve (agregando todas las clases)\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), y_pred_proba_multi.ravel())\n",
    "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "# Graficar ROC curves para cada clase\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr_multi[i], tpr_multi[i], color=color, lw=2,\n",
    "            label=f'Clase {i} (AUC = {roc_auc_multi[i]:.2f})')\n",
    "\n",
    "plt.plot(fpr_micro, tpr_micro, color='deeppink', linestyle=':', lw=3,\n",
    "        label=f'Micro-average (AUC = {roc_auc_micro:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Multiclass (One-vs-Rest)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# AUC multiclase con sklearn (weighted average)\n",
    "roc_auc_weighted = roc_auc_score(y_test, y_pred_proba_multi, \n",
    "                                 multi_class='ovr', average='weighted')\n",
    "print(f'ROC AUC (weighted): {roc_auc_weighted:.4f}')\n",
    "\n",
    "# ============================================\n",
    "# PR CURVE - Clasificaci√≥n Multiclase\n",
    "# ============================================\n",
    "precision_multi = dict()\n",
    "recall_multi = dict()\n",
    "ap_multi = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision_multi[i], recall_multi[i], _ = precision_recall_curve(\n",
    "        y_test_bin[:, i], y_pred_proba_multi[:, i])\n",
    "    ap_multi[i] = average_precision_score(y_test_bin[:, i], y_pred_proba_multi[:, i])\n",
    "\n",
    "# Graficar PR curves para cada clase\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall_multi[i], precision_multi[i], color=color, lw=2,\n",
    "            label=f'Clase {i} (AP = {ap_multi[i]:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Multiclass')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZACI√ìN COMBINADA ROC + PR\n",
    "# ============================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "ax1.plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "ax2.plot(recall, precision, 'r-', lw=2, label=f'PR (AP = {ap_score:.2f})')\n",
    "ax2.axhline(y=np.mean(y_test), color='k', linestyle='--', lw=2, label='Baseline')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "ax2.legend(loc=\"best\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# AN√ÅLISIS DE THRESHOLDS\n",
    "# ============================================\n",
    "def analyze_thresholds(y_true, y_proba, thresholds_to_test=None):\n",
    "    \"\"\"Analiza m√©tricas para diferentes thresholds\"\"\"\n",
    "    if thresholds_to_test is None:\n",
    "        thresholds_to_test = np.linspace(0, 1, 21)\n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds_to_test:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'TPR': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "            'FPR': fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "# Analizar thresholds\n",
    "df_thresholds = analyze_thresholds(y_test, y_pred_proba)\n",
    "print(df_thresholds)\n",
    "\n",
    "# Graficar m√©tricas vs threshold\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(df_thresholds['Threshold'], df_thresholds['Accuracy'], 'b-', lw=2)\n",
    "axes[0, 0].set_title('Accuracy vs Threshold')\n",
    "axes[0, 0].set_xlabel('Threshold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(df_thresholds['Threshold'], df_thresholds['Precision'], 'r-', lw=2, label='Precision')\n",
    "axes[0, 1].plot(df_thresholds['Threshold'], df_thresholds['Recall'], 'g-', lw=2, label='Recall')\n",
    "axes[0, 1].set_title('Precision & Recall vs Threshold')\n",
    "axes[0, 1].set_xlabel('Threshold')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(df_thresholds['Threshold'], df_thresholds['F1'], 'm-', lw=2)\n",
    "axes[1, 0].set_title('F1 Score vs Threshold')\n",
    "axes[1, 0].set_xlabel('Threshold')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(df_thresholds['Threshold'], df_thresholds['TPR'], 'b-', lw=2, label='TPR')\n",
    "axes[1, 1].plot(df_thresholds['Threshold'], df_thresholds['FPR'], 'r-', lw=2, label='FPR')\n",
    "axes[1, 1].set_title('TPR & FPR vs Threshold')\n",
    "axes[1, 1].set_xlabel('Threshold')\n",
    "axes[1, 1].set_ylabel('Rate')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# TIPS Y MEJORES PR√ÅCTICAS\n",
    "# ============================================\n",
    "# 1. ROC Curve: usar cuando clases est√°n balanceadas\n",
    "# 2. PR Curve: usar cuando hay desbalance de clases (enfoque en clase positiva)\n",
    "# 3. AUC = 0.5 en ROC significa modelo no mejor que random\n",
    "# 4. En PR curve, baseline es la prevalencia de clase positiva\n",
    "# 5. Threshold √≥ptimo depende del contexto (costo de FP vs FN)\n",
    "# 6. Para multiclase: usar One-vs-Rest o One-vs-One\n",
    "# 7. Siempre visualizar ambas curves para entender mejor el modelo\n",
    "# 8. Average Precision es m√°s informativo que solo mirar la curva\n",
    "# 9. Micro-average: √∫til cuando clases tienen tama√±os diferentes\n",
    "# 10. Macro-average: trata todas las clases por igual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66f3ad",
   "metadata": {},
   "source": [
    "## 16. Metricas de Evaluacion - Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd36908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Ejemplo basico con regresion lineal\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Calcular RMSE, MAE, R2\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")\n",
    "\n",
    "# Graficar errores\n",
    "errores = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errores, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Histograma de Errores')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
