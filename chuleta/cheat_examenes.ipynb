{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cheat Sheet de Ex√°menes: C√≥digo Expandido y Analizado",
        "",
        "Este notebook contiene c√≥digo real extra√≠do de los ex√°menes (2022, 2023, 2024) y ejercicios de clase, con an√°lisis detallado de cada t√©cnica y patr√≥n usado.",
        "",
        "## üìã Contenido",
        "",
        "1. **Configuraci√≥n Inicial y Librer√≠as**",
        "2. **Carga y Exploraci√≥n de Datos**",
        "3. **Limpieza y Preparaci√≥n de Datos**",
        "4. **An√°lisis Exploratorio con Pandas**",
        "5. **Visualizaci√≥n de Datos**",
        "6. **Machine Learning: Regresi√≥n**",
        "7. **Machine Learning: Clasificaci√≥n**",
        "8. **Machine Learning: Clustering**",
        "9. **Patrones Completos de Examen**",
        "",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuraci√≥n Inicial y Librer√≠as",
        "",
        "### üéØ Objetivo",
        "Preparar el entorno de trabajo con todas las librer√≠as necesarias.",
        "",
        "### üìö An√°lisis",
        "Este es el patr√≥n est√°ndar que aparece en TODOS los ex√°menes y ejercicios. Es importante importar todas las librer√≠as al inicio para evitar errores de dependencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpiar entorno (com√∫n en notebooks)",
        "from IPython import get_ipython",
        "get_ipython().run_line_magic('reset', '-f')",
        "get_ipython().run_line_magic('clear', '-f')",
        "",
        "# Librer√≠as fundamentales",
        "import os",
        "import numpy as np",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import warnings",
        "warnings.filterwarnings('ignore')",
        "",
        "# Machine Learning",
        "from sklearn.model_selection import train_test_split, cross_val_score",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV",
        "from sklearn.cluster import KMeans, AgglomerativeClustering",
        "from sklearn.decomposition import PCA",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score",
        "from sklearn.metrics import precision_recall_curve, auc, silhouette_score",
        "",
        "# Estad√≠stica",
        "from scipy import stats",
        "from scipy.stats import pearsonr, spearmanr, ranksums",
        "from scipy.cluster.hierarchy import dendrogram, linkage",
        "",
        "# Configuraci√≥n de visualizaci√≥n",
        "sns.set_style('whitegrid')",
        "plt.rcParams['figure.figsize'] = (10, 6)",
        "plt.rcParams['font.size'] = 10",
        "",
        "print(\"‚úÖ Librer√≠as cargadas correctamente\")",
        "print(f\"üìÅ Directorio actual: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Notas Importantes",
        "- `warnings.filterwarnings('ignore')` evita mensajes molestos pero √∫salo con cuidado en producci√≥n",
        "- La configuraci√≥n de matplotlib/seaborn afecta TODOS los gr√°ficos posteriores",
        "- Verificar el directorio de trabajo es crucial para cargar archivos correctamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 2. Carga y Exploraci√≥n de Datos",
        "",
        "### üéØ Objetivo",
        "Cargar datos desde CSV y realizar exploraci√≥n inicial para entender la estructura.",
        "",
        "### üìä Patr√≥n del Examen 2022 - Pel√≠culas IMDb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo de carga de datos (Examen 2022)",
        "# Nota: Este es c√≥digo de ejemplo, adaptalo a tu archivo",
        "",
        "try:",
        "    # Intentar cargar datos",
        "    data_movies = pd.read_csv(\"IMDb_All_Genres_etf_clean1.csv\", sep=',')",
        "    print(\"‚úÖ Datos cargados correctamente\")",
        "    print(f\"Dimensiones: {data_movies.shape}\")",
        "except FileNotFoundError:",
        "    print(\"‚ö†Ô∏è Archivo no encontrado. Creando datos de ejemplo...\")",
        "    # Crear datos de ejemplo para demostraci√≥n",
        "    data_movies = pd.DataFrame({",
        "        'Title': ['Movie A', 'Movie B', 'Movie C', 'Movie D', 'Movie E'],",
        "        'Year': [2020, 2019, 2021, 2018, 2022],",
        "        'Total_Gross': ['$150.5M', '$89.3M', '$203.7M', '$45.2M', '$178.9M'],",
        "        'main_genre': ['Action', 'Drama', 'Action', 'Comedy', 'Drama'],",
        "        'Runtime': [120, 95, 135, 88, 102]",
        "    })",
        "    print(\"Datos de ejemplo creados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Exploraci√≥n Inicial Completa",
        "",
        "Este es el conjunto de comandos que SIEMPRE debes ejecutar al cargar datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploraci√≥n completa del dataset",
        "print(\"=\"*60)",
        "print(\"EXPLORACI√ìN INICIAL DE DATOS\")",
        "print(\"=\"*60)",
        "",
        "# 1. Dimensiones",
        "print(f\"\\nüìè Dimensiones: {data_movies.shape[0]} filas x {data_movies.shape[1]} columnas\")",
        "",
        "# 2. Primeras filas",
        "print(\"\\nüìã Primeras 3 filas:\")",
        "display(data_movies.head(3))",
        "",
        "# 3. Informaci√≥n de tipos y nulos",
        "print(\"\\nüìä Informaci√≥n de columnas:\")",
        "print(data_movies.info())",
        "",
        "# 4. Estad√≠sticas descriptivas",
        "print(\"\\nüìà Estad√≠sticas descriptivas:\")",
        "display(data_movies.describe())",
        "",
        "# 5. Valores nulos",
        "print(\"\\nüîç Valores nulos por columna:\")",
        "null_counts = data_movies.isnull().sum()",
        "null_percent = (data_movies.isnull().mean() * 100).round(2)",
        "null_df = pd.DataFrame({",
        "    'Nulos': null_counts,",
        "    'Porcentaje': null_percent",
        "})",
        "print(null_df[null_df['Nulos'] > 0])  # Solo mostrar columnas con nulos",
        "",
        "# 6. Tipos de datos",
        "print(\"\\nüî§ Tipos de datos:\")",
        "print(data_movies.dtypes)",
        "",
        "# 7. Valores √∫nicos en columnas categ√≥ricas",
        "print(\"\\nüéØ Valores √∫nicos:\")",
        "for col in data_movies.select_dtypes(include='object').columns[:3]:  # Primeras 3",
        "    print(f\"{col}: {data_movies[col].nunique()} valores √∫nicos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Patr√≥n del Examen 2023 - Quejas Bancarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo Examen 2023 - Selecci√≥n de columnas espec√≠ficas",
        "try:",
        "    bankAccounts = pd.read_csv('./bank_account_or_service_complaints.csv')",
        "    print(\"‚úÖ Datos bancarios cargados\")",
        "except:",
        "    print(\"‚ö†Ô∏è Creando datos de ejemplo...\")",
        "    bankAccounts = pd.DataFrame({",
        "        'product': ['Account', 'Credit', 'Account', 'Loan', 'Credit'],",
        "        'sub_product': ['Checking', 'Card', 'Savings', 'Personal', 'Card'],",
        "        'issue': ['Fee', 'Rate', 'Access', 'Payment', 'Fee'],",
        "        'company': ['Bank A', 'Bank B', 'Bank A', 'Bank C', 'Bank B'],",
        "        'state': ['CA', 'NY', 'CA', 'TX', 'NY'],",
        "        'submitted_via': ['Web', 'Phone', 'Web', 'Mail', 'Web'],",
        "        'timely_response': ['Yes', 'Yes', 'No', 'Yes', 'No'],",
        "        'consumer_disputed': ['No', 'Yes', 'No', 'No', 'Yes'],",
        "        'extra_col1': [1, 2, 3, 4, 5],",
        "        'extra_col2': ['a', 'b', 'c', 'd', 'e']",
        "    })",
        "",
        "# ‚≠ê Patr√≥n importante: Seleccionar solo columnas relevantes",
        "columnsToAnalyze = ['product', 'sub_product', 'issue', 'company', ",
        "                   'state', 'submitted_via', 'timely_response', ",
        "                   'consumer_disputed']",
        "",
        "# Crear DataFrame limpio solo con columnas de inter√©s",
        "bankAccounts_cleaned = bankAccounts[columnsToAnalyze].dropna()",
        "",
        "print(f\"\\nüìä Datos originales: {bankAccounts.shape}\")",
        "print(f\"‚ú® Datos limpios: {bankAccounts_cleaned.shape}\")",
        "print(f\"üóëÔ∏è Filas eliminadas: {bankAccounts.shape[0] - bankAccounts_cleaned.shape[0]}\")",
        "",
        "display(bankAccounts_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° An√°lisis Pedag√≥gico",
        "",
        "**Por qu√© seleccionar columnas espec√≠ficas:**",
        "- Reduce memoria y procesamiento",
        "- Elimina columnas irrelevantes para el an√°lisis",
        "- Facilita la visualizaci√≥n y comprensi√≥n",
        "",
        "**Por qu√© usar `.copy()`:**",
        "```python",
        "df_new = df[columnas].copy()  # ‚úÖ Correcto: crea copia independiente",
        "df_new = df[columnas]          # ‚ö†Ô∏è Puede causar SettingWithCopyWarning",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 3. Limpieza y Preparaci√≥n de Datos",
        "",
        "### üßπ Conversi√≥n de Tipos de Datos",
        "",
        "#### Patr√≥n del Examen 2022: Limpiar texto con s√≠mbolos monetarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ejemplo con datos monetarios como texto",
        "df_example = pd.DataFrame({",
        "    'Title': ['Movie A', 'Movie B', 'Movie C'],",
        "    'Total_Gross': ['$150.5M', '$89.3M', '$203.7M']",
        "})",
        "",
        "print(\"üî¥ ANTES de la conversi√≥n:\")",
        "print(df_example)",
        "print(f\"Tipo de 'Total_Gross': {df_example['Total_Gross'].dtype}\\n\")",
        "",
        "# ‚≠ê Conversi√≥n de texto monetario a n√∫mero",
        "# Paso 1: Eliminar '$' y 'M'",
        "df_example['Total_Gross_Clean'] = df_example['Total_Gross'].str.replace('$', '')",
        "df_example['Total_Gross_Clean'] = df_example['Total_Gross_Clean'].str.replace('M', '')",
        "",
        "# Paso 2: Convertir a float",
        "df_example['Total_Gross_Clean'] = df_example['Total_Gross_Clean'].astype(float)",
        "",
        "print(\"üü¢ DESPU√âS de la conversi√≥n:\")",
        "print(df_example)",
        "print(f\"Tipo de 'Total_Gross_Clean': {df_example['Total_Gross_Clean'].dtype}\")",
        "",
        "# Ahora podemos hacer operaciones num√©ricas",
        "print(f\"\\nüìä Total combinado: ${df_example['Total_Gross_Clean'].sum():.1f}M\")",
        "print(f\"üìä Promedio: ${df_example['Total_Gross_Clean'].mean():.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Patr√≥n del Examen 2024: Eliminar columnas con muchos nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset de ejemplo con valores nulos",
        "np.random.seed(42)",
        "df_missing = pd.DataFrame({",
        "    'col_completa': range(100),",
        "    'col_pocos_nulos': [np.nan if i % 10 == 0 else i for i in range(100)],",
        "    'col_muchos_nulos': [np.nan if i % 2 == 0 else i for i in range(100)],",
        "    'col_casi_todo_nulo': [np.nan if i < 80 else i for i in range(100)],",
        "    'target': np.random.rand(100)",
        "})",
        "",
        "print(\"üìä Dataset original:\")",
        "print(f\"Shape: {df_missing.shape}\")",
        "print(\"\\nüîç Porcentaje de valores nulos por columna:\")",
        "percent_missing = df_missing.isnull().mean() * 100",
        "print(percent_missing.sort_values(ascending=False))",
        "",
        "# ‚≠ê ESTRATEGIA DEL EXAMEN 2024",
        "# Paso 1: Eliminar columnas con >25% de nulos",
        "threshold = 25",
        "selected_var = percent_missing < threshold",
        "df_clean = df_missing.loc[:, selected_var]",
        "",
        "print(f\"\\n‚úÇÔ∏è Columnas eliminadas: {df_missing.shape[1] - df_clean.shape[1]}\")",
        "print(f\"Columnas restantes: {list(df_clean.columns)}\")",
        "",
        "# Paso 2: Eliminar filas con cualquier nulo restante",
        "df_clean = df_clean.dropna()",
        "",
        "print(f\"\\n‚úÖ Dataset final:\")",
        "print(f\"Shape: {df_clean.shape}\")",
        "print(f\"Filas eliminadas: {df_missing.shape[0] - df_clean.shape[0]}\")",
        "print(f\"Valores nulos restantes: {df_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Estrategias de Limpieza",
        "",
        "**Decisi√≥n: ¬ødropna() o fillna()?**",
        "",
        "```python",
        "# Opci√≥n 1: Eliminar (cuando hay pocos nulos)",
        "df_clean = df.dropna()",
        "",
        "# Opci√≥n 2: Rellenar con estad√≠stica",
        "df_clean = df.fillna(df.mean())      # Para num√©ricas",
        "df_clean = df.fillna(df.mode()[0])   # Para categ√≥ricas",
        "",
        "# Opci√≥n 3: Rellenar con valor espec√≠fico",
        "df_clean = df.fillna(0)",
        "df_clean = df.fillna('Desconocido')",
        "",
        "# Opci√≥n 4: Forward/Backward fill (series temporales)",
        "df_clean = df.fillna(method='ffill')",
        "```",
        "",
        "**¬øCu√°ndo usar cada una?**",
        "- **dropna()**: < 5% de datos nulos",
        "- **fillna(mean/median)**: Datos num√©ricos, distribuci√≥n normal",
        "- **fillna(mode)**: Datos categ√≥ricos",
        "- **fillna(0)**: Cuando 0 tiene significado (ej: sin compras = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 4. An√°lisis Exploratorio con Pandas",
        "",
        "### üîç GroupBy - La t√©cnica m√°s importante",
        "",
        "#### Patr√≥n b√°sico: Agrupar y agregar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset de ejemplo para demostraci√≥n",
        "df_sales = pd.DataFrame({",
        "    'categoria': ['Electr√≥nica', 'Ropa', 'Electr√≥nica', 'Comida', 'Ropa', ",
        "                  'Electr√≥nica', 'Comida', 'Ropa', 'Comida', 'Electr√≥nica'] * 5,",
        "    'producto': ['TV', 'Camisa', 'M√≥vil', 'Pan', 'Pantal√≥n', ",
        "                 'Laptop', 'Leche', 'Zapatos', 'Fruta', 'Auriculares'] * 5,",
        "    'ventas': np.random.randint(100, 1000, 50),",
        "    'unidades': np.random.randint(1, 20, 50),",
        "    'a√±o': np.random.choice([2022, 2023, 2024], 50),",
        "    'mes': np.random.randint(1, 13, 50)",
        "})",
        "",
        "print(\"üìä Dataset de ejemplo:\")",
        "display(df_sales.head())",
        "",
        "print(\"\\n\" + \"=\"*60)",
        "print(\"EJEMPLOS DE GROUPBY\")",
        "print(\"=\"*60)",
        "",
        "# 1. Agrupar por una columna, una agregaci√≥n",
        "print(\"\\n1Ô∏è‚É£ Ventas totales por categor√≠a:\")",
        "ventas_categoria = df_sales.groupby('categoria')['ventas'].sum().sort_values(ascending=False)",
        "print(ventas_categoria)",
        "",
        "# 2. M√∫ltiples agregaciones",
        "print(\"\\n2Ô∏è‚É£ Estad√≠sticas por categor√≠a:\")",
        "stats_categoria = df_sales.groupby('categoria')['ventas'].agg(['sum', 'mean', 'count', 'std'])",
        "print(stats_categoria.round(2))",
        "",
        "# 3. Agrupar por m√∫ltiples columnas",
        "print(\"\\n3Ô∏è‚É£ Ventas por categor√≠a y a√±o:\")",
        "ventas_cat_a√±o = df_sales.groupby(['categoria', 'a√±o'])['ventas'].sum().unstack(fill_value=0)",
        "print(ventas_cat_a√±o)",
        "",
        "# 4. M√∫ltiples columnas, m√∫ltiples agregaciones",
        "print(\"\\n4Ô∏è‚É£ An√°lisis completo por categor√≠a:\")",
        "analisis_completo = df_sales.groupby('categoria').agg({",
        "    'ventas': ['sum', 'mean'],",
        "    'unidades': ['sum', 'mean'],",
        "    'producto': 'count'",
        "})",
        "print(analisis_completo.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Patr√≥n del Examen 2023: Top N con value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo: Encontrar las 5 compa√±√≠as con m√°s quejas",
        "df_complaints = pd.DataFrame({",
        "    'company': ['Bank A', 'Bank B', 'Bank C', 'Bank A', 'Bank B', ",
        "                'Bank A', 'Bank D', 'Bank B', 'Bank C', 'Bank A'] * 10,",
        "    'issue': np.random.choice(['Fee', 'Rate', 'Access', 'Payment'], 100)",
        "})",
        "",
        "print(\"üéØ Top 5 compa√±√≠as con m√°s quejas:\")",
        "print(\"\\nM√©todo 1: value_counts() - M√ÅS COM√öN\")",
        "top5_companies = df_complaints['company'].value_counts().head(5)",
        "print(top5_companies)",
        "",
        "print(\"\\nM√©todo 2: groupby + size()\")",
        "top5_alt = df_complaints.groupby('company').size().sort_values(ascending=False).head(5)",
        "print(top5_alt)",
        "",
        "print(\"\\nM√©todo 3: groupby + count()\")",
        "top5_alt2 = df_complaints.groupby('company')['issue'].count().sort_values(ascending=False).head(5)",
        "print(top5_alt2)",
        "",
        "print(\"\\nüí° Los tres m√©todos dan el mismo resultado, pero value_counts() es m√°s directo\")",
        "",
        "# Filtrar DataFrame para incluir solo top 5 companies",
        "top5_names = top5_companies.index.tolist()",
        "df_top5 = df_complaints[df_complaints['company'].isin(top5_names)]",
        "",
        "print(f\"\\n‚úÖ Dataset filtrado: {df_complaints.shape[0]} ‚Üí {df_top5.shape[0]} filas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Pivot Tables - Reorganizar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pivot table para an√°lisis cruzado",
        "print(\"üìä Pivot Table: Quejas por compa√±√≠a y tipo de problema\")",
        "",
        "pivot_analysis = pd.crosstab(",
        "    df_top5['company'], ",
        "    df_top5['issue'], ",
        "    margins=True,  # A√±ade totales",
        "    margins_name='Total'",
        ")",
        "",
        "print(pivot_analysis)",
        "",
        "print(\"\\nüí° pd.crosstab es √∫til para ver frecuencias cruzadas\")",
        "print(\"Para valores agregados, usa pd.pivot_table:\")",
        "",
        "# Ejemplo con pivot_table",
        "df_sales_pivot = df_sales.pivot_table(",
        "    values='ventas',",
        "    index='categoria',",
        "    columns='a√±o',",
        "    aggfunc='sum',",
        "    fill_value=0",
        ")",
        "",
        "print(\"\\nüìä Ventas por categor√≠a y a√±o:\")",
        "print(df_sales_pivot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 5. Visualizaci√≥n de Datos",
        "",
        "### üìä Barplots - Lo m√°s usado en ex√°menes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para visualizaci√≥n",
        "plot_data = df_sales.groupby('categoria')['ventas'].sum().sort_values(ascending=False)",
        "",
        "print(\"üé® Ejemplos de Barplots\\n\")",
        "",
        "# 1. Barplot simple con Matplotlib",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
        "",
        "# Matplotlib",
        "plot_data.plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')",
        "axes[0].set_title('Ventas por Categor√≠a (Matplotlib)', fontsize=14, fontweight='bold')",
        "axes[0].set_xlabel('Categor√≠a', fontsize=12)",
        "axes[0].set_ylabel('Ventas Totales', fontsize=12)",
        "axes[0].tick_params(axis='x', rotation=45)",
        "axes[0].grid(axis='y', alpha=0.3)",
        "",
        "# Seaborn",
        "sns.barplot(x=plot_data.index, y=plot_data.values, ax=axes[1], palette='viridis')",
        "axes[1].set_title('Ventas por Categor√≠a (Seaborn)', fontsize=14, fontweight='bold')",
        "axes[1].set_xlabel('Categor√≠a', fontsize=12)",
        "axes[1].set_ylabel('Ventas Totales', fontsize=12)",
        "axes[1].tick_params(axis='x', rotation=45)",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"üí° Ambas librer√≠as funcionan, pero Seaborn da mejor est√©tica por defecto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Patr√≥n del Examen 2023: Barplot con hue (agrupaci√≥n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simular datos del examen 2023: quejas por compa√±√≠a y medio de env√≠o",
        "df_plot_exam = pd.DataFrame({",
        "    'company': ['Bank A', 'Bank A', 'Bank A', 'Bank B', 'Bank B', 'Bank B',",
        "                'Bank C', 'Bank C', 'Bank C'] * 5,",
        "    'submitted_via': ['Web', 'Phone', 'Mail', 'Web', 'Phone', 'Mail',",
        "                      'Web', 'Phone', 'Mail'] * 5,",
        "    'count': np.random.randint(10, 50, 45)",
        "})",
        "",
        "# Agregar datos",
        "plot_data_grouped = df_plot_exam.groupby(['company', 'submitted_via'])['count'].sum().reset_index()",
        "",
        "# ‚≠ê Patr√≥n del Examen 2023",
        "plt.figure(figsize=(12, 6))",
        "sns.barplot(",
        "    data=plot_data_grouped,",
        "    x='company',",
        "    y='count',",
        "    hue='submitted_via',  # ‚≠ê Esto crea grupos dentro de cada categor√≠a",
        "    palette='Set2'",
        ")",
        "plt.title('Quejas por Compa√±√≠a y Medio de Env√≠o', fontsize=14, fontweight='bold')",
        "plt.xlabel('Compa√±√≠a', fontsize=12)",
        "plt.ylabel('N√∫mero de Quejas', fontsize=12)",
        "plt.xticks(rotation=45, ha='right')",
        "plt.legend(title='Medio de Env√≠o', title_fontsize=11)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"ÔøΩÔøΩ El par√°metro 'hue' es clave para comparar subgrupos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìà Scatter Plots y Correlaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear datos con correlaci√≥n",
        "np.random.seed(42)",
        "x = np.random.rand(100) * 100",
        "y = 2 * x + np.random.randn(100) * 20  # Relaci√≥n lineal con ruido",
        "z = np.random.rand(100) * 100  # Sin correlaci√≥n con x",
        "",
        "df_corr = pd.DataFrame({'Variable_X': x, 'Variable_Y': y, 'Variable_Z': z})",
        "",
        "# Calcular correlaciones",
        "corr_xy = df_corr['Variable_X'].corr(df_corr['Variable_Y'])",
        "corr_xz = df_corr['Variable_X'].corr(df_corr['Variable_Z'])",
        "",
        "# Visualizar",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
        "",
        "# Scatter con correlaci√≥n fuerte",
        "axes[0].scatter(df_corr['Variable_X'], df_corr['Variable_Y'], ",
        "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)",
        "axes[0].set_title(f'X vs Y (Correlaci√≥n: {corr_xy:.3f})', fontsize=13)",
        "axes[0].set_xlabel('Variable X')",
        "axes[0].set_ylabel('Variable Y')",
        "axes[0].grid(True, alpha=0.3)",
        "",
        "# Scatter sin correlaci√≥n",
        "axes[1].scatter(df_corr['Variable_X'], df_corr['Variable_Z'], ",
        "                alpha=0.6, s=50, color='coral', edgecolors='black', linewidth=0.5)",
        "axes[1].set_title(f'X vs Z (Correlaci√≥n: {corr_xz:.3f})', fontsize=13)",
        "axes[1].set_xlabel('Variable X')",
        "axes[1].set_ylabel('Variable Z')",
        "axes[1].grid(True, alpha=0.3)",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "# Matriz de correlaci√≥n completa",
        "print(\"\\nüìä Matriz de Correlaci√≥n:\")",
        "correlation_matrix = df_corr.corr()",
        "print(correlation_matrix.round(3))",
        "",
        "# Heatmap de correlaci√≥n",
        "plt.figure(figsize=(8, 6))",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})",
        "plt.title('Matriz de Correlaci√≥n', fontsize=14, fontweight='bold')",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 6. Machine Learning: Regresi√≥n",
        "",
        "### üéØ Workflow Completo - Patr√≥n del Examen 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset sint√©tico para regresi√≥n",
        "np.random.seed(42)",
        "n_samples = 200",
        "",
        "df_ml = pd.DataFrame({",
        "    'feature1': np.random.rand(n_samples) * 100,",
        "    'feature2': np.random.rand(n_samples) * 50,",
        "    'feature3': np.random.rand(n_samples) * 30,",
        "    'feature4': np.random.choice(['A', 'B', 'C'], n_samples),",
        "    'target': np.random.rand(n_samples) * 1000",
        "})",
        "",
        "# Hacer target dependiente de features (crear relaci√≥n real)",
        "df_ml['target'] = (df_ml['feature1'] * 5 + ",
        "                   df_ml['feature2'] * 3 + ",
        "                   df_ml['feature3'] * 2 + ",
        "                   np.random.randn(n_samples) * 50)",
        "",
        "print(\"üìä Dataset para Machine Learning:\")",
        "print(f\"Shape: {df_ml.shape}\")",
        "display(df_ml.head())",
        "print(f\"\\nCorrelaciones con target:\")",
        "print(df_ml.corr()['target'].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 1: Preparar X e y",
        "print(\"=\"*60)",
        "print(\"PASO 1: PREPARACI√ìN DE DATOS\")",
        "print(\"=\"*60)",
        "",
        "# Codificar variable categ√≥rica",
        "df_ml_encoded = pd.get_dummies(df_ml, columns=['feature4'], drop_first=True)",
        "",
        "print(f\"\\n‚úÖ Variables categ√≥ricas codificadas\")",
        "print(f\"Columnas: {list(df_ml_encoded.columns)}\")",
        "",
        "# Separar features y target",
        "X = df_ml_encoded.drop('target', axis=1)",
        "y = df_ml_encoded['target']",
        "",
        "print(f\"\\nüìä X shape: {X.shape}\")",
        "print(f\"üìä y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 2: Train-Test Split",
        "print(\"=\"*60)",
        "print(\"PASO 2: DIVIDIR EN TRAIN Y TEST\")",
        "print(\"=\"*60)",
        "",
        "X_train, X_test, y_train, y_test = train_test_split(",
        "    X, y, ",
        "    test_size=0.2,    # 20% para test",
        "    random_state=42   # Para reproducibilidad",
        ")",
        "",
        "print(f\"\\n‚úÖ Divisi√≥n completada:\")",
        "print(f\"Train: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.0f}%)\")",
        "print(f\"Test:  {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 3: Estandarizaci√≥n",
        "print(\"=\"*60)",
        "print(\"PASO 3: ESTANDARIZACI√ìN\")",
        "print(\"=\"*60)",
        "",
        "scaler = StandardScaler()",
        "",
        "# ‚ö†Ô∏è IMPORTANTE: fit_transform solo en train, transform en test",
        "X_train_scaled = scaler.fit_transform(X_train)",
        "X_test_scaled = scaler.transform(X_test)",
        "",
        "print(f\"\\n‚úÖ Estandarizaci√≥n completada\")",
        "print(f\"\\nMedia antes (train): {X_train.mean(axis=0).round(2).tolist()}\")",
        "print(f\"Media despu√©s (train): {X_train_scaled.mean(axis=0).round(2).tolist()}\")",
        "print(f\"\\nüí° Despu√©s de estandarizar, media ‚âà 0 y std ‚âà 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 4: Entrenar modelo - Elastic Net con CV",
        "print(\"=\"*60)",
        "print(\"PASO 4: ENTRENAR MODELO\")",
        "print(\"=\"*60)",
        "",
        "# ElasticNetCV encuentra autom√°ticamente el mejor alpha",
        "model = ElasticNetCV(",
        "    cv=5,              # 5-fold cross-validation",
        "    random_state=42,",
        "    max_iter=10000     # Asegurar convergencia",
        ")",
        "",
        "print(\"\\nüîÑ Entrenando modelo con validaci√≥n cruzada...\")",
        "model.fit(X_train_scaled, y_train)",
        "",
        "print(f\"\\n‚úÖ Modelo entrenado\")",
        "print(f\"Mejor alpha encontrado: {model.alpha_:.4f}\")",
        "print(f\"N√∫mero de features usadas: {np.sum(model.coef_ != 0)}/{len(model.coef_)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 5: Predecir",
        "print(\"=\"*60)",
        "print(\"PASO 5: HACER PREDICCIONES\")",
        "print(\"=\"*60)",
        "",
        "# Predecir en train y test",
        "y_train_pred = model.predict(X_train_scaled)",
        "y_test_pred = model.predict(X_test_scaled)",
        "",
        "print(\"\\n‚úÖ Predicciones generadas\")",
        "print(f\"Train predictions: {y_train_pred.shape}\")",
        "print(f\"Test predictions: {y_test_pred.shape}\")",
        "",
        "# Mostrar algunas predicciones vs reales",
        "print(\"\\nüìä Ejemplo de predicciones vs valores reales (Test):\")",
        "comparison = pd.DataFrame({",
        "    'Real': y_test.values[:10],",
        "    'Predicho': y_test_pred[:10],",
        "    'Error': np.abs(y_test.values[:10] - y_test_pred[:10])",
        "})",
        "print(comparison.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 6: Evaluar modelo",
        "print(\"=\"*60)",
        "print(\"PASO 6: EVALUACI√ìN DEL MODELO\")",
        "print(\"=\"*60)",
        "",
        "from sklearn.metrics import mean_absolute_error, explained_variance_score",
        "",
        "# Calcular m√©tricas",
        "train_r2 = r2_score(y_train, y_train_pred)",
        "test_r2 = r2_score(y_test, y_test_pred)",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))",
        "train_mae = mean_absolute_error(y_train, y_train_pred)",
        "test_mae = mean_absolute_error(y_test, y_test_pred)",
        "explained_var = explained_variance_score(y_test, y_test_pred)",
        "",
        "print(\"\\nüìä M√âTRICAS DE EVALUACI√ìN:\")",
        "print(\"-\" * 60)",
        "print(f\"{'M√©trica':<25} {'Train':<15} {'Test':<15}\")",
        "print(\"-\" * 60)",
        "print(f\"{'R¬≤ Score':<25} {train_r2:>14.4f} {test_r2:>14.4f}\")",
        "print(f\"{'RMSE':<25} {train_rmse:>14.2f} {test_rmse:>14.2f}\")",
        "print(f\"{'MAE':<25} {train_mae:>14.2f} {test_mae:>14.2f}\")",
        "print(f\"{'Explained Variance':<25} {'-':>14} {explained_var:>14.4f}\")",
        "print(\"-\" * 60)",
        "",
        "# Interpretaci√≥n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")",
        "if test_r2 > 0.8:",
        "    print(\"‚úÖ Excelente ajuste (R¬≤ > 0.8)\")",
        "elif test_r2 > 0.6:",
        "    print(\"üëç Buen ajuste (R¬≤ > 0.6)\")",
        "elif test_r2 > 0.4:",
        "    print(\"‚ö†Ô∏è Ajuste moderado (R¬≤ > 0.4)\")",
        "else:",
        "    print(\"‚ùå Ajuste pobre (R¬≤ < 0.4)\")",
        "",
        "if abs(train_r2 - test_r2) > 0.1:",
        "    print(\"‚ö†Ô∏è Posible overfitting (gran diferencia train-test)\")",
        "else:",
        "    print(\"‚úÖ No hay signos de overfitting\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 7: Visualizar resultados",
        "print(\"=\"*60)",
        "print(\"PASO 7: VISUALIZACI√ìN DE RESULTADOS\")",
        "print(\"=\"*60)",
        "",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
        "",
        "# Plot 1: Predicciones vs Reales",
        "axes[0].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='black', linewidth=0.5)",
        "axes[0].plot([y_test.min(), y_test.max()], ",
        "             [y_test.min(), y_test.max()], ",
        "             'r--', lw=2, label='Predicci√≥n perfecta')",
        "axes[0].set_xlabel('Valores Reales', fontsize=12)",
        "axes[0].set_ylabel('Predicciones', fontsize=12)",
        "axes[0].set_title(f'Predicciones vs Reales (R¬≤ = {test_r2:.3f})', ",
        "                  fontsize=13, fontweight='bold')",
        "axes[0].legend()",
        "axes[0].grid(True, alpha=0.3)",
        "",
        "# Plot 2: Distribuci√≥n de residuos",
        "residuals = y_test - y_test_pred",
        "axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)",
        "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Residuo = 0')",
        "axes[1].set_xlabel('Residuo (Real - Predicho)', fontsize=12)",
        "axes[1].set_ylabel('Frecuencia', fontsize=12)",
        "axes[1].set_title('Distribuci√≥n de Residuos', fontsize=13, fontweight='bold')",
        "axes[1].legend()",
        "axes[1].grid(True, alpha=0.3, axis='y')",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\nüí° INTERPRETACI√ìN DE GR√ÅFICOS:\")",
        "print(\"- Scatter plot: Puntos cerca de la l√≠nea roja = buenas predicciones\")",
        "print(\"- Histograma: Debe estar centrado en 0 y tener forma de campana\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 7. Machine Learning: Clasificaci√≥n",
        "",
        "### üéØ Regresi√≥n Log√≠stica - Patr√≥n del Examen 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset de clasificaci√≥n binaria",
        "np.random.seed(42)",
        "from sklearn.datasets import make_classification",
        "",
        "X_class, y_class = make_classification(",
        "    n_samples=300,",
        "    n_features=5,",
        "    n_informative=3,",
        "    n_redundant=1,",
        "    n_classes=2,",
        "    random_state=42",
        ")",
        "",
        "# Convertir a DataFrame",
        "df_classification = pd.DataFrame(",
        "    X_class,",
        "    columns=[f'feature_{i+1}' for i in range(5)]",
        ")",
        "df_classification['target'] = y_class",
        "",
        "print(\"üìä Dataset de Clasificaci√≥n:\")",
        "print(f\"Shape: {df_classification.shape}\")",
        "print(f\"\\nDistribuci√≥n de clases:\")",
        "print(df_classification['target'].value_counts())",
        "print(f\"\\nBalance: {df_classification['target'].value_counts(normalize=True).round(3).tolist()}\")",
        "",
        "display(df_classification.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para clasificaci√≥n",
        "X_clf = df_classification.drop('target', axis=1)",
        "y_clf = df_classification['target']",
        "",
        "# Train-test split",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(",
        "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf",
        ")",
        "",
        "# Estandarizar",
        "scaler_clf = StandardScaler()",
        "X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)",
        "X_test_clf_scaled = scaler_clf.transform(X_test_clf)",
        "",
        "print(\"‚úÖ Datos preparados para clasificaci√≥n\")",
        "print(f\"Train: {X_train_clf_scaled.shape}, Test: {X_test_clf_scaled.shape}\")",
        "print(f\"\\nDistribuci√≥n en train:\")",
        "print(pd.Series(y_train_clf).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê Entrenar Logistic Regression con CV (Patr√≥n Examen 2023)",
        "print(\"=\"*60)",
        "print(\"REGRESI√ìN LOG√çSTICA CON VALIDACI√ìN CRUZADA\")",
        "print(\"=\"*60)",
        "",
        "# LogisticRegressionCV encuentra el mejor C autom√°ticamente",
        "lr_model = LogisticRegressionCV(",
        "    cv=5,",
        "    random_state=42,",
        "    max_iter=10000,",
        "    scoring='accuracy'",
        ")",
        "",
        "print(\"\\nüîÑ Entrenando modelo...\")",
        "lr_model.fit(X_train_clf_scaled, y_train_clf)",
        "",
        "print(f\"\\n‚úÖ Modelo entrenado\")",
        "print(f\"Mejor C encontrado: {lr_model.C_[0]:.4f}\")",
        "",
        "# Predecir",
        "y_train_pred_clf = lr_model.predict(X_train_clf_scaled)",
        "y_test_pred_clf = lr_model.predict(X_test_clf)",
        "",
        "# Obtener probabilidades (importante para curvas ROC y PR)",
        "y_train_proba = lr_model.predict_proba(X_train_clf_scaled)[:, 1]",
        "y_test_proba = lr_model.predict_proba(X_test_clf_scaled)[:, 1]",
        "",
        "print(\"\\nüìä Predicciones generadas\")",
        "print(f\"Clases √∫nicas predichas: {np.unique(y_test_pred_clf)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar clasificaci√≥n",
        "from sklearn.metrics import confusion_matrix, classification_report",
        "",
        "print(\"=\"*60)",
        "print(\"EVALUACI√ìN DEL MODELO DE CLASIFICACI√ìN\")",
        "print(\"=\"*60)",
        "",
        "# Accuracy",
        "train_acc = accuracy_score(y_train_clf, y_train_pred_clf)",
        "test_acc = accuracy_score(y_test_clf, y_test_pred_clf)",
        "",
        "print(f\"\\nüìä Accuracy:\")",
        "print(f\"Train: {train_acc:.4f}\")",
        "print(f\"Test:  {test_acc:.4f}\")",
        "",
        "# Matriz de confusi√≥n",
        "print(\"\\nüìä Matriz de Confusi√≥n (Test):\")",
        "cm = confusion_matrix(y_test_clf, y_test_pred_clf)",
        "print(cm)",
        "print(\"\\n    [[TN  FP]\")",
        "print(\"     [FN  TP]]\")",
        "",
        "# Classification report",
        "print(\"\\nüìä Reporte de Clasificaci√≥n (Test):\")",
        "print(classification_report(y_test_clf, y_test_pred_clf, ",
        "                          target_names=['Clase 0', 'Clase 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ Curva Precision-Recall (Patr√≥n Examen 2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê Curva Precision-Recall y AUC (Patr√≥n del Examen 2023)",
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score",
        "",
        "# Calcular precision-recall curve",
        "precision, recall, thresholds_pr = precision_recall_curve(y_test_clf, y_test_proba)",
        "auc_pr = auc(recall, precision)",
        "",
        "# Calcular ROC curve",
        "fpr, tpr, thresholds_roc = roc_curve(y_test_clf, y_test_proba)",
        "auc_roc = roc_auc_score(y_test_clf, y_test_proba)",
        "",
        "print(f\"üìä AUC Precision-Recall: {auc_pr:.4f}\")",
        "print(f\"üìä AUC ROC: {auc_roc:.4f}\")",
        "",
        "# Visualizar",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
        "",
        "# Precision-Recall Curve",
        "axes[0].plot(recall, precision, linewidth=2, label=f'AUC = {auc_pr:.3f}')",
        "axes[0].set_xlabel('Recall', fontsize=12)",
        "axes[0].set_ylabel('Precision', fontsize=12)",
        "axes[0].set_title('Curva Precision-Recall', fontsize=13, fontweight='bold')",
        "axes[0].legend()",
        "axes[0].grid(True, alpha=0.3)",
        "axes[0].set_xlim([0, 1])",
        "axes[0].set_ylim([0, 1])",
        "",
        "# ROC Curve",
        "axes[1].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc_roc:.3f}')",
        "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Azar')",
        "axes[1].set_xlabel('False Positive Rate', fontsize=12)",
        "axes[1].set_ylabel('True Positive Rate', fontsize=12)",
        "axes[1].set_title('Curva ROC', fontsize=13, fontweight='bold')",
        "axes[1].legend()",
        "axes[1].grid(True, alpha=0.3)",
        "axes[1].set_xlim([0, 1])",
        "axes[1].set_ylim([0, 1])",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\nüí° INTERPRETACI√ìN:\")",
        "print(f\"- AUC = 1.0: Clasificador perfecto\")",
        "print(f\"- AUC = 0.5: Clasificador aleatorio\")",
        "print(f\"- Tu modelo: AUC = {auc_roc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 8. Machine Learning: Clustering",
        "",
        "### üéØ K-Means - Patr√≥n del Examen 2024",
        "",
        "El clustering es aprendizaje NO supervisado: no tenemos etiquetas, queremos descubrir grupos naturales en los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset para clustering",
        "np.random.seed(42)",
        "from sklearn.datasets import make_blobs",
        "",
        "X_cluster, y_true = make_blobs(",
        "    n_samples=300,",
        "    n_features=4,",
        "    centers=3,",
        "    cluster_std=1.0,",
        "    random_state=42",
        ")",
        "",
        "df_cluster = pd.DataFrame(",
        "    X_cluster,",
        "    columns=['feature1', 'feature2', 'feature3', 'feature4']",
        ")",
        "",
        "print(\"üìä Dataset para Clustering:\")",
        "print(f\"Shape: {df_cluster.shape}\")",
        "display(df_cluster.head())",
        "print(\"\\nüí° No hay columna 'target' - es aprendizaje no supervisado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 1: Estandarizar (CRUCIAL para K-Means)",
        "print(\"=\"*60)",
        "print(\"PASO 1: ESTANDARIZACI√ìN\")",
        "print(\"=\"*60)",
        "",
        "scaler_cluster = StandardScaler()",
        "X_cluster_scaled = scaler_cluster.fit_transform(df_cluster)",
        "",
        "print(\"\\n‚úÖ Datos estandarizados\")",
        "print(f\"Shape: {X_cluster_scaled.shape}\")",
        "print(f\"Media por feature: {X_cluster_scaled.mean(axis=0).round(4)}\")",
        "print(f\"Std por feature: {X_cluster_scaled.std(axis=0).round(4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 2: M√©todo del Codo - Encontrar K √≥ptimo",
        "print(\"=\"*60)",
        "print(\"PASO 2: DETERMINAR N√öMERO √ìPTIMO DE CLUSTERS\")",
        "print(\"=\"*60)",
        "",
        "inertias = []",
        "silhouette_scores_list = []",
        "K_range = range(2, 11)",
        "",
        "print(\"\\nüîÑ Probando diferentes valores de K...\")",
        "for k in K_range:",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)",
        "    clusters_temp = kmeans_temp.fit_predict(X_cluster_scaled)",
        "    ",
        "    inertias.append(kmeans_temp.inertia_)",
        "    silhouette_scores_list.append(silhouette_score(X_cluster_scaled, clusters_temp))",
        "    ",
        "    print(f\"K={k}: Inercia={kmeans_temp.inertia_:.2f}, \"",
        "          f\"Silhouette={silhouette_score(X_cluster_scaled, clusters_temp):.3f}\")",
        "",
        "print(\"\\n‚úÖ An√°lisis completado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar m√©todo del codo y silhouette score",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))",
        "",
        "# M√©todo del Codo",
        "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)",
        "axes[0].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)",
        "axes[0].set_ylabel('Inercia (Within-cluster sum of squares)', fontsize=12)",
        "axes[0].set_title('M√©todo del Codo', fontsize=13, fontweight='bold')",
        "axes[0].grid(True, alpha=0.3)",
        "axes[0].set_xticks(K_range)",
        "",
        "# Silhouette Score",
        "axes[1].plot(K_range, silhouette_scores_list, 'ro-', linewidth=2, markersize=8)",
        "axes[1].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)",
        "axes[1].set_ylabel('Silhouette Score', fontsize=12)",
        "axes[1].set_title('Silhouette Score por K', fontsize=13, fontweight='bold')",
        "axes[1].grid(True, alpha=0.3)",
        "axes[1].set_xticks(K_range)",
        "axes[1].axhline(y=0.5, color='g', linestyle='--', alpha=0.5, label='Threshold 0.5')",
        "axes[1].legend()",
        "",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "# Determinar K √≥ptimo",
        "k_optimal = K_range[np.argmax(silhouette_scores_list)]",
        "print(f\"\\nüí° K √≥ptimo sugerido (max silhouette): {k_optimal}\")",
        "print(f\"   Silhouette score: {max(silhouette_scores_list):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 3: Aplicar K-Means con K √≥ptimo",
        "print(\"=\"*60)",
        "print(\"PASO 3: APLICAR K-MEANS\")",
        "print(\"=\"*60)",
        "",
        "k_final = 3  # Basado en an√°lisis anterior",
        "",
        "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=10)",
        "df_cluster['cluster'] = kmeans_final.fit_predict(X_cluster_scaled)",
        "",
        "print(f\"\\n‚úÖ K-Means aplicado con K={k_final}\")",
        "print(f\"\\nüìä Distribuci√≥n de clusters:\")",
        "print(df_cluster['cluster'].value_counts().sort_index())",
        "",
        "# Caracter√≠sticas de cada cluster",
        "print(f\"\\nüìä Caracter√≠sticas promedio por cluster:\")",
        "cluster_means = df_cluster.groupby('cluster').mean()",
        "display(cluster_means.round(2))",
        "",
        "# Silhouette score final",
        "final_silhouette = silhouette_score(X_cluster_scaled, df_cluster['cluster'])",
        "print(f\"\\nüìä Silhouette Score final: {final_silhouette:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚≠ê PASO 4: Visualizar con PCA (Patr√≥n Examen 2024)",
        "print(\"=\"*60)",
        "print(\"PASO 4: VISUALIZACI√ìN CON PCA\")",
        "print(\"=\"*60)",
        "",
        "# Aplicar PCA para reducir a 2 dimensiones",
        "pca = PCA(n_components=2)",
        "X_pca = pca.fit_transform(X_cluster_scaled)",
        "",
        "print(f\"\\n‚úÖ PCA aplicado\")",
        "print(f\"Varianza explicada por PC1: {pca.explained_variance_ratio_[0]:.1%}\")",
        "print(f\"Varianza explicada por PC2: {pca.explained_variance_ratio_[1]:.1%}\")",
        "print(f\"Varianza total explicada: {pca.explained_variance_ratio_.sum():.1%}\")",
        "",
        "# Visualizar clusters en espacio PCA",
        "plt.figure(figsize=(10, 7))",
        "",
        "# Scatter de puntos",
        "scatter = plt.scatter(",
        "    X_pca[:, 0], ",
        "    X_pca[:, 1], ",
        "    c=df_cluster['cluster'], ",
        "    cmap='viridis',",
        "    alpha=0.6,",
        "    s=50,",
        "    edgecolors='black',",
        "    linewidth=0.5",
        ")",
        "",
        "# Centros de clusters (tambi√©n necesitan PCA)",
        "centers_original = kmeans_final.cluster_centers_",
        "centers_pca = pca.transform(centers_original)",
        "",
        "plt.scatter(",
        "    centers_pca[:, 0],",
        "    centers_pca[:, 1],",
        "    c='red',",
        "    marker='X',",
        "    s=300,",
        "    edgecolors='black',",
        "    linewidth=2,",
        "    label='Centros',",
        "    zorder=5",
        ")",
        "",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)', fontsize=12)",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)', fontsize=12)",
        "plt.title(f'Clusters K-Means (K={k_final}) en Espacio PCA\\nSilhouette Score: {final_silhouette:.3f}', ",
        "          fontsize=13, fontweight='bold')",
        "plt.colorbar(scatter, label='Cluster')",
        "plt.legend()",
        "plt.grid(True, alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\nüí° Los puntos del mismo color pertenecen al mismo cluster\")",
        "print(\"üí° Las X rojas marcan los centros de cada cluster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 9. Patrones Completos de Examen",
        "",
        "### üéØ Plantilla Completa: An√°lisis + Regresi√≥n",
        "",
        "Esta secci√≥n muestra el flujo completo desde carga de datos hasta modelado, combinando todas las t√©cnicas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)",
        "print(\"EJEMPLO COMPLETO: PIPELINE DE DATA SCIENCE\")",
        "print(\"=\"*70)",
        "",
        "# Simular dataset realista (como en ex√°menes)",
        "np.random.seed(42)",
        "n = 500",
        "",
        "df_complete = pd.DataFrame({",
        "    'id': range(1, n+1),",
        "    'categoria': np.random.choice(['A', 'B', 'C', 'D'], n),",
        "    'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste'], n),",
        "    'valor1': np.random.rand(n) * 100,",
        "    'valor2': np.random.rand(n) * 50,",
        "    'valor3': np.random.rand(n) * 30,",
        "    'calidad': np.random.choice(['Alta', 'Media', 'Baja', np.nan], n, p=[0.3, 0.4, 0.25, 0.05]),",
        "    'precio': np.random.rand(n) * 1000",
        "})",
        "",
        "# Crear relaci√≥n real entre features y target",
        "df_complete['precio'] = (",
        "    df_complete['valor1'] * 5 + ",
        "    df_complete['valor2'] * 3 + ",
        "    df_complete['valor3'] * 2 +",
        "    np.random.randn(n) * 50",
        ")",
        "",
        "print(\"\\nüìä Dataset completo creado\")",
        "print(f\"Shape: {df_complete.shape}\")",
        "display(df_complete.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FASE 1: EXPLORACI√ìN Y LIMPIEZA",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"FASE 1: EXPLORACI√ìN Y LIMPIEZA\")",
        "print(\"=\"*70)",
        "",
        "print(\"\\nüìã Informaci√≥n b√°sica:\")",
        "print(df_complete.info())",
        "",
        "print(\"\\nüîç Valores nulos:\")",
        "print(df_complete.isnull().sum())",
        "",
        "print(\"\\nüìä Estad√≠sticas num√©ricas:\")",
        "display(df_complete.describe())",
        "",
        "# Limpiar datos",
        "print(\"\\nüßπ Limpiando datos...\")",
        "df_clean = df_complete.drop('id', axis=1)  # ID no es √∫til para modelado",
        "df_clean = df_clean.dropna()  # Eliminar filas con nulos",
        "print(f\"‚úÖ Filas despu√©s de limpieza: {df_clean.shape[0]} (eliminadas: {df_complete.shape[0] - df_clean.shape[0]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FASE 2: AN√ÅLISIS EXPLORATORIO",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"FASE 2: AN√ÅLISIS EXPLORATORIO\")",
        "print(\"=\"*70)",
        "",
        "# An√°lisis por categor√≠a",
        "print(\"\\nüìä Precio promedio por categor√≠a:\")",
        "precio_categoria = df_clean.groupby('categoria')['precio'].agg(['mean', 'count', 'std'])",
        "display(precio_categoria.round(2))",
        "",
        "# An√°lisis por regi√≥n",
        "print(\"\\nüìä Precio promedio por regi√≥n:\")",
        "precio_region = df_clean.groupby('region')['precio'].mean().sort_values(ascending=False)",
        "print(precio_region.round(2))",
        "",
        "# Correlaciones",
        "print(\"\\nüìä Correlaci√≥n con precio:\")",
        "correlations = df_clean[['valor1', 'valor2', 'valor3', 'precio']].corr()['precio'].sort_values(ascending=False)",
        "print(correlations.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaciones exploratorias",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))",
        "",
        "# 1. Distribuci√≥n del precio",
        "axes[0, 0].hist(df_clean['precio'], bins=30, edgecolor='black', alpha=0.7)",
        "axes[0, 0].set_title('Distribuci√≥n del Precio', fontweight='bold')",
        "axes[0, 0].set_xlabel('Precio')",
        "axes[0, 0].set_ylabel('Frecuencia')",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')",
        "",
        "# 2. Precio por categor√≠a",
        "sns.boxplot(data=df_clean, x='categoria', y='precio', ax=axes[0, 1])",
        "axes[0, 1].set_title('Precio por Categor√≠a', fontweight='bold')",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')",
        "",
        "# 3. Correlaci√≥n m√°s fuerte",
        "axes[1, 0].scatter(df_clean['valor1'], df_clean['precio'], alpha=0.5)",
        "axes[1, 0].set_title('Precio vs Valor1', fontweight='bold')",
        "axes[1, 0].set_xlabel('Valor1')",
        "axes[1, 0].set_ylabel('Precio')",
        "axes[1, 0].grid(True, alpha=0.3)",
        "",
        "# 4. Precio por regi√≥n",
        "precio_region_plot = df_clean.groupby('region')['precio'].mean().sort_values()",
        "axes[1, 1].barh(precio_region_plot.index, precio_region_plot.values, color='skyblue', edgecolor='black')",
        "axes[1, 1].set_title('Precio Promedio por Regi√≥n', fontweight='bold')",
        "axes[1, 1].set_xlabel('Precio Promedio')",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FASE 3: PREPARACI√ìN PARA MACHINE LEARNING",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"FASE 3: PREPARACI√ìN PARA ML\")",
        "print(\"=\"*70)",
        "",
        "# Codificar variables categ√≥ricas",
        "df_ml_ready = pd.get_dummies(df_clean, columns=['categoria', 'region', 'calidad'], drop_first=True)",
        "",
        "print(f\"\\n‚úÖ Variables categ√≥ricas codificadas\")",
        "print(f\"Columnas originales: {df_clean.shape[1]}\")",
        "print(f\"Columnas despu√©s de encoding: {df_ml_ready.shape[1]}\")",
        "",
        "# Separar X e y",
        "X_final = df_ml_ready.drop('precio', axis=1)",
        "y_final = df_ml_ready['precio']",
        "",
        "print(f\"\\nüìä X shape: {X_final.shape}\")",
        "print(f\"üìä y shape: {y_final.shape}\")",
        "",
        "# Train-test split",
        "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(",
        "    X_final, y_final, test_size=0.2, random_state=42",
        ")",
        "",
        "# Estandarizar",
        "scaler_final = StandardScaler()",
        "X_train_final_scaled = scaler_final.fit_transform(X_train_final)",
        "X_test_final_scaled = scaler_final.transform(X_test_final)",
        "",
        "print(f\"\\n‚úÖ Datos preparados:\")",
        "print(f\"Train: {X_train_final_scaled.shape}\")",
        "print(f\"Test: {X_test_final_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FASE 4: MODELADO Y EVALUACI√ìN",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"FASE 4: MODELADO Y EVALUACI√ìN\")",
        "print(\"=\"*70)",
        "",
        "# Entrenar modelo",
        "model_final = ElasticNetCV(cv=5, random_state=42, max_iter=10000)",
        "print(\"\\nüîÑ Entrenando modelo...\")",
        "model_final.fit(X_train_final_scaled, y_train_final)",
        "",
        "# Predecir",
        "y_train_pred_final = model_final.predict(X_train_final_scaled)",
        "y_test_pred_final = model_final.predict(X_test_final_scaled)",
        "",
        "# Evaluar",
        "train_r2_final = r2_score(y_train_final, y_train_pred_final)",
        "test_r2_final = r2_score(y_test_final, y_test_pred_final)",
        "test_rmse_final = np.sqrt(mean_squared_error(y_test_final, y_test_pred_final))",
        "test_mae_final = mean_absolute_error(y_test_final, y_test_pred_final)",
        "",
        "print(\"\\n\" + \"=\"*60)",
        "print(\"RESULTADOS FINALES\")",
        "print(\"=\"*60)",
        "print(f\"\\n‚úÖ Modelo: ElasticNet con CV\")",
        "print(f\"‚úÖ Alpha √≥ptimo: {model_final.alpha_:.4f}\")",
        "print(f\"‚úÖ Features usadas: {np.sum(model_final.coef_ != 0)}/{len(model_final.coef_)}\")",
        "print(f\"\\nüìä M√©tricas de Evaluaci√≥n:\")",
        "print(f\"   R¬≤ (Train): {train_r2_final:.4f}\")",
        "print(f\"   R¬≤ (Test):  {test_r2_final:.4f}\")",
        "print(f\"   RMSE (Test): {test_rmse_final:.2f}\")",
        "print(f\"   MAE (Test):  {test_mae_final:.2f}\")",
        "",
        "# Visualizaci√≥n final",
        "plt.figure(figsize=(10, 6))",
        "plt.scatter(y_test_final, y_test_pred_final, alpha=0.6, edgecolors='black', linewidth=0.5)",
        "plt.plot([y_test_final.min(), y_test_final.max()], ",
        "         [y_test_final.min(), y_test_final.max()], ",
        "         'r--', lw=2, label='Predicci√≥n perfecta')",
        "plt.xlabel('Valores Reales', fontsize=12)",
        "plt.ylabel('Predicciones', fontsize=12)",
        "plt.title(f'Resultado Final del Modelo (R¬≤ = {test_r2_final:.3f})', ",
        "          fontsize=14, fontweight='bold')",
        "plt.legend()",
        "plt.grid(True, alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## 10. Resumen Final y Consejos",
        "",
        "### ‚úÖ Checklist para Ex√°menes",
        "",
        "**Antes de empezar:**",
        "- [ ] Leer TODOS los ejercicios primero",
        "- [ ] Identificar cu√°les son m√°s f√°ciles/dif√≠ciles",
        "- [ ] Planificar tiempo por ejercicio",
        "",
        "**Para cada ejercicio de datos:**",
        "- [ ] Cargar datos con `read_csv()`",
        "- [ ] Explorar: `info()`, `describe()`, `head()`",
        "- [ ] Verificar nulos: `isnull().sum()`",
        "- [ ] Limpiar/preparar datos seg√∫n necesidad",
        "- [ ] An√°lisis exploratorio con groupby/visualizaci√≥n",
        "- [ ] Si hay ML: split, scale, fit, predict, evaluate",
        "",
        "**Para ejercicios de ML:**",
        "- [ ] Separar X e y correctamente",
        "- [ ] Train-test split ANTES de cualquier preprocesamiento",
        "- [ ] Estandarizar si usas regularizaci√≥n/clustering",
        "- [ ] fit_transform en train, solo transform en test",
        "- [ ] Usar random_state para reproducibilidad",
        "- [ ] Evaluar con m√©tricas apropiadas",
        "- [ ] Visualizar resultados",
        "",
        "### üéØ Errores Comunes a Evitar",
        "",
        "1. **‚ùå No explorar datos antes de limpiar**",
        "   - ‚úÖ Siempre usar info(), describe(), isnull().sum()",
        "",
        "2. **‚ùå Hacer split despu√©s de estandarizar**",
        "   - ‚úÖ Split primero, luego fit_transform en train",
        "",
        "3. **‚ùå Usar fit_transform en test set**",
        "   - ‚úÖ Solo usar transform() en test",
        "",
        "4. **‚ùå No codificar variables categ√≥ricas**",
        "   - ‚úÖ Usar pd.get_dummies() o LabelEncoder",
        "",
        "5. **‚ùå Olvidar estandarizar para modelos que lo requieren**",
        "   - ‚úÖ Ridge, Lasso, ElasticNet, KMeans, SVM necesitan estandarizaci√≥n",
        "",
        "6. **‚ùå No verificar distribuci√≥n de clases en clasificaci√≥n**",
        "   - ‚úÖ Usar stratify=y en train_test_split para datos desbalanceados",
        "",
        "7. **‚ùå Gr√°ficos sin etiquetas o t√≠tulos**",
        "   - ‚úÖ Siempre xlabel, ylabel, title",
        "",
        "8. **‚ùå No interpretar resultados**",
        "   - ‚úÖ A√±adir prints explicativos de m√©tricas",
        "",
        "### üí° Trucos y Atajos",
        "",
        "```python",
        "# Quick data check",
        "df.info()",
        "df.describe()",
        "df.head()",
        "df.isnull().sum()",
        "",
        "# Quick groupby stats",
        "df.groupby('col')['value'].agg(['count', 'mean', 'std'])",
        "",
        "# Quick visualization",
        "df.groupby('cat')['val'].mean().plot(kind='bar')",
        "",
        "# Quick correlation check",
        "df.corr()['target'].sort_values()",
        "",
        "# Quick model evaluation",
        "print(f\"R¬≤ = {r2_score(y_test, y_pred):.3f}\")",
        "print(f\"RMSE = {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")",
        "```",
        "",
        "### üìö Recursos en el Repositorio",
        "",
        "Este notebook complementa:",
        "- `cheat_sheet.md` - Gu√≠a te√≥rica completa",
        "- `py_chu_*.ipynb` - Referencias r√°pidas por librer√≠a",
        "- Ejercicios en `clase_jf/` y `clase_io/` - Para practicar",
        "- Ex√°menes pasados - Para simular condiciones reales",
        "",
        "### üéì Estrategia de Estudio",
        "",
        "1. **Entender conceptos** con este notebook y cheat_sheet.md",
        "2. **Practicar** con ejercicios de clase_jf y clase_io",
        "3. **Simular examen** con ex√°menes pasados bajo tiempo l√≠mite",
        "4. **Revisar errores** y reforzar √°reas d√©biles",
        "5. **Crear res√∫menes propios** de patrones que m√°s usas",
        "",
        "### üöÄ √öltima Recomendaci√≥n",
        "",
        "**Practica el workflow completo m√∫ltiples veces:**",
        "",
        "1. Carga ‚Üí 2. Explora ‚Üí 3. Limpia ‚Üí 4. Analiza ‚Üí 5. Visualiza ‚Üí 6. Modela ‚Üí 7. Eval√∫a",
        "",
        "Hasta que sea autom√°tico. En el examen, el tiempo vuela y necesitas ser eficiente.",
        "",
        "---",
        "",
        "**¬°Mucho √©xito en tus ex√°menes! üéâ**",
        "",
        "*Este notebook fue generado mediante an√°lisis exhaustivo de todos los ejercicios y ex√°menes del repositorio Python_examen_Tecnun.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}