# Cheat Sheet: Python para Data Science - An√°lisis Completo

Este documento es una gu√≠a completa y anal√≠tica basada en los ejercicios de las carpetas `clase_jf/` y `clase_io/`, as√≠ como en los ex√°menes de 2022, 2023 y 2024.

---

## üìö Tabla de Contenidos

1. [Python B√°sico](#1-python-b√°sico)
2. [NumPy: Operaciones Num√©ricas](#2-numpy-operaciones-num√©ricas)
3. [Pandas: Manipulaci√≥n de Datos](#3-pandas-manipulaci√≥n-de-datos)
4. [Limpieza y Preparaci√≥n de Datos](#4-limpieza-y-preparaci√≥n-de-datos)
5. [Visualizaci√≥n de Datos](#5-visualizaci√≥n-de-datos)
6. [Estad√≠stica](#6-estad√≠stica)
7. [Machine Learning con Scikit-learn](#7-machine-learning-con-scikit-learn)
8. [Patrones Comunes en Ex√°menes](#8-patrones-comunes-en-ex√°menes)
9. [Resumen General](#9-resumen-general)

---

## 1. Python B√°sico

### 1.1 Variables y Tipos de Datos

**Por qu√© es importante:** Python es din√°micamente tipado, lo que significa que no necesitas declarar expl√≠citamente el tipo de variable. Sin embargo, entender los tipos es crucial para evitar errores.

```python
# Tipos b√°sicos
entero = 42
flotante = 3.14
texto = "Data Science"
booleano = True
lista = [1, 2, 3, 4, 5]
tupla = (1, 2, 3)
diccionario = {'nombre': 'Juan', 'edad': 25}
```

**An√°lisis:** Las listas son mutables (se pueden modificar), las tuplas son inmutables. Los diccionarios son fundamentales para estructurar datos clave-valor.

### 1.2 Estructuras de Control

**Bucles - For y While:**

```python
# For loop - itera sobre una secuencia
for i in range(5):
    print(f"Iteraci√≥n {i}")

# List comprehension - forma pyth√≥nica de crear listas
cuadrados = [x**2 for x in range(10)]

# Comprehension con condici√≥n
pares = [x for x in range(20) if x % 2 == 0]
```

**Por qu√© usar list comprehensions:** Son m√°s r√°pidas y legibles que los bucles tradicionales para operaciones simples.

**Condicionales:**

```python
# If-elif-else
valor = 75
if valor >= 90:
    categoria = 'Excelente'
elif valor >= 70:
    categoria = 'Bueno'
else:
    categoria = 'Necesita mejorar'
```

### 1.3 Funciones

**Definici√≥n y uso:**

```python
def calcular_estadisticas(datos):
    """
    Calcula media, mediana y desviaci√≥n est√°ndar.
    
    Args:
        datos (list): Lista de n√∫meros
    
    Returns:
        dict: Diccionario con las estad√≠sticas
    """
    import numpy as np
    return {
        'media': np.mean(datos),
        'mediana': np.median(datos),
        'std': np.std(datos)
    }

# Uso
resultado = calcular_estadisticas([1, 2, 3, 4, 5])
```

**An√°lisis:** Las funciones deben tener docstrings que expliquen qu√© hacen, sus par√°metros y valores de retorno. Esto es especialmente importante en proyectos de data science.

### 1.4 Gesti√≥n del Entorno

**Patr√≥n com√∫n en notebooks:**

```python
from IPython import get_ipython
get_ipython().run_line_magic('reset', '-f')
get_ipython().run_line_magic('clear', '-f')

import os
# Verificar directorio de trabajo
print(os.getcwd())

# Cambiar directorio si es necesario
# os.chdir('/ruta/al/directorio')
```

**Por qu√©:** Limpiar el entorno asegura que no haya variables residuales de ejecuciones anteriores que puedan causar errores.

---

## 2. NumPy: Operaciones Num√©ricas

### 2.1 Creaci√≥n de Arrays

**Conceptos b√°sicos:**

```python
import numpy as np

# Crear arrays de diferentes formas
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.zeros((3, 4))        # Array de ceros
arr3 = np.ones((2, 3))         # Array de unos
arr4 = np.arange(0, 10, 2)     # Array con secuencia
arr5 = np.linspace(0, 1, 5)    # 5 n√∫meros equiespaciados entre 0 y 1
arr6 = np.random.rand(3, 3)    # Array aleatorio
```

**Por qu√© NumPy:** Los arrays de NumPy son mucho m√°s eficientes que las listas de Python para operaciones num√©ricas, gracias a la implementaci√≥n en C.

### 2.2 Indexaci√≥n y Slicing

```python
arr = np.array([10, 20, 30, 40, 50])

# Indexaci√≥n b√°sica
primer_elemento = arr[0]      # 10
ultimo_elemento = arr[-1]     # 50

# Slicing
primeros_tres = arr[:3]       # [10, 20, 30]
desde_segundo = arr[1:]       # [20, 30, 40, 50]
cada_dos = arr[::2]           # [10, 30, 50]

# Indexaci√≥n booleana (muy com√∫n en an√°lisis)
arr_mayor_25 = arr[arr > 25]  # [30, 40, 50]
```

**An√°lisis:** La indexaci√≥n booleana es fundamental para filtrar datos bas√°ndose en condiciones.

### 2.3 Operaciones Vectorizadas

```python
# Operaciones elemento a elemento (mucho m√°s r√°pidas que loops)
arr = np.array([1, 2, 3, 4, 5])

# Operaciones aritm√©ticas
arr_multiplicado = arr * 2     # [2, 4, 6, 8, 10]
arr_cuadrado = arr ** 2        # [1, 4, 9, 16, 25]

# Funciones matem√°ticas
arr_raiz = np.sqrt(arr)
arr_exp = np.exp(arr)
arr_log = np.log(arr)
```

**Por qu√© vectorizaci√≥n:** Evitar bucles expl√≠citos mejora dr√°sticamente el rendimiento.

### 2.4 Estad√≠sticas con NumPy

```python
datos = np.array([15, 23, 18, 30, 25, 19, 22])

# Estad√≠sticas descriptivas
media = np.mean(datos)
mediana = np.median(datos)
std = np.std(datos)
varianza = np.var(datos)
minimo = np.min(datos)
maximo = np.max(datos)
suma = np.sum(datos)
```

---

## 3. Pandas: Manipulaci√≥n de Datos

### 3.1 Lectura de Datos

**Patr√≥n m√°s com√∫n en ex√°menes:**

```python
import pandas as pd

# Leer CSV
df = pd.read_csv('archivo.csv')

# Opciones √∫tiles
df = pd.read_csv('archivo.csv', 
                 sep=',',           # Delimitador
                 encoding='utf-8',  # Codificaci√≥n
                 na_values=['NA', 'N/A', ''])  # Valores nulos
```

**Exploraci√≥n inicial:**

```python
# Primeras y √∫ltimas filas
df.head()      # Primeras 5 filas
df.tail(10)    # √öltimas 10 filas

# Informaci√≥n del DataFrame
df.info()      # Tipos de datos y valores nulos
df.describe()  # Estad√≠sticas descriptivas
df.shape       # (filas, columnas)
df.columns     # Nombres de columnas
df.dtypes      # Tipos de datos de cada columna
```

**An√°lisis:** Siempre comienza con exploraci√≥n. `info()` te dice si hay valores nulos y el tipo de cada columna.

### 3.2 Selecci√≥n y Filtrado

```python
# Seleccionar columnas
df['columna']                    # Una columna (Serie)
df[['col1', 'col2']]            # M√∫ltiples columnas (DataFrame)

# Seleccionar filas y columnas
df.loc[0:5, 'columna']          # Por etiquetas
df.iloc[0:5, 0:3]               # Por √≠ndices num√©ricos

# Filtrado con condiciones (MUY COM√öN EN EX√ÅMENES)
df_filtrado = df[df['edad'] > 25]
df_filtrado = df[(df['edad'] > 25) & (df['ciudad'] == 'Madrid')]
df_filtrado = df[df['nombre'].isin(['Juan', 'Mar√≠a', 'Pedro'])]
```

**Por qu√© importante:** El filtrado condicional es la base del an√°lisis de datos. Nota el uso de `&` (and) y `|` (or) en lugar de `and`/`or`.

### 3.3 Operaciones de Agregaci√≥n

**groupby - Patr√≥n fundamental:**

```python
# Agrupar y agregar
resultado = df.groupby('categoria')['ventas'].mean()

# M√∫ltiples agregaciones
resultado = df.groupby('categoria').agg({
    'ventas': ['sum', 'mean', 'count'],
    'precio': ['min', 'max']
})

# Agrupar por m√∫ltiples columnas
resultado = df.groupby(['categoria', 'a√±o'])['ventas'].sum()
```

**An√°lisis:** `groupby` es como el "GROUP BY" de SQL. Es esencial para an√°lisis agregados (ej: "ventas por categor√≠a y a√±o").

**Ejemplo pr√°ctico del Examen 2023:**

```python
# Contar quejas por compa√±√≠a
top_companies = df.groupby('company').size().sort_values(ascending=False).head(5)
```

### 3.4 Transformaci√≥n de Datos

```python
# Aplicar funciones
df['columna_nueva'] = df['columna_vieja'].apply(lambda x: x * 2)

# Map - reemplazar valores
df['categoria_num'] = df['categoria'].map({'A': 1, 'B': 2, 'C': 3})

# Replace - reemplazar valores espec√≠ficos
df['columna'] = df['columna'].replace('valor_viejo', 'valor_nuevo')

# Cambiar tipos de datos (MUY COM√öN)
df['columna'] = df['columna'].astype(float)
df['fecha'] = pd.to_datetime(df['fecha'])
```

**Ejemplo del Examen 2022:**

```python
# Convertir texto con '$' a n√∫mero
df['Total_Gross'] = df['Total_Gross'].str.replace('$', '').astype(float)
```

### 3.5 Combinaci√≥n de DataFrames

```python
# Merge (similar a JOIN de SQL)
df_merged = pd.merge(df1, df2, on='id', how='inner')
# how puede ser: 'inner', 'outer', 'left', 'right'

# Concatenar DataFrames
df_concat = pd.concat([df1, df2], axis=0)  # Verticalmente
df_concat = pd.concat([df1, df2], axis=1)  # Horizontalmente
```

### 3.6 Valores √önicos y Conteos

```python
# Valores √∫nicos
unicos = df['columna'].unique()
num_unicos = df['columna'].nunique()

# Conteo de valores (MUY USADO EN EX√ÅMENES)
conteos = df['columna'].value_counts()
conteos_porcentaje = df['columna'].value_counts(normalize=True)
```

---

## 4. Limpieza y Preparaci√≥n de Datos

### 4.1 Manejo de Valores Nulos

**Identificaci√≥n:**

```python
# Contar valores nulos
df.isnull().sum()
df.isna().sum()  # Equivalente

# Porcentaje de valores nulos
percent_missing = df.isnull().mean() * 100
print(percent_missing)
```

**Tratamiento:**

```python
# Eliminar filas con valores nulos
df_clean = df.dropna()                    # Cualquier nulo
df_clean = df.dropna(subset=['col1'])     # Nulos en columna espec√≠fica

# Eliminar columnas con valores nulos
df_clean = df.dropna(axis=1)

# Rellenar valores nulos
df_filled = df.fillna(0)                  # Con cero
df_filled = df.fillna(df.mean())          # Con la media
df_filled = df.fillna(method='ffill')     # Forward fill
df_filled = df.fillna(method='bfill')     # Backward fill
```

**Patr√≥n del Examen 2024:**

```python
# Eliminar columnas con m√°s del 25% de valores nulos
percent_missing = df.isnull().mean() * 100
selected_columns = percent_missing < 25
df_clean = df.loc[:, selected_columns]
df_clean = df_clean.dropna()
```

**An√°lisis:** Esta es una estrategia com√∫n: eliminar columnas con muchos nulos, luego eliminar filas con pocos nulos restantes.

### 4.2 Detecci√≥n y Manejo de Outliers

```python
# M√©todo IQR (Interquartile Range)
Q1 = df['columna'].quantile(0.25)
Q3 = df['columna'].quantile(0.75)
IQR = Q3 - Q1

# Definir l√≠mites
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

# Filtrar outliers
df_sin_outliers = df[(df['columna'] >= limite_inferior) & 
                     (df['columna'] <= limite_superior)]
```

### 4.3 Codificaci√≥n de Variables Categ√≥ricas

```python
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# One-Hot Encoding (crear columnas dummy)
df_encoded = pd.get_dummies(df, columns=['categoria'], drop_first=True)

# Label Encoding (convertir a n√∫meros)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['categoria_encoded'] = le.fit_transform(df['categoria'])
```

**Por qu√© es importante:** Los modelos de machine learning requieren datos num√©ricos. One-hot encoding es preferible para variables nominales, label encoding para ordinales.

### 4.4 Normalizaci√≥n y Estandarizaci√≥n

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Estandarizaci√≥n (media=0, std=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Normalizaci√≥n (rango 0-1)
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)
```

**An√°lisis:** La estandarizaci√≥n es esencial para algoritmos sensibles a escalas (SVM, KNN, regresi√≥n regularizada). Usa `fit_transform` en train, `transform` en test.

---

## 5. Visualizaci√≥n de Datos

### 5.1 Matplotlib - Gr√°ficos B√°sicos

**Configuraci√≥n inicial:**

```python
import matplotlib.pyplot as plt

# Configurar tama√±o de figura
plt.figure(figsize=(10, 6))

# Gr√°fico de l√≠nea
plt.plot(x, y, label='Datos', color='blue', linewidth=2)
plt.xlabel('Eje X')
plt.ylabel('Eje Y')
plt.title('T√≠tulo del Gr√°fico')
plt.legend()
plt.grid(True)
plt.show()
```

**Gr√°fico de barras:**

```python
# Vertical
plt.bar(categorias, valores)

# Horizontal
plt.barh(categorias, valores)

# Personalizaci√≥n
plt.bar(categorias, valores, color='skyblue', edgecolor='black', alpha=0.7)
```

**Scatter plot:**

```python
plt.scatter(x, y, c=colores, s=tama√±os, alpha=0.5)
plt.colorbar()  # Si usas colores basados en valores
```

**Histograma:**

```python
plt.hist(datos, bins=20, edgecolor='black', alpha=0.7)
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
```

### 5.2 Seaborn - Visualizaciones Estad√≠sticas

**Por qu√© Seaborn:** Construido sobre Matplotlib, ofrece visualizaciones estad√≠sticas m√°s sofisticadas con menos c√≥digo.

```python
import seaborn as sns

# Configurar estilo
sns.set_style('whitegrid')
sns.set_palette('husl')
```

**Barplot con estad√≠sticas:**

```python
# Autom√°ticamente muestra media con intervalo de confianza
sns.barplot(x='categoria', y='valor', data=df)

# Personalizar
sns.barplot(x='categoria', y='valor', data=df, 
            ci=95,           # Intervalo de confianza
            estimator=np.median)  # Usar mediana en lugar de media
```

**Patr√≥n del Examen 2023:**

```python
# Visualizar quejas por compa√±√≠a y tipo de env√≠o
plt.figure(figsize=(12, 6))
sns.barplot(x='company', y='count', hue='submitted_via', data=df_plot)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

**Boxplot:**

```python
sns.boxplot(x='categoria', y='valor', data=df)
# √ötil para detectar outliers y comparar distribuciones
```

**Heatmap - Matriz de correlaci√≥n:**

```python
# Calcular correlaci√≥n
correlation_matrix = df.corr()

# Visualizar
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlaci√≥n')
plt.show()
```

**Pairplot - Visualizaci√≥n multivariada:**

```python
sns.pairplot(df, hue='categoria')
# Muestra relaciones entre todas las variables num√©ricas
```

**Violinplot:**

```python
sns.violinplot(x='categoria', y='valor', data=df)
# Combina boxplot con estimaci√≥n de densidad
```

### 5.3 Mejores Pr√°cticas de Visualizaci√≥n

```python
# 1. Siempre etiquetar ejes y a√±adir t√≠tulo
plt.xlabel('Descripci√≥n del eje X', fontsize=12)
plt.ylabel('Descripci√≥n del eje Y', fontsize=12)
plt.title('T√≠tulo Descriptivo', fontsize=14, fontweight='bold')

# 2. Ajustar tama√±o de figura seg√∫n contenido
plt.figure(figsize=(12, 6))  # Para gr√°ficos anchos

# 3. Rotar etiquetas si son largas
plt.xticks(rotation=45, ha='right')

# 4. Usar tight_layout para evitar solapamientos
plt.tight_layout()

# 5. Guardar figuras con alta resoluci√≥n
plt.savefig('grafico.png', dpi=300, bbox_inches='tight')
```

---

## 6. Estad√≠stica

### 6.1 Estad√≠stica Descriptiva

```python
import numpy as np
import scipy.stats as stats

# Medidas de tendencia central
media = np.mean(datos)
mediana = np.median(datos)
moda = stats.mode(datos)

# Medidas de dispersi√≥n
std = np.std(datos)           # Desviaci√≥n est√°ndar
varianza = np.var(datos)      # Varianza
rango = np.max(datos) - np.min(datos)

# Cuartiles
Q1 = np.percentile(datos, 25)
Q2 = np.percentile(datos, 50)  # Mediana
Q3 = np.percentile(datos, 75)
```

### 6.2 Correlaci√≥n

```python
from scipy.stats import pearsonr, spearmanr

# Correlaci√≥n de Pearson (lineal)
corr, p_value = pearsonr(x, y)
print(f"Correlaci√≥n: {corr:.3f}, p-value: {p_value:.4f}")

# Correlaci√≥n de Spearman (monot√≥nica, no necesariamente lineal)
corr, p_value = spearmanr(x, y)

# Matriz de correlaci√≥n con pandas
correlation_matrix = df.corr()
```

**Interpretaci√≥n:**
- Correlaci√≥n cercana a 1: relaci√≥n positiva fuerte
- Correlaci√≥n cercana a -1: relaci√≥n negativa fuerte
- Correlaci√≥n cercana a 0: no hay relaci√≥n lineal
- p-value < 0.05: correlaci√≥n estad√≠sticamente significativa

### 6.3 Tests Estad√≠sticos

**Patr√≥n del Examen 2022 - Test de Wilcoxon:**

```python
from scipy.stats import ranksums

# Comparar dos grupos
stat, p_value = ranksums(grupo1, grupo2)

if p_value < 0.05:
    print("Hay diferencias significativas entre los grupos")
else:
    print("No hay diferencias significativas")
```

**Test de normalidad:**

```python
from scipy.stats import shapiro

stat, p_value = shapiro(datos)
if p_value > 0.05:
    print("Los datos siguen una distribuci√≥n normal")
```

---

## 7. Machine Learning con Scikit-learn

### 7.1 Workflow General de Machine Learning

```python
# 1. Importar bibliotecas
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 2. Preparar datos (X: features, y: target)
X = df[['feature1', 'feature2', 'feature3']]
y = df['target']

# 3. Dividir en train y test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Normalizar/Estandarizar (si es necesario)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Solo transform!

# 5. Entrenar modelo
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# 6. Predecir
y_pred = model.predict(X_test_scaled)

# 7. Evaluar
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse:.2f}, R¬≤: {r2:.3f}")
```

**An√°lisis cr√≠tico:** Nota que usamos `fit_transform` solo en train y `transform` en test. Esto evita data leakage.

### 7.2 Regresi√≥n Lineal

**Simple:**

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

# Coeficientes
print(f"Intercepto: {model.intercept_}")
print(f"Coeficientes: {model.coef_}")

# Predicci√≥n
y_pred = model.predict(X_test)
```

**Con validaci√≥n cruzada:**

```python
from sklearn.model_selection import cross_val_score

# Evaluar modelo con 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"R¬≤ medio: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 7.3 Regresi√≥n con Regularizaci√≥n

**Ridge (L2):**

```python
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)  # alpha controla la regularizaci√≥n
model.fit(X_train, y_train)
```

**Elastic Net (L1 + L2):**

```python
from sklearn.linear_model import ElasticNetCV

# CV selecciona autom√°ticamente el mejor alpha
model = ElasticNetCV(cv=5, random_state=42)
model.fit(X_train, y_train)
print(f"Mejor alpha: {model.alpha_}")
```

**Patr√≥n del Examen 2024:**

```python
from sklearn.linear_model import ElasticNetCV
from sklearn.metrics import explained_variance_score

# Entrenar con validaci√≥n cruzada
model = ElasticNetCV(cv=5, random_state=0)
model.fit(X_train, y_train)

# Predecir y evaluar
y_pred = model.predict(X_test)
score = explained_variance_score(y_test, y_pred)
print(f"Explained Variance: {score:.3f}")
```

**Por qu√© regularizaci√≥n:** Previene overfitting penalizando coeficientes grandes. Ridge mantiene todas las features, Lasso puede eliminar algunas (selecci√≥n de features).

### 7.4 Regresi√≥n Log√≠stica (Clasificaci√≥n)

```python
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import accuracy_score, classification_report

# Con validaci√≥n cruzada para encontrar mejor C
model = LogisticRegressionCV(cv=5, random_state=42)
model.fit(X_train, y_train)

# Predecir
y_pred = model.predict(X_test)

# Evaluar
accuracy = accuracy_score(y_test, y_pred)
print(classification_report(y_test, y_pred))
```

**Probabilidades de predicci√≥n:**

```python
# Obtener probabilidades en lugar de clases
probs = model.predict_proba(X_test)
```

**Patr√≥n del Examen 2023:**

```python
from sklearn.metrics import precision_recall_curve, auc

# Calcular curva precision-recall
precision, recall, thresholds = precision_recall_curve(y_test, y_probs)
auc_score = auc(recall, precision)
print(f"AUC: {auc_score:.3f}")
```

### 7.5 K-Means Clustering

**Uso b√°sico:**

```python
from sklearn.cluster import KMeans

# Entrenar modelo
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

# A√±adir clusters al DataFrame
df['cluster'] = clusters

# Centros de clusters
centers = kmeans.cluster_centers_
```

**Encontrar n√∫mero √≥ptimo de clusters - M√©todo del codo:**

```python
inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# Visualizar
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('N√∫mero de Clusters')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo')
plt.show()
```

**Silhouette score:**

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, clusters)
print(f"Silhouette Score: {score:.3f}")
# Valores cercanos a 1 indican clusters bien definidos
```

**Patr√≥n del Examen 2024:**

```python
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# 1. Aplicar K-means
kmeans = KMeans(n_clusters=3, random_state=0)
clusters = kmeans.fit_predict(X_scaled)

# 2. Reducir dimensionalidad para visualizar
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 3. Visualizar
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            c='red', marker='X', s=200, label='Centros')
plt.legend()
plt.show()
```

### 7.6 Clustering Jer√°rquico

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# Crear dendrograma
Z = linkage(X, method='ward')
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title('Dendrograma')
plt.show()

# Clustering jer√°rquico
hierarchical = AgglomerativeClustering(n_clusters=3)
clusters = hierarchical.fit_predict(X)
```

### 7.7 PCA (An√°lisis de Componentes Principales)

```python
from sklearn.decomposition import PCA

# Reducir a 2 componentes
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Varianza explicada
print(f"Varianza explicada: {pca.explained_variance_ratio_}")
print(f"Varianza total: {pca.explained_variance_ratio_.sum():.3f}")

# Visualizar
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
plt.show()
```

**Por qu√© PCA:** Reduce dimensionalidad preservando m√°xima varianza. √ötil para visualizaci√≥n y cuando hay muchas features correlacionadas.

### 7.8 M√©tricas de Evaluaci√≥n

**Regresi√≥n:**

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from statsmodels.tools.eval_measures import rmse

# Errores
mse = mean_squared_error(y_test, y_pred)
rmse_score = rmse(y_test, y_pred)  # o np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)

# R¬≤ score (coeficiente de determinaci√≥n)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse_score:.2f}")
print(f"MAE: {mae:.2f}")
print(f"R¬≤: {r2:.3f}")
```

**Interpretaci√≥n R¬≤:** 
- R¬≤ = 1: predicci√≥n perfecta
- R¬≤ = 0: modelo no mejor que predecir la media
- R¬≤ < 0: modelo peor que predecir la media

**Clasificaci√≥n:**

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report

# M√©tricas b√°sicas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)
print("Matriz de Confusi√≥n:")
print(cm)

# Reporte completo
print(classification_report(y_test, y_pred))
```

---

## 8. Patrones Comunes en Ex√°menes

### 8.1 Estructura T√≠pica de un Examen

**Patr√≥n observado en los 3 ex√°menes:**

1. **Carga y exploraci√≥n de datos** (1 punto)
   - Leer CSV
   - Explorar estructura (info, describe, shape)
   - Identificar tipos de datos

2. **Limpieza y preparaci√≥n** (1-2 puntos)
   - Manejar valores nulos
   - Convertir tipos de datos
   - Filtrar datos relevantes
   - Seleccionar columnas de inter√©s

3. **An√°lisis exploratorio y visualizaci√≥n** (2-3 puntos)
   - Agrupar datos (groupby)
   - Crear visualizaciones (barplot, scatter, etc.)
   - Identificar patrones

4. **An√°lisis estad√≠stico o ML** (3-4 puntos)
   - Construir modelo predictivo
   - Aplicar clustering
   - Realizar test estad√≠stico
   - Evaluar resultados

### 8.2 Ejercicio Tipo: An√°lisis de Datos

**Plantilla general:**

```python
# 1. CONFIGURACI√ìN INICIAL
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Configuraci√≥n de visualizaci√≥n
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

# 2. CARGA DE DATOS
df = pd.read_csv('datos.csv')

# 3. EXPLORACI√ìN INICIAL
print("Shape:", df.shape)
print("\nInfo:")
print(df.info())
print("\nPrimeras filas:")
print(df.head())
print("\nEstad√≠sticas:")
print(df.describe())
print("\nValores nulos:")
print(df.isnull().sum())

# 4. LIMPIEZA
# Seleccionar columnas relevantes
columnas_analisis = ['col1', 'col2', 'col3', 'target']
df_clean = df[columnas_analisis].copy()

# Manejar nulos
df_clean = df_clean.dropna()

# Convertir tipos si es necesario
df_clean['columna'] = df_clean['columna'].astype(float)

# 5. AN√ÅLISIS EXPLORATORIO
# Agrupar y agregar
resultado = df_clean.groupby('categoria')['valor'].agg(['mean', 'count', 'std'])
print(resultado)

# Visualizar
plt.figure(figsize=(12, 6))
sns.barplot(x='categoria', y='valor', data=df_clean)
plt.xticks(rotation=45)
plt.title('An√°lisis por Categor√≠a')
plt.tight_layout()
plt.show()

# 6. PREPARACI√ìN PARA ML
X = df_clean[['feature1', 'feature2', 'feature3']]
y = df_clean['target']

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Escalar
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 7. MODELO
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

model = LinearRegression()
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)

# 8. EVALUACI√ìN
r2 = r2_score(y_test, y_pred)
print(f"R¬≤ Score: {r2:.3f}")

# Visualizar predicciones
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], 
         [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title('Predicciones vs Valores Reales')
plt.show()
```

### 8.3 Ejercicio Tipo: Clustering

**Plantilla basada en Examen 2024:**

```python
# 1. Preparar datos (solo features, no target)
X = df[['feature1', 'feature2', 'feature3']].copy()

# 2. Estandarizar
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Determinar n√∫mero √≥ptimo de clusters
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertias = []
silhouette_scores = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    clusters = kmeans.fit_predict(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, clusters))

# Visualizar m√©todo del codo
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(K, inertias, 'bo-')
ax1.set_xlabel('N√∫mero de Clusters')
ax1.set_ylabel('Inercia')
ax1.set_title('M√©todo del Codo')

ax2.plot(K, silhouette_scores, 'ro-')
ax2.set_xlabel('N√∫mero de Clusters')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Score por K')

plt.tight_layout()
plt.show()

# 4. Aplicar K-means con k √≥ptimo
k_optimal = 3
kmeans = KMeans(n_clusters=k_optimal, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)

# 5. Analizar clusters
print("Distribuci√≥n de clusters:")
print(df['cluster'].value_counts().sort_index())

print("\nCaracter√≠sticas por cluster:")
print(df.groupby('cluster')[['feature1', 'feature2', 'feature3']].mean())

# 6. Visualizar con PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                      c=df['cluster'], cmap='viridis', alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('Clusters en Espacio PCA')
plt.colorbar(scatter, label='Cluster')
plt.show()
```

### 8.4 Errores Comunes y C√≥mo Evitarlos

**1. Data Leakage:**
```python
# ‚ùå INCORRECTO - fit_transform en todo el dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test = train_test_split(X_scaled, ...)

# ‚úÖ CORRECTO - fit solo en train
X_train, X_test = train_test_split(X, ...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**2. No manejar valores nulos antes de ML:**
```python
# ‚úÖ Siempre verificar y manejar nulos
print(df.isnull().sum())
df = df.dropna()  # o fillna()
```

**3. Olvidar estandarizar para algoritmos sensibles:**
```python
# Algoritmos que REQUIEREN estandarizaci√≥n:
# - Regresi√≥n con regularizaci√≥n (Ridge, Lasso, ElasticNet)
# - SVM
# - KNN
# - K-means
# - PCA
# - Regresi√≥n log√≠stica (recomendado)

# No la necesitan:
# - √Årboles de decisi√≥n
# - Random Forest
# - Gradient Boosting
```

**4. Usar variables categ√≥ricas sin codificar:**
```python
# ‚úÖ Codificar antes de usar en modelo
df_encoded = pd.get_dummies(df, columns=['categoria'], drop_first=True)
```

**5. No hacer split aleatorio reproducible:**
```python
# ‚úÖ Usar random_state para reproducibilidad
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

---

## 9. Resumen General

### 9.1 Workflow Completo de Data Science

```
1. ENTENDER EL PROBLEMA
   ‚Üì
2. CARGAR Y EXPLORAR DATOS
   - read_csv()
   - info(), describe(), head()
   ‚Üì
3. LIMPIAR DATOS
   - Valores nulos (dropna/fillna)
   - Tipos de datos (astype)
   - Outliers
   ‚Üì
4. AN√ÅLISIS EXPLORATORIO
   - groupby, value_counts
   - Visualizaciones (matplotlib/seaborn)
   - Correlaciones
   ‚Üì
5. PREPARAR PARA MODELADO
   - Seleccionar features
   - Train/test split
   - Codificar categ√≥ricas
   - Estandarizar
   ‚Üì
6. ENTRENAR MODELO
   - Elegir algoritmo apropiado
   - fit() con datos de entrenamiento
   ‚Üì
7. EVALUAR
   - M√©tricas apropiadas
   - Visualizar resultados
   - Validaci√≥n cruzada
   ‚Üì
8. INTERPRETAR Y COMUNICAR
   - Conclusiones
   - Visualizaciones finales
   - Recomendaciones
```

### 9.2 Principales T√©cnicas por Tipo de Problema

**Regresi√≥n (predecir valor continuo):**
- LinearRegression
- Ridge/Lasso/ElasticNet (con regularizaci√≥n)
- M√©tricas: MSE, RMSE, MAE, R¬≤

**Clasificaci√≥n (predecir categor√≠a):**
- LogisticRegression
- M√©tricas: Accuracy, Precision, Recall, F1, AUC

**Clustering (agrupar sin etiquetas):**
- K-Means
- Hierarchical Clustering
- M√©tricas: Silhouette Score, Inercia

**Reducci√≥n de Dimensionalidad:**
- PCA
- Para visualizaci√≥n o pre-procesamiento

### 9.3 Librer√≠as Esenciales

```python
# Manipulaci√≥n de datos
import pandas as pd
import numpy as np

# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

# Estad√≠stica
from scipy import stats
```

### 9.4 Consejos Finales para Ex√°menes

1. **Lee cuidadosamente** qu√© pide cada ejercicio
2. **Explora los datos primero** - usa info(), describe(), head()
3. **Verifica valores nulos** antes de continuar
4. **Divide train/test ANTES** de cualquier preprocessing
5. **Estandariza** cuando uses regularizaci√≥n o clustering
6. **Visualiza** para validar tus resultados
7. **Comenta tu c√≥digo** brevemente para explicar tu razonamiento
8. **Verifica que tus gr√°ficos** tengan etiquetas y t√≠tulos
9. **Usa random_state** para reproducibilidad
10. **Gestiona el tiempo** - no te quedes atascado en un ejercicio

### 9.5 Relaci√≥n entre Temas

```
Python B√°sico
    ‚Üì
NumPy (arrays, operaciones vectorizadas)
    ‚Üì
Pandas (DataFrames, an√°lisis de datos)
    ‚Üì
    ‚îú‚îÄ‚Üí Estad√≠stica (correlaci√≥n, tests)
    ‚îÇ       ‚Üì
    ‚îÇ   Visualizaci√≥n (Matplotlib, Seaborn)
    ‚îÇ       ‚Üì
    ‚îî‚îÄ‚Üí Machine Learning (Scikit-learn)
            ‚îú‚îÄ‚Üí Regresi√≥n
            ‚îú‚îÄ‚Üí Clasificaci√≥n
            ‚îî‚îÄ‚Üí Clustering
```

**Todos los temas se conectan:** NumPy proporciona arrays eficientes, Pandas los usa para DataFrames, la visualizaci√≥n y ML trabajan sobre estos datos estructurados.

---

## üìù Notas Aclaratorias

### Diferencias entre Ejercicios y Ex√°menes

**Ejercicios de clase:**
- M√°s guiados, con explicaciones paso a paso
- Enfoque did√°ctico en entender conceptos
- C√≥digo m√°s extenso y comentado
- Ejemplos m√°s simples

**Ex√°menes:**
- Requieren aplicar m√∫ltiples conceptos
- Menos gu√≠a, m√°s autonom√≠a
- Problemas realistas con datasets reales
- Tiempo limitado - necesitas ser eficiente

### Soluciones Alternativas

En varios ejercicios se observan diferentes enfoques:

**Para agrupar y contar:**
```python
# Opci√≥n 1: groupby + size
df.groupby('columna').size()

# Opci√≥n 2: value_counts (m√°s directo)
df['columna'].value_counts()

# Opci√≥n 3: groupby + count (cuenta por cada columna)
df.groupby('columna').count()
```

**Para filtrar top N:**
```python
# Opci√≥n 1: sort + head
df.sort_values('valor', ascending=False).head(5)

# Opci√≥n 2: nlargest
df.nlargest(5, 'valor')
```

Ambos enfoques son v√°lidos. Elige el que te resulte m√°s natural y legible.

### Recursos Adicionales

Los archivos de chuletas existentes en la carpeta `chuleta/` son complementarios:
- `py_chu_numpy.ipynb` - Referencia r√°pida de NumPy
- `py_chu_pandas.ipynb` - Referencia r√°pida de Pandas
- `py_chu_matplotlib.ipynb` - Referencia r√°pida de Matplotlib
- `py_chu_seaborn.ipynb` - Referencia r√°pida de Seaborn
- `py_chu_stats.ipynb` - Referencia r√°pida de estad√≠stica
- `py_chu_skl.ipynb` - Referencia r√°pida de Scikit-learn

---

**√öltima actualizaci√≥n:** Este documento se basa en el an√°lisis de todos los ejercicios de `clase_jf/`, `clase_io/` y los ex√°menes de 2022, 2023 y 2024 del repositorio Python_examen_Tecnun.

**Autor:** Generado autom√°ticamente mediante an√°lisis completo del repositorio.

**Uso:** Este documento es una gu√≠a de estudio. Practica con los notebooks originales para consolidar los conceptos.
