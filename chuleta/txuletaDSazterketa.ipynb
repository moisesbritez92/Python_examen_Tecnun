{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTACIÓN DE LIBRERÍAS ESENCIALES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Al inicio de cualquier notebook de análisis de datos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Cargar todas las librerías necesarias para manipulación de datos, visualización, estadística y machine learning básico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías fundamentales\n",
    "import numpy as np                    # Operaciones numéricas y arrays\n",
    "import pandas as pd                   # Manipulación de dataframes\n",
    "import re                             # Expresiones regulares (limpieza de texto)\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt       # Gráficos básicos\n",
    "import seaborn as sns                 # Gráficos estadísticos avanzados\n",
    "\n",
    "# Estadística\n",
    "import scipy.stats                    # Tests estadísticos\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuración\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')     # Ocultar warnings molestos\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGA DE DATOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para leer archivos CSV u otros formatos de datos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Importar datasets desde archivos, especificando separadores y rutas correctas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de CSV con separador personalizado\n",
    "df = pd.read_csv('ruta/al/archivo.csv', sep=',')\n",
    "\n",
    "# Alternativas comunes:\n",
    "df = pd.read_csv('archivo.csv', sep=';')      # Separador punto y coma\n",
    "df = pd.read_csv('archivo.csv', sep='\\\t')     # Separador tabulación\n",
    "df = pd.read_excel('archivo.xlsx')            # Para Excel\n",
    "\n",
    "# Inspección inicial\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"Columnas: {df.columns.tolist()}\")\n",
    "df.head()  # Primeras 5 filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LIMPIEZA DE DATOS CON EXPRESIONES REGULARES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando hay que limpiar columnas con texto que contienen caracteres especiales, símbolos monetarios, unidades, etc.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Convertir strings con formato inconsistente a valores numéricos utilizables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN: Eliminar caracteres no deseados y convertir a numérico\n",
    "\n",
    "# Ejemplo: Columna con formato \"$123.45M\" -> 123.45\n",
    "def limpiar_columna_numerica(valor):\n",
    "    \"\"\"\n",
    "    Limpia strings con símbolos y los convierte a float\n",
    "    Ej: \"$45.2M\" -> 45.2\n",
    "    \"\"\"\n",
    "\n",
    "    # Eliminar símbolo $ y luego M (o cualquier letra)\n",
    "    limpio = re.sub(\"\\$\", \"\", str(valor))  # Quitar $\n",
    "    limpio = re.sub(\"[A-Za-z]\", \"\", limpio) # Quitar letras (M, K, etc.)\n",
    "    return float(limpio)\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar a toda una columna (opción 1: con apply)\n",
    "df['columna_limpia'] = df['columna_sucia'].apply(limpiar_columna_numerica)\n",
    "\n",
    "# Aplicar a toda una columna (opción 2: con list comprehension - más rápido)\n",
    "df['columna_limpia'] = [float(re.sub(\"M\", \"\", re.sub(\"\\$\", \"\", str(x)))) for x in df['columna_sucia'].values]   \n",
    "\n",
    "# IMPORTANTE: Filtrar valores problemáticos ANTES de convertir\n",
    "# Eliminar filas con valores como \"Unknown\", \"N/A\", etc.\n",
    "df = df[df['columna'] != \"Unknown\"].reset_index(drop=True)\n",
    "df = df[df['columna'] != \"N/A\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CREACIÓN DE VARIABLES DERIVADAS (FEATURE ENGINEERING)\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para crear nuevas columnas basadas en transformaciones de las existentes.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Generar variables más útiles para el análisis (ej: décadas desde años, categorías desde rangos, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN 1: Agrupar años en décadas\n",
    "# Si tienes años: 1987, 1993, 2001 -> décadas: 1980, 1990, 2000\n",
    "df['decada'] = df['año'] - df['año'] % 10\n",
    "\n",
    "# PATRÓN 2: Crear rangos/bins\n",
    "df['rango_edad'] = pd.cut(df['edad'], \n",
    "                          bins=[0, 18, 35, 50, 100], \n",
    "                          labels=['Niño', 'Joven', 'Adulto', 'Mayor'])\n",
    "\n",
    "# PATRÓN 3: Categorizar basado en condiciones\n",
    "df['categoria'] = df['valor'].apply(lambda x: 'Alto' if x > 50 else 'Bajo')\n",
    "\n",
    "# PATRÓN 4: Extraer información de fechas\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "df['año'] = df['fecha'].dt.year\n",
    "df['mes'] = df['fecha'].dt.month\n",
    "df['dia_semana'] = df['fecha'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AGREGACIONES Y GROUPBY\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para calcular estadísticas por grupos (conteos, promedios, sumas).\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Resumir datos agrupados por una o más categorías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN 1: Contar elementos por grupo\n",
    "conteos = df.groupby(['categoria1', 'categoria2']).count()['columna_a_contar'].reset_index()\n",
    "conteos.columns = ['categoria1', 'categoria2', 'conteo']  # Renombrar para claridad\n",
    "\n",
    "# PATRÓN 2: Calcular promedios por grupo\n",
    "promedios = df.groupby('categoria')['variable_numerica'].mean().reset_index()\n",
    "\n",
    "# PATRÓN 3: Múltiples agregaciones a la vez\n",
    "resumen = df.groupby('categoria').agg({\n",
    "    'columna1': 'mean',\n",
    "    'columna2': 'sum',\n",
    "    'columna3': 'count',\n",
    "    'columna4': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# PATRÓN 4: Filtrar grupos según condición\n",
    "# Ejemplo: Mantener solo categorías con más de N apariciones\n",
    "conteo_por_categoria = df.groupby('categoria')['id'].count()\n",
    "categorias_frecuentes = conteo_por_categoria[conteo_por_categoria > 100].index\n",
    "df_filtrado = df[df['categoria'].isin(categorias_frecuentes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SELECCIÓN DE TOP N POR GRUPO\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando necesitas los N elementos más altos/bajos de cada categoría.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Extraer rankings dentro de grupos (ej: top 5 productos por categoría).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN: Obtener top N elementos por cada grupo\n",
    "resultado_final = pd.DataFrame()  # DataFrame vacío para acumular\n",
    "\n",
    "# Iterar sobre cada grupo único\n",
    "for grupo in df['categoria'].unique():\n",
    "    # Filtrar datos del grupo actual\n",
    "    datos_grupo = df[df['categoria'] == grupo].copy()\n",
    "\n",
    "    # Ordenar por la métrica deseada y tomar top N\n",
    "    top_n = min(5, len(datos_grupo))  # Protección si hay menos de 5 elementos\n",
    "    top_elementos = datos_grupo.sort_values('metrica', ascending=False).head(top_n)\n",
    "\n",
    "    # Añadir al resultado acumulado\n",
    "    resultado_final = pd.concat([resultado_final, top_elementos], ignore_index=True)\n",
    "\n",
    "# Alternativa más eficiente con groupby (sin loop)\n",
    "top_por_grupo = (df.sort_values('metrica', ascending=False)\n",
    "                   .groupby('categoria')\n",
    "                   .head(5)  # Top 5 de cada grupo\n",
    "                   .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VISUALIZACIÓN: GRÁFICOS DE BARRAS CON FACETAS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para comparar categorías a través de múltiples grupos/subconjuntos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Crear múltiples subgráficos (facetas) organizados por una variable categórica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BARPLOT CON FACETAS (catplot)\n",
    "# Útil para mostrar distribuciones de categorías divididas por otra variable\n",
    "\n",
    "sns.catplot(\n",
    "    kind=\"bar\",              # Tipo: barplot (también: box, violin, strip, etc.)\n",
    "    data=df,                 # DataFrame con los datos\n",
    "    x=\"categoria\",           # Variable en eje X (categórica)\n",
    "    y=\"valor\",               # Variable en eje Y (numérica)\n",
    "    col=\"grupo\",             # Variable para crear facetas (columnas)\n",
    "    col_wrap=4,              # Número de columnas antes de saltar a nueva fila\n",
    "    hue=\"categoria\",         # Color según categoría (opcional)\n",
    "    sharex=False,            # Ejes X independientes por faceta\n",
    "    sharey=False,            # Ejes Y independientes por faceta\n",
    "    legend=False,            # Ocultar leyenda si es redundante\n",
    "    height=4,                # Altura de cada faceta\n",
    "    aspect=1.2               # Relación ancho/alto\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. VISUALIZACIÓN: LÍNEAS TEMPORALES CON GRUPOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para mostrar evolución temporal de múltiples grupos/categorías.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Visualizar tendencias a lo largo del tiempo, diferenciando por colores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEPLOT CON MÚLTIPLES SERIES\n",
    "# Perfecto para series temporales comparando grupos\n",
    "\n",
    "# IMPORTANTE: Filtrar categorías poco frecuentes antes de graficar\n",
    "# Esto evita gráficos saturados y mejora la interpretabilidad\n",
    "conteo_categorias = df.groupby('categoria')['id'].count()\n",
    "categorias_relevantes = conteo_categorias[conteo_categorias > 200].index\n",
    "df_filtrado = df[df['categoria'].isin(categorias_relevantes)]\n",
    "\n",
    "# Crear el gráfico de líneas\n",
    "sns.relplot(\n",
    "    kind=\"line\",             # Tipo: line (también: scatter)\n",
    "    data=df_filtrado,        # Datos filtrados\n",
    "    x=\"tiempo\",              # Variable temporal (eje X)\n",
    "    y=\"valor\",               # Variable numérica (eje Y)\n",
    "    hue=\"categoria\",         # Colorear por categoría\n",
    "    style=\"categoria\",       # Estilo de línea por categoría (opcional)\n",
    "    markers=True,            # Mostrar marcadores en puntos (opcional)\n",
    "    dashes=False,            # Líneas sólidas (True para discontinuas)\n",
    "    height=5,\n",
    "    aspect=1.5\n",
    ")\n",
    "\n",
    "plt.title(\"Evolución temporal por categoría\")\n",
    "plt.show()\n",
    "\n",
    "# NOTA: relplot con kind=\"line\" calcula automáticamente la MEDIA \n",
    "# y el intervalo de confianza para cada punto temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TESTS ESTADÍSTICOS: NORMALIDAD Y COMPARACIÓN DE GRUPOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para determinar si hay diferencias significativas entre grupos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Aplicar el test estadístico apropiado según la distribución de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1: COMPROBAR NORMALIDAD\n",
    "# Importante: Los tests paramétricos (t-test, ANOVA) requieren normalidad\n",
    "\n",
    "# Visualización de distribución\n",
    "df['variable'].hist(bins=50, edgecolor='black')\n",
    "plt.title(\"Distribución de la variable\")\n",
    "plt.show()\n",
    "\n",
    "# Test de Shapiro-Wilk (H0: los datos son normales)\n",
    "statistic, p_value = scipy.stats.shapiro(df['variable'])\n",
    "print(f\"Shapiro-Wilk p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Los datos NO son normales -> Usar tests NO paramétricos\")\n",
    "else:\n",
    "    print(\"Los datos SÍ son normales -> Usar tests paramétricos\")\n",
    "\n",
    "# REGLA GENERAL:\n",
    "# p-value < 0.05 -> Rechazamos H0 -> NO normalidad -> Tests no paramétricos\n",
    "# p-value >= 0.05 -> No rechazamos H0 -> Normalidad -> Tests paramétricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TESTS ESTADÍSTICOS: UN GROPO VS RESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: COMPARAR UN GRUPO VS EL RESTO (1 vs All)\n",
    "# Útil para detectar si una categoría es significativamente diferente\n",
    "\n",
    "from scipy.stats import ranksums, ttest_ind\n",
    "\n",
    "# Para datos NO NORMALES: Wilcoxon rank-sum test (Mann-Whitney)\n",
    "categorias = df['categoria'].unique()\n",
    "for cat in categorias:\n",
    "    grupo_actual = df['variable'][df['categoria'] == cat]\n",
    "    resto = df['variable'][df['categoria'] != cat]\n",
    "\n",
    "    # Test de rangos (NO paramétrico)\n",
    "    _, p_val = ranksums(grupo_actual, resto, alternative='greater')\n",
    "\n",
    "    if p_val < 0.05:\n",
    "        print(f\"{cat} es significativamente MAYOR que el resto (p={p_val:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "# Para datos NORMALES: T-test\n",
    "_, p_val = ttest_ind(grupo_actual, resto, alternative='greater')\n",
    "\n",
    "\n",
    "# Alternativas para 'alternative':\n",
    "# - 'greater': grupo_actual > resto\n",
    "# - 'less': grupo_actual < resto  \n",
    "# - 'two-sided': grupo_actual ≠ resto (diferente, sin dirección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 3: COMPARAR MÚLTIPLES GRUPOS SIMULTÁNEAMENTE\n",
    "from scipy.stats import kruskal, f_oneway\n",
    "\n",
    "# Preparar grupos como listas separadas\n",
    "grupos = [df['variable'][df['categoria'] == cat].values \n",
    "          for cat in df['categoria'].unique()]\n",
    "\n",
    "\n",
    "# Para datos NO NORMALES: Kruskal-Wallis (ANOVA no paramétrico)\n",
    "statistic, p_val = kruskal(*grupos)\n",
    "print(f\"Kruskal-Wallis p-value: {p_val:.4f}\")\n",
    "\n",
    "\n",
    "# Para datos NORMALES: ANOVA\n",
    "statistic, p_val = f_oneway(*grupos)\n",
    "\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"Al menos un grupo es significativamente diferente\")\n",
    "else:\n",
    "    print(\"No hay diferencias significativas entre grupos\")\n",
    "\n",
    "\n",
    "# RESUMEN DE TESTS:\n",
    "# ┌─────────────────┬──────────────────┬──────────────────────┐\n",
    "# │  Comparación    │  Paramétrico     │  No Paramétrico      │\n",
    "# ├─────────────────┼──────────────────┼──────────────────────┤\n",
    "# │  2 grupos       │  t-test          │  ranksums (Wilcoxon) │\n",
    "# │  3+ grupos      │  ANOVA           │  Kruskal-Wallis      │\n",
    "# └─────────────────┴──────────────────┴──────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PCA (ANÁLISIS DE COMPONENTES PRINCIPALES)\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para reducir dimensionalidad de datos con muchas variables, especialmente tras one-hot encoding.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Proyectar datos de alta dimensión a 2D/3D para visualización y detectar patrones/clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: REDUCCIÓN DE DIMENSIONALIDAD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PASO 1: Estandarizar los datos (CRÍTICO para PCA)\n",
    "# PCA es sensible a la escala, por lo que todas las variables deben estar\n",
    "# en la misma escala (media=0, varianza=1)\n",
    "scaler = StandardScaler()\n",
    "X_estandarizado = scaler.fit_transform(matriz_de_features)\n",
    "\n",
    "# PASO 2: Aplicar PCA\n",
    "pca = PCA(n_components=2)  # Reducir a 2 dimensiones para visualización\n",
    "X_pca = pca.fit_transform(X_estandarizado)\n",
    "\n",
    "\n",
    "# Información sobre la varianza explicada\n",
    "print(f\"Varianza explicada por PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"Varianza explicada por PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"Varianza total explicada: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# PASO 3: Crear DataFrame con componentes principales + variables originales\n",
    "df_pca = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'variable_color': df['categoria'].values,       # Para colorear gráfico\n",
    "    'variable_continua': df['valor_numerico'].values  # Para gradiente de color\n",
    "})\n",
    "\n",
    "# PASO 4: Visualizar resultados\n",
    "# Gráfico 1: Coloreado por variable categórica\n",
    "sns.relplot(data=df_pca, x='PC1', y='PC2', hue='variable_color', \n",
    "            height=6, aspect=1.3)\n",
    "plt.title(\"PCA coloreado por categoría\")\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 2: Coloreado por variable continua (gradiente)\n",
    "sns.relplot(data=df_pca, x='PC1', y='PC2', hue='variable_continua',\n",
    "            palette='viridis', height=6, aspect=1.3)\n",
    "plt.title(\"PCA coloreado por variable numérica\")\n",
    "plt.show()\n",
    "\n",
    "# INTERPRETACIÓN:\n",
    "# - Si hay clusters visibles -> los datos tienen estructura/agrupaciones\n",
    "# - Si el color (categoría) coincide con clusters -> las categorías son distinguibles\n",
    "# - PC1 y PC2 son las direcciones de máxima varianza en los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. FLUJO COMPLETO: MATRIZ ONE-HOT + FILTRADO + PCA\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Pipeline completo para analizar datos categóricos de alta dimensión.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Desde texto con múltiples valores hasta visualización en 2D pasando por filtrado y reducción dimensional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE COMPLETO: ONE-HOT -> FILTRADO -> PCA -> VISUALIZACIÓN\n",
    "\n",
    "# 1) One-hot encoding de columna con múltiples valores\n",
    "matriz_oh = df['columna_multivalor'].str.get_dummies(sep=\", \")\n",
    "\n",
    "# 2) Filtrar valores poco frecuentes (aparecer en > N filas)\n",
    "matriz_oh_filtrada = matriz_oh.loc[:, matriz_oh.sum(axis=0) > 10]\n",
    "\n",
    "# 3) Estandarizar\n",
    "X_std = StandardScaler().fit_transform(matriz_oh_filtrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. TIPS Y BUENAS PRÁCTICAS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de datos:\n",
    "\n",
    "- **Siempre** usar `.copy()` cuando trabajes con subconjuntos para evitar SettingWithCopyWarning\n",
    "\n",
    "- Preferir list comprehension sobre `.apply()` para mejor rendimiento en operaciones simples\n",
    "\n",
    "- Usar `.isin()` para filtrar por múltiples valores es más eficiente que múltiples condiciones OR\n",
    "\n",
    "- Filtrar columnas por % de NAs antes de eliminar filas ahorra datos\n",
    "\n",
    "- Usar `.sample()` frecuentemente durante desarrollo para verificar transformaciones\n",
    "\n",
    "- Desarrollar funciones complejas paso a paso, probando con 1-2 casos antes de apply\n",
    "\n",
    "\n",
    "\n",
    "### Fechas y tiempo:\n",
    "\n",
    "- Siempre convertir fechas a datetime con `pd.to_datetime()`\n",
    "\n",
    "- Para UNIX timestamps usar `unit='s'` (segundos) o `unit='ms'` (milisegundos)\n",
    "\n",
    "- Usar `.dt` para extraer componentes (year, month, day, hour, etc.)\n",
    "\n",
    "- Verificar completitud de datos temporales (último año puede estar incompleto)\n",
    "\n",
    "- Ordenar por fecha con `sort_index()` antes de graficar series temporales\n",
    "\n",
    "\n",
    "\n",
    "### Tests estadísticos:\n",
    "\n",
    "- **Siempre** verificar normalidad antes de elegir test (Shapiro-Wilk)\n",
    "\n",
    "- Si p < 0.05 en Shapiro → datos NO normales → usar tests NO paramétricos\n",
    "\n",
    "- Ranksums/Wilcoxon para 2 grupos (no paramétrico)\n",
    "\n",
    "- Kruskal-Wallis para 3+ grupos (no paramétrico)\n",
    "\n",
    "- T-test para 2 grupos (paramétrico, requiere normalidad)\n",
    "\n",
    "- ANOVA para 3+ grupos (paramétrico, requiere normalidad)\n",
    "\n",
    "\n",
    "\n",
    "### PCA:\n",
    "\n",
    "- **SIEMPRE** estandarizar datos antes de PCA (StandardScaler)\n",
    "\n",
    "- 2 componentes son suficientes para visualización\n",
    "\n",
    "- Verificar varianza explicada para evaluar calidad de reducción\n",
    "\n",
    "- PCA es útil para detectar outliers visualmente\n",
    "\n",
    "\n",
    "\n",
    "### Machine Learning - Clasificación:\n",
    "\n",
    "- Usar `class_weight='balanced'` en modelos para compensar desbalanceo\n",
    "\n",
    "- OneHotEncoder de sklearn genera matrices sparse (eficientes en memoria)\n",
    "\n",
    "- Validación cruzada (cv=5) ayuda a obtener estimaciones más robustas\n",
    "\n",
    "- Para clasificación: LogisticRegressionCV selecciona automáticamente el mejor parámetro C\n",
    "\n",
    "\n",
    "\n",
    "### Machine Learning - Regresión:\n",
    "\n",
    "- Estandarizar Y es opcional pero ayuda a convergencia\n",
    "\n",
    "- ElasticNet combina L1 (feature selection) y L2 (regularización)\n",
    "\n",
    "- l1_ratio=0 → Ridge puro, l1_ratio=1 → Lasso puro\n",
    "\n",
    "- ElasticNetCV encuentra automáticamente alpha y l1_ratio óptimos\n",
    "\n",
    "- Visualizar siempre real vs predicho para detectar patrones en errores\n",
    "\n",
    "- Distribución de errores centrada en 0 = modelo no sesgado\n",
    "\n",
    "- Comparar con regresión lineal simple para evaluar beneficio de regularización\n",
    "\n",
    "- Analizar gap train-test para detectar overfitting\n",
    "\n",
    "\n",
    "\n",
    "### Clustering:\n",
    "\n",
    "- **SIEMPRE** estandarizar datos antes de K-means (sensible a escalas)\n",
    "\n",
    "- Usar Silhouette Score para seleccionar K óptimo\n",
    "\n",
    "- Considerar complejidad: a veces K menor es más interpretable\n",
    "\n",
    "- Excluir variables identificadoras o targets del clustering\n",
    "\n",
    "- Visualizar con PCA para evaluar calidad visual de separación\n",
    "\n",
    "- `n_init='auto'` en sklearn reciente (antes era n_init=10)\n",
    "\n",
    "- K-means puede encontrar óptimos locales, usar random_state para reproducibilidad\n",
    "\n",
    "\n",
    "\n",
    "### Detección de outliers:\n",
    "\n",
    "- Identificar outliers por errores grandes en predicciones\n",
    "\n",
    "- Analizar qué features contribuyen a la anomalía\n",
    "\n",
    "- Evaluar si eliminar outliers mejora significativamente el modelo\n",
    "\n",
    "- No eliminar outliers sin justificación (pueden ser datos válidos)\n",
    "\n",
    "\n",
    "\n",
    "### Agregaciones y análisis de grupos:\n",
    "\n",
    "- `pd.NamedAgg` hace el código más legible y explícito\n",
    "\n",
    "- Usar funciones lambda para cálculos personalizados: `lambda x: (x == True).mean() * 100`\n",
    "\n",
    "- `value_counts().nlargest(N)` es más eficiente que `value_counts().sort_values().head(N)`\n",
    "\n",
    "- `.agg()` permite aplicar múltiples funciones a la vez\n",
    "\n",
    "- Usar `sort_index()` vs `sort_values()` apropiadamente:\n",
    "\n",
    "  - `sort_index()` → ordenar por categorías/índices (ej: años cronológicamente)\n",
    "\n",
    "  - `sort_values()` → ordenar por valores/frecuencias\n",
    "\n",
    "- **CRÍTICO**: Filtrar categorías con pocas muestras antes de agregar (ej: >= 5 apariciones)\n",
    "\n",
    "- Verificar tamaño de muestra con `describe()` para variables categóricas\n",
    "\n",
    "- Cuidado con sesgos: normalizar métricas cuando sea necesario (ej: comentarios por vista)\n",
    "\n",
    "\n",
    "\n",
    "### Documentación y debugging:\n",
    "\n",
    "- **SIEMPRE** leer documentación de funciones nuevas\n",
    "\n",
    "- Usar `help(función)` para ver documentación en Python\n",
    "\n",
    "- Probar código en piezas pequeñas antes de aplicar a todo el dataset\n",
    "\n",
    "- Usar `.sample()` para verificar transformaciones durante desarrollo\n",
    "\n",
    "- Verificar tipos de datos con `type()` y `.dtypes`\n",
    "\n",
    "- Comprobar que resultados tienen sentido (estadísticas descriptivas, extremos)\n",
    "\n",
    "### Orden recomendado en análisis con ML:\n",
    "1. Cargar datos y exploración inicial (shape, head, info, describe)\n",
    "2. Limpieza (valores faltantes, conversión de tipos, regex)\n",
    "3. Feature engineering (crear variables derivadas)\n",
    "4. Análisis exploratorio (groupby, agregaciones, visualizaciones)\n",
    "5. Tests estadísticos (normalidad, comparaciones)\n",
    "6. Preparación para ML (one-hot encoding, train/test split)\n",
    "7. Entrenamiento de modelos (con validación cruzada)\n",
    "8. Evaluación (múltiples métricas: AUC-PR, AUC-ROC, confusion matrix)\n",
    "9. Visualización de resultados (curvas, distribuciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Expresiones regulares comunes:\n",
    "re.sub(\"\\$\", \"\", texto)         # Eliminar símbolo $\n",
    "re.sub(\"[A-Za-z]\", \"\", texto)   # Eliminar todas las letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE II: MACHINE LEARNING Y MODELADO PREDICTIVO\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. LIMPIEZA DE DATOS: SELECCIÓN DE COLUMNAS Y ELIMINACIÓN DE NULOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Al inicio del análisis para preparar un dataset limpio.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Seleccionar solo las columnas relevantes y eliminar filas con valores faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN: Seleccionar columnas específicas y eliminar nulos\n",
    "\n",
    "# Definir columnas de interés\n",
    "columnas_relevantes = ['col1', 'col2', 'col3', 'target']\n",
    "\n",
    "# Seleccionar y limpiar en un solo paso\n",
    "df_limpio = df[columnas_relevantes].dropna()\n",
    "\n",
    "# Verificar resultado\n",
    "print(f\"Filas originales: {len(df)}\")\n",
    "print(f\"Filas tras limpieza: {len(df_limpio)}\")\n",
    "print(f\"Filas eliminadas: {len(df) - len(df_limpio)}\")\n",
    "print(f\"Columnas seleccionadas: {df_limpio.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. SELECCIÓN DE TOP N VALORES MÁS FRECUENTES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para enfocarse en las categorías más comunes y reducir ruido.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Identificar y filtrar por los N valores con mayor frecuencia en una columna categórica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN: Obtener top N valores más frecuentes y filtrar dataset\n",
    "\n",
    "# Paso 1: Contar y obtener top N (ej: top 5)\n",
    "top_n_valores = df['columna_categorica'].value_counts().nlargest(5)\n",
    "print(\"Top 5 categorías y sus frecuencias:\")\n",
    "print(top_n_valores)\n",
    "\n",
    "# Paso 2: Extraer solo los índices (los valores/nombres)\n",
    "top_n_nombres = top_n_valores.index\n",
    "print(f\"Nombres de top 5: {top_n_nombres.tolist()}\")\n",
    "\n",
    "\n",
    "# Paso 3: Filtrar el dataframe original\n",
    "df_filtrado = df[df['columna_categorica'].isin(top_n_nombres)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. VISUALIZACIÓN: COUNTPLOT CON HUE\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para visualizar frecuencias de categorías divididas por otra variable.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Mostrar conteos con barras apiladas o agrupadas coloreadas por subgrupos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTPLOT: Conteo de categorías con subdivisión por color\n",
    "plt.figure(figsize=(10, 6))  # Tamaño de figura\n",
    "sns.set_theme(style=\"darkgrid\")  # Estilo de fondo\n",
    "\n",
    "plot = sns.countplot(\n",
    "    data=df,\n",
    "    x='categoria_principal',      # Variable en eje X\n",
    "    hue='subcategoria',           # Variable para colorear/dividir barras\n",
    "    palette='Set2'                # Paleta de colores (opcional)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. AGREGACIONES AVANZADAS CON pd.NamedAgg\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando necesitas aplicar múltiples funciones de agregación y nombrarlas claramente.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Crear agregaciones complejas con nombres descriptivos de columnas, incluyendo funciones lambda personalizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGACIONES COMPLEJAS CON NamedAgg\n",
    "# Útil para calcular múltiples métricas simultáneamente\n",
    "resultado = df.groupby(['categoria1', 'categoria2']).agg(\n",
    "    # Formato: nombre_columna_resultado=pd.NamedAgg(column='columna_origen', aggfunc=función)\n",
    "    total_registros=pd.NamedAgg(\n",
    "        column='cualquier_columna',\n",
    "        aggfunc='count'\n",
    "    ),\n",
    "    porcentaje_verdaderos=pd.NamedAgg(\n",
    "        column='columna_booleana',\n",
    "        aggfunc=lambda x: (x == True).mean() * 100  # % de valores True\n",
    "    ),\n",
    ").reset_index()\n",
    "\n",
    "print(resultado.head())\n",
    "\n",
    "# FUNCIONES LAMBDA ÚTILES PARA BOOLEANOS:\n",
    "# (x == True).mean() * 100   -> Porcentaje de True\n",
    "# (x == False).mean() * 100  -> Porcentaje de False\n",
    "# (x == valor).sum()         -> Conteo de un valor específico\n",
    "# x.nunique()                -> Número de valores únicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. FILTRADO BASADO EN RESULTADO DE AGREGACIÓN\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando necesitas filtrar el dataframe original basándote en resultados de un groupby.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Seleccionar subconjuntos de datos según criterios calculados en agregaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATRÓN: Filtrar dataset original según resultado de agregación\n",
    "# Paso 1: Realizar agregación y obtener top N según criterio\n",
    "agregado = df.groupby('categoria').agg(\n",
    "    metrica=pd.NamedAgg(column='variable', aggfunc='mean')\n",
    ").reset_index()\n",
    "\n",
    "# Paso 2: Ordenar y seleccionar top N\n",
    "top_categorias = (agregado\n",
    "                  .sort_values('metrica', ascending=False)\n",
    "                  .head(3)  # Top 3\n",
    "                  ['categoria'])  # Solo la columna con nombres\n",
    "\n",
    "\n",
    "# Paso 3: Filtrar dataframe original\n",
    "df_filtrado = df[df['categoria'].isin(top_categorias)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. VISUALIZACIÓN: BARPLOT AGRUPADO CON HUE\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para mostrar valores agregados (no conteos) divididos por categorías y subgrupos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Visualizar métricas calculadas (promedios, porcentajes) con barras agrupadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BARPLOT: Para valores agregados (no conteos)\n",
    "# Diferencia con countplot: barplot muestra valores de una columna numérica\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "plot = sns.barplot(\n",
    "    data=df_agregado,            # DataFrame con datos ya agregados\n",
    "    x='categoria_principal',     # Eje X: categorías principales\n",
    "    y='metrica_calculada',       # Eje Y: valores numéricos (ej: porcentajes)\n",
    "    hue='subcategoria',          # Dividir/colorear por subcategoría\n",
    "    palette='viridis',           # Paleta de colores\n",
    "    ci=None                      # Sin barras de error (datos ya agregados)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Subcategoría', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# NOTA: Si barplot recibe datos sin agregar, calculará la media automáticamente\n",
    "# y mostrará barras de error (intervalo de confianza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. PREPARACIÓN DE DATOS PARA MACHINE LEARNING\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Antes de entrenar cualquier modelo supervisado.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Separar features (X) y target (y), y convertir tipos de datos apropiadamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARACIÓN X e y PARA MODELOS\n",
    "# Paso 1: Definir variable objetivo (y)\n",
    "y = df['variable_objetivo']\n",
    "\n",
    "# Paso 2: Definir features (X) eliminando la variable objetivo\n",
    "X = df.drop(columns=['variable_objetivo'])\n",
    "\n",
    "# Paso 3: Convertir y a tipo numérico si es necesario\n",
    "# Para clasificación binaria (True/False -> 1/0)\n",
    "y = y.astype(int)\n",
    "\n",
    "# ALTERNATIVA: Convertir categorías a números con LabelEncoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "# Verificar shapes\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")\n",
    "print(f\"Tipo de y: {y.dtype}\")\n",
    "print(\"Distribución de clases:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Columnas en X: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. ONE-HOT ENCODING CON SCIKIT-LEARN\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para convertir variables categóricas en formato numérico para modelos de ML.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Transformar categorías en matrices binarias (0/1) compatibles con algoritmos de ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-HOT ENCODING CON SKLEARN (para ML)\n",
    "# Diferencia con pandas: sklearn devuelve matriz sparse (más eficiente)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Datos categóricos (todas las columnas deben ser categóricas)\n",
    "X_categorico = df[['columna1', 'columna2', 'columna3']]\n",
    "\n",
    "# Paso 1: Crear y ajustar el encoder\n",
    "encoder = OneHotEncoder(\n",
    "    sparse_output=False,  \n",
    "    handle_unknown='ignore'  # Ignorar categorías nuevas en test\n",
    ")\n",
    "\n",
    "# Paso 2: Fit y transform\n",
    "X_encoded = encoder.fit_transform(X_categorico)\n",
    "\n",
    "# Paso 3: Obtener nombres de las columnas generadas\n",
    "feature_names = encoder.get_feature_names_out(input_features=X_categorico.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. DIVISIÓN TRAIN/TEST CON ESTRATIFICACIÓN\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Antes de entrenar modelos, especialmente con datos desbalanceados.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Dividir datos en conjuntos de entrenamiento y prueba manteniendo las proporciones de clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN/TEST SPLIT CON ESTRATIFICACIÓN\n",
    "# CRÍTICO para datos desbalanceados\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                    # Features\n",
    "    y,                    # Target\n",
    "    test_size=0.1,        # 10% para test, 90% para train\n",
    "    stratify=y,           # IMPORTANTE: Mantener proporción de clases\n",
    "    random_state=42       # Para reproducibilidad\n",
    ")\n",
    "\n",
    "# Otras opciones útiles:\n",
    "# test_size=0.2          # 20% test (más común)\n",
    "# test_size=0.3          # 30% test\n",
    "# shuffle=True           # Mezclar antes de dividir (por defecto)\n",
    "# random_state=None      # Sin seed (resultados no reproducibles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. REGRESIÓN LOGÍSTICA CON VALIDACIÓN CRUZADA Y DATOS DESBALANCEADOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para clasificación binaria con datos desbalanceados.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Entrenar modelo de clasificación con regularización L2 (Ridge), validación cruzada y balance de clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESIÓN LOGÍSTICA CON CV Y CLASS BALANCING\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Entrenar modelo\n",
    "modelo = LogisticRegressionCV(\n",
    "    Cs=10,                      # Número de valores de C (regularización) a probar\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    random_state=42,            # Reproducibilidad\n",
    "    solver='liblinear',         # Solver rápido para datasets pequeños/medianos\n",
    "    max_iter=100,               # Iteraciones máximas\n",
    "    penalty='l2',               # Regularización Ridge (L2)\n",
    "    class_weight='balanced',    # CRÍTICO: Balancear clases automáticamente\n",
    "    scoring='roc_auc'           # Métrica para seleccionar mejor C\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejor C encontrado: {modelo.C_[0]}\")\n",
    "print(f\"Score en train: {modelo.score(X_train, y_train):.4f}\") # Este es accuracy\n",
    "print(f\"Score en test: {modelo.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# EXPLICACIÓN DE PARÁMETROS CLAVE:\n",
    "# - Cs: Valores de regularización a probar (10 valores logarítmicamente espaciados)\n",
    "# - cv: Número de folds para validación cruzada\n",
    "# - penalty='l2': Regularización Ridge (penaliza coeficientes grandes)\n",
    "# - penalty='l1': Regularización Lasso (selección de features)\n",
    "# - class_weight='balanced': Ajusta pesos según frecuencia inversa de clases\n",
    "#   peso_clase_i = n_samples / (n_classes * n_samples_clase_i)\n",
    "# - solver opciones:\n",
    "#   'liblinear': Bueno para datasets pequeños/medianos, soporta L1 y L2\n",
    "#   'lbfgs': Default, bueno para multiclase\n",
    "#   'saga': Bueno para datasets grandes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. MÉTRICAS DE EVALUACIÓN: AUC-PR (PRECISION-RECALL)\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para evaluar modelos de clasificación, especialmente con datos desbalanceados.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Calcular curvas de precisión-recall y su AUC, más informativa que ROC para datos desbalanceados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURVA PRECISION-RECALL Y AUC\n",
    "# MÁS APROPIADA que ROC para datos desbalanceados\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# OPCIÓN 1: Usando predicciones binarias (menos recomendado)\n",
    "y_pred_train = modelo.predict(X_train)\n",
    "precision_train, recall_train, _ = precision_recall_curve(y_train, y_pred_train)\n",
    "auc_pr_train = auc(recall_train, precision_train)\n",
    "\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "precision_test, recall_test, _ = precision_recall_curve(y_test, y_pred_test)\n",
    "auc_pr_test = auc(recall_test, precision_test)\n",
    "\n",
    "print(f\"AUC-PR en train: {auc_pr_train:.4f}\")\n",
    "print(f\"AUC-PR en test: {auc_pr_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. VISUALIZACIÓN: DENSIDAD DE PROBABILIDADES PREDICHAS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para diagnosticar la separabilidad de clases en un modelo de clasificación.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Visualizar la distribución de probabilidades predichas para cada clase y evaluar la capacidad discriminativa del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSIDAD DE PROBABILIDADES PREDICHAS POR CLASE\n",
    "# Útil para diagnosticar qué tan bien separa el modelo las clases\n",
    "# Obtener probabilidades de la clase positiva\n",
    "y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# VISUALIZACIÓN 1: Densidad por clase (usando etiquetas verdaderas)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    y_proba[y_test == 1],   # Probabilidades para clase positiva\n",
    "    label='Clase 1 (Positiva)',\n",
    "    fill=True,\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. VISUALIZACIÓN: CURVAS ROC Y PRECISION-RECALL COMBINADAS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para evaluación completa del modelo de clasificación.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Visualizar ambas curvas en el mismo gráfico para comparar rendimiento desde diferentes perspectivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRÁFICO COMBINADO: ROC + PRECISION-RECALL\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Obtener probabilidades (MÁS APROPIADO que usar predicciones binarias)\n",
    "y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcular curva ROC\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calcular curva Precision-Recall\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Graficar ambas curvas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr,\n",
    "         label=f'ROC curve (AUC = {roc_auc:.3f})',\n",
    "         linewidth=2, color='blue')\n",
    "plt.plot(recall, precision,\n",
    "         label=f'PR curve (AUC = {pr_auc:.3f})',\n",
    "         linewidth=2, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. COMPARACIÓN: PREDICCIONES BINARIAS VS PROBABILIDADES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para entender la diferencia entre usar `.predict()` vs `.predict_proba()`.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Comprender cuándo usar predicciones discretas (0/1) versus probabilidades continuas [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICCIONES BINARIAS VS PROBABILIDADES\n",
    "# MÉTODO 1: Predicciones binarias (0 o 1)\n",
    "y_pred_binario = modelo.predict(X_test)\n",
    "print(\"Predicciones binarias (primeras 10):\")\n",
    "print(y_pred_binario[:10])\n",
    "print(f\"Tipo: {type(y_pred_binario[0])}\")\n",
    "print(f\"Valores únicos: {np.unique(y_pred_binario)}\")\n",
    "# Extraer solo probabilidad de clase positiva (columna 1)\n",
    "y_proba_positiva = modelo.predict_proba(X_test)[:, 1]\n",
    "print(\"Probabilidad de clase positiva (primeras 10):\")\n",
    "print(y_proba_positiva[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. FLUJO COMPLETO: PIPELINE DE CLASIFICACIÓN CON DATOS DESBALANCEADOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Como referencia del proceso completo de clasificación.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Pipeline end-to-end desde datos crudos hasta evaluación del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE COMPLETO DE CLASIFICACIÓN\n",
    "# 1. PREPARACIÓN DE DATOS\n",
    "columnas_relevantes = ['feature1', 'feature2', 'feature3', 'target']\n",
    "df_limpio = df[columnas_relevantes].dropna()\n",
    "\n",
    "# 2. SEPARAR X e y\n",
    "y = df_limpio['target'].astype(int)\n",
    "X_categorico = df_limpio.drop(columns=['target'])\n",
    "\n",
    "# 3. ONE-HOT ENCODING\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=True)\n",
    "X = encoder.fit_transform(X_categorico)\n",
    "\n",
    "# 4. TRAIN/TEST SPLIT CON ESTRATIFICACIÓN\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 5. ENTRENAR MODELO\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "modelo = LogisticRegressionCV(\n",
    "    Cs=10, cv=5, penalty='l2',\n",
    "    class_weight='balanced',  # Para datos desbalanceados\n",
    "    solver='liblinear', random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# 6. EVALUACIÓN\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "# Obtener probabilidades\n",
    "y_proba_test = modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcular métricas\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
    "pr_auc = auc(recall, precision)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"AUC-PR: {pr_auc:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANTE: También considerar la complejidad/interpretabilidad\n",
    "# A veces un K menor con score ligeramente inferior es preferible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. SELECCIÓN DE COLUMNAS POR TIPO DE DATO\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para separar automáticamente variables numéricas, categóricas, booleanas, etc.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Filtrar columnas según su tipo de dato (int, float, bool, object) para procesamiento diferenciado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECCIÓN AUTOMÁTICA POR TIPO DE DATO\n",
    "# Seleccionar solo columnas numéricas (int y float)\n",
    "columnas_numericas = df.select_dtypes(include=['int', 'float']).columns\n",
    "df_numerico = df[columnas_numericas]\n",
    "print(f\"Columnas numéricas ({len(columnas_numericas)}): {columnas_numericas.tolist()}\")\n",
    "\n",
    "# Seleccionar solo columnas booleanas\n",
    "columnas_booleanas = df.select_dtypes(include=['bool']).columns\n",
    "df_booleano = df[columnas_booleanas]\n",
    "print(f\"Columnas booleanas ({len(columnas_booleanas)}): {columnas_booleanas.tolist()}\")\n",
    "\n",
    "\n",
    "# CONVERTIR BOOLEANOS A 0/1\n",
    "df_booleano_int = df_booleano.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. INTEGRACIÓN DE DATAFRAMES Y ELIMINACIÓN DE COLUMNAS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para combinar diferentes subconjuntos de datos procesados y eliminar columnas irrelevantes.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Unir horizontalmente dataframes (por columnas) y limpiar variables no deseadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENAR DATAFRAMES HORIZONTALMENTE (POR COLUMNAS)\n",
    "# Ejemplo: Integrar columnas numéricas + booleanas convertidas\n",
    "df_integrado = pd.concat([df_numerico, df_booleano_int], axis=1)\n",
    "\n",
    "# axis=0 -> Concatenar verticalmente (añadir filas)\n",
    "# axis=1 -> Concatenar horizontalmente (añadir columnas)\n",
    "\n",
    "# ELIMINAR COLUMNAS ESPECÍFICAS\n",
    "df_limpio = df_integrado.drop(columns=['columna_irrelevante', 'otra_columna'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. FILTRADO DE COLUMNAS POR PORCENTAJE DE VALORES NULOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para eliminar automáticamente columnas con demasiados valores faltantes.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Identificar y eliminar columnas según un umbral de porcentaje de NAs (ej: >= 25%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTRAR COLUMNAS POR PORCENTAJE DE NAs\n",
    "# Paso 1: Calcular porcentaje de NAs por columna\n",
    "porcentaje_nulos = df.isnull().mean() * 100\n",
    "\n",
    "# Paso 2: Crear máscara booleana (True = columna válida, False = eliminar)\n",
    "columnas_validas = porcentaje_nulos < 25\n",
    "\n",
    "# Paso 3: Filtrar dataframe\n",
    "df_filtrado = df.loc[:, columnas_validas]\n",
    "\n",
    "print(f\"Columnas originales: {df.shape[1]}\")\n",
    "print(f\"Columnas eliminadas: {df.shape[1] - df_filtrado.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. ESTANDARIZACIÓN DE VARIABLE OBJETIVO (Y)\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** En regresión cuando se necesita normalizar también la variable objetivo.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Estandarizar Y (mean=0, std=1) para mejorar convergencia de modelos y comparabilidad de errores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTANDARIZACIÓN DE Y (VARIABLE OBJETIVO)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MÉTODO 1: Manual (simple)\n",
    "Y = df['variable_objetivo']\n",
    "media_Y = Y.mean()\n",
    "std_Y = Y.std()\n",
    "Y_escalado = (Y - media_Y) / std_Y\n",
    "\n",
    "print(f\"Media original: {media_Y:.2f}\")\n",
    "print(f\"Std original: {std_Y:.2f}\")\n",
    "print(f\"Media escalada: {Y_escalado.mean():.6f}\")\n",
    "print(f\"Std escalada: {Y_escalado.std():.6f}\")\n",
    "\n",
    "# MÉTODO 2: Con StandardScaler (recomendado para train/test)\n",
    "# Ventaja: Permite aplicar la misma transformación a test\n",
    "scaler_Y = StandardScaler()\n",
    "Y_escalado = scaler_Y.fit_transform(Y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# reshape(-1, 1) convierte array 1D a 2D (requerido por StandardScaler)\n",
    "# ravel() convierte de vuelta a 1D\n",
    "\n",
    "# IMPORTANTE: Para train/test, hacer:\n",
    "# 1. Ajustar scaler solo con Y_train\n",
    "scaler_Y = StandardScaler()\n",
    "Y_train_escalado = scaler_Y.fit_transform(Y_train.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# 2. Transformar Y_test con el mismo scaler (SIN fit)\n",
    "Y_test_escalado = scaler_Y.transform(Y_test.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. ELASTIC NET PARA REGRESIÓN CON SELECCIÓN DE FEATURES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Regresión con muchas features, necesidad de selección automática de variables.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Combinar L1 (Lasso) y L2 (Ridge) para regularización y selección de features simultáneamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELASTIC NET CON VALIDACIÓN CRUZADA\n",
    "# Combina Lasso (L1) y Ridge (L2) regularization\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Preparar datos (después de train/test split y escalado)\n",
    "# X_train_scaled, X_test_scaled, Y_train_scaled, Y_test_scaled\n",
    "\n",
    "# Definir grids de hiperparámetros\n",
    "m = 11\n",
    "l1_ratios = np.linspace(0, 1, m)  # 0=Ridge puro, 1=Lasso puro\n",
    "alphas = np.logspace(-10, 10, m)  # Valores de regularización\n",
    "\n",
    "# Entrenar modelo con CV\n",
    "modelo = ElasticNetCV(\n",
    "    alphas=alphas,           # Valores de alpha (fuerza de regularización)\n",
    "    l1_ratio=l1_ratios,      # Balance entre L1 y L2\n",
    "    cv=10,                   # 10-fold cross-validation\n",
    "    random_state=42,         # Reproducibilidad\n",
    "    max_iter=10000           # Iteraciones máximas (aumentar si no converge)\n",
    ").fit(X_train_scaled, Y_train_scaled)\n",
    "\n",
    "# PREDICCIONES Y EVALUACIÓN\n",
    "# En train\n",
    "Y_pred_train = modelo.predict(X_train_scaled)\n",
    "explained_var_train = explained_variance_score(Y_train_scaled, Y_pred_train)\n",
    "corr_train = spearmanr(Y_train_scaled, Y_pred_train)[0]\n",
    "\n",
    "# En test\n",
    "Y_pred_test = modelo.predict(X_test_scaled)\n",
    "explained_var_test = explained_variance_score(Y_test_scaled, Y_pred_test)\n",
    "corr_test = spearmanr(Y_test_scaled, Y_pred_test)[0]\n",
    "\n",
    "# ANÁLISIS DE FEATURES SELECCIONADAS\n",
    "betas = modelo.coef_\n",
    "n_features_activas = sum(abs(betas) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. DIAGNÓSTICO Y VISUALIZACIÓN DE MODELOS DE REGRESIÓN\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para evaluar visualmente el rendimiento de modelos de regresión.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Detectar patrones en errores, outliers, y evaluar calidad de predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACIÓN DE DIAGNÓSTICO PARA REGRESIÓN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GRÁFICO 1: Scatter Plot Real vs Predicho\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(Y_test_scaled, Y_pred_test, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "# Línea de predicción perfecta (45 grados)\n",
    "plt.plot([Y_test_scaled.min(), Y_test_scaled.max()],\n",
    "         [Y_test_scaled.min(), Y_test_scaled.max()],\n",
    "         'r--', linewidth=2, label='Predicción perfecta')\n",
    "plt.xlabel('Valor Real')\n",
    "plt.ylabel('Valor Predicho')\n",
    "plt.title('Real vs Predicho (Test Set)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ESTADÍSTICAS DE ERRORES\n",
    "print(\"=== ANÁLISIS DE ERRORES ===\")\n",
    "print(f\"Error medio: {errores.mean():.4f}\")\n",
    "print(f\"Error absoluto medio: {abs(errores).mean():.4f}\")\n",
    "print(f\"Error std: {errores.std():.4f}\")\n",
    "print(f\"Error máximo: {abs(errores).max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. DETECCIÓN DE OUTLIERS CON PCA\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para identificar observaciones atípicas que afectan el modelo.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Usar PCA para visualizar outliers y analizar qué features contribuyen a su anomalía.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECCIÓN Y ANÁLISIS DE OUTLIERS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paso 1: Identificar observación con mayor error\n",
    "errores = abs(Y_test_scaled - Y_pred_test)\n",
    "indice_outlier = np.argmax(errores)\n",
    "print(f\"Observación con mayor error: índice {indice_outlier}\")\n",
    "print(f\"Error: {errores[indice_outlier]:.4f}\")\n",
    "\n",
    "# Paso 2: Proyectar datos en PCA para visualizar\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c='lightblue', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_pca[indice_outlier, 0], X_pca[indice_outlier, 1],\n",
    "            c='red', s=200, marker='X', edgecolors='black', linewidth=2,\n",
    "            label=f'Outlier (índice {indice_outlier})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Detección de Outlier con PCA')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Paso 3: Identificar qué feature contribuye más a PC1\n",
    "# (PC1 suele capturar la mayor variabilidad)\n",
    "correlaciones = []\n",
    "for i in range(X_test_scaled.shape[1]):\n",
    "    corr = np.corrcoef(X_test_scaled[:, i], X_pca[:, 0])[0, 1]\n",
    "    correlaciones.append(abs(corr))  # Valor absoluto para magnitud\n",
    "\n",
    "indice_feature_max = np.argmax(correlaciones)\n",
    "print(f\"Feature con mayor correlación con PC1: índice {indice_feature_max}\")\n",
    "print(f\"Correlación: {correlaciones[indice_feature_max]:.4f}\")s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. K-MEANS CLUSTERING\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para agrupar datos en clusters sin supervisión.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Particionar datos en K grupos homogéneos internamente y heterogéneos entre sí.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-MEANS CLUSTERING BÁSICO\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# IMPORTANTE: Estandarizar datos antes de clustering\n",
    "# K-means es sensible a escalas diferentes entre variables\n",
    "X = df.drop(columns=['variable_a_excluir'])  # Ej: no usar precio en clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Entrenar K-means con K clusters\n",
    "K = 3  # Número de clusters (luego veremos cómo elegirlo)\n",
    "kmeans = KMeans(\n",
    "    n_clusters=K,\n",
    "    n_init='auto',      # Número de inicializaciones (auto en sklearn reciente)\n",
    "    random_state=42,    # Reproducibilidad\n",
    "    max_iter=300        # Iteraciones máximas\n",
    ")\n",
    "\n",
    "# Ajustar y obtener etiquetas\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Convertir etiquetas (0, 1, 2...) a (1, 2, 3...)\n",
    "labels = labels + 1\n",
    "\n",
    "# Añadir al dataframe original\n",
    "df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. SELECCIÓN ÓPTIMA DE K CON ÍNDICE DE SILHOUETTE\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para determinar el número óptimo de clusters en K-means.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Evaluar la calidad del clustering para diferentes valores de K y seleccionar el mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÚSQUEDA DEL K ÓPTIMO CON SILHOUETTE SCORE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Rango de K a probar (mínimo 2, máximo a elegir)\n",
    "K_max = 20\n",
    "K_range = range(2, K_max + 1)\n",
    "silhouette_scores = []\n",
    "\n",
    "# Probar cada K\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k}, Silhouette Score={score:.4f}\")\n",
    "\n",
    "# Visualizar resultados\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K_range, silhouette_scores, marker='o', linestyle='-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. VISUALIZACIÓN DE CLUSTERS CON PCA\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para visualizar clusters en 2D cuando los datos tienen muchas dimensiones.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Proyectar datos clusterizados en 2 componentes principales para evaluación visual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACIÓN DE CLUSTERS CON PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Entrenar solución final con K óptimo\n",
    "kmeans_final = KMeans(n_clusters=K_optimo, n_init='auto', random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Ajustar etiquetas para empezar en 1\n",
    "labels_final = labels_final + 1\n",
    "\n",
    "# Aplicar PCA para visualización 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0],\n",
    "    X_pca[:, 1],\n",
    "    c=labels_final,           # Color por cluster\n",
    "    cmap='tab10',             # Paleta de colores\n",
    "    alpha=0.6,                # Transparencia\n",
    "    edgecolors='k',           # Borde negro\n",
    "    linewidth=0.5,\n",
    "    s=50                      # Tamaño de puntos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. COMPARACIÓN DE MODELOS: CON Y SIN REGULARIZACIÓN\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para entender el impacto de la regularización.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Comparar rendimiento de regresión lineal simple vs modelos regularizados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARACIÓN: LINEAR REGRESSION VS ELASTIC NET\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Modelo sin regularización\n",
    "modelo_simple = LinearRegression()\n",
    "modelo_simple.fit(X_train_scaled, Y_train_scaled)\n",
    "\n",
    "# Evaluación\n",
    "score_train_simple = modelo_simple.score(X_train_scaled, Y_train_scaled)\n",
    "score_test_simple = modelo_simple.score(X_test_scaled, Y_test_scaled)\n",
    "\n",
    "print(\"Regresión Lineal Simple (sin regularización):\")\n",
    "print(f\"  R² Train: {score_train_simple:.4f}\")\n",
    "print(f\"  R² Test: {score_test_simple:.4f}\")\n",
    "print(f\"  Features usadas: {X_train_scaled.shape[1]}/{X_train_scaled.shape[1]}\")\n",
    "\n",
    "print(\"Elastic Net (con regularización):\")\n",
    "print(f\"  R² Train: {explained_var_train:.4f}\")\n",
    "print(f\"  R² Test: {explained_var_test:.4f}\")\n",
    "print(f\"  Features usadas: {n_features_activas}/{X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE IV: MANIPULACIÓN AVANZADA DE DATOS CON PANDAS\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. EXPLORACIÓN INICIAL DE DATOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Siempre al inicio de cualquier análisis.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Entender la estructura del dataset: dimensiones, tipos de datos y valores faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORACIÓN INICIAL COMPLETA\n",
    "\n",
    "# Ver primeras filas\n",
    "df.head()  # Por defecto muestra 5 filas\n",
    "df.head(10)  # Mostrar 10 filas\n",
    "\n",
    "# Ver últimas filas\n",
    "df.tail()  # Por defecto 5 filas\n",
    "\n",
    "# Dimensiones del dataset\n",
    "print(f\"Shape: {df.shape}\")  # (n_filas, n_columnas)\n",
    "print(f\"Número de filas: {df.shape[0]}\")\n",
    "print(f\"Número de columnas: {df.shape[1]}\")\n",
    "\n",
    "# Tipos de datos de cada columna\n",
    "print(\"Tipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Información general resumida\n",
    "df.info()\n",
    "\n",
    "# Estadísticas descriptivas de columnas numéricas\n",
    "df.describe()\n",
    "\n",
    "# Estadísticas de columnas categóricas\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. CREACIÓN DE VARIABLES DERIVADAS CON OPERACIONES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para calcular métricas o ratios entre columnas existentes.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Crear nuevas columnas basadas en operaciones aritméticas entre columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR NUEVAS COLUMNAS CON OPERACIONES\n",
    "\n",
    "# División entre dos columnas\n",
    "df['ratio'] = df['columna_numerador'] / df['columna_denominador']\n",
    "\n",
    "# Ejemplo: Comentarios por vista\n",
    "df['comments_per_view'] = df['comments'] / df['views']\n",
    "\n",
    "# Operaciones inversas\n",
    "df['views_per_comment'] = df['views'] / df['comments']\n",
    "\n",
    "# Multiplicación\n",
    "df['producto'] = df['col1'] * df['col2']\n",
    "\n",
    "# Suma\n",
    "df['total'] = df['col1'] + df['col2'] + df['col3']\n",
    "\n",
    "# Resta\n",
    "df['diferencia'] = df['col1'] - df['col2']\n",
    "\n",
    "# Operaciones más complejas\n",
    "df['metrica_compuesta'] = (df['col1'] * 0.5 + df['col2'] * 0.3) / df['col3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. ORDENAMIENTO Y SELECCIÓN DE EXTREMOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para encontrar valores máximos o mínimos de una variable.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Ordenar el dataframe y extraer las observaciones con valores más altos/bajos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDENAMIENTO Y SELECCIÓN DE EXTREMOS\n",
    "\n",
    "# Ordenar por una columna (ascendente por defecto)\n",
    "df_ordenado = df.sort_values('columna')\n",
    "\n",
    "# Ordenar descendente\n",
    "df_ordenado = df.sort_values('columna', ascending=False)\n",
    "\n",
    "# Obtener los N valores más ALTOS (tail después de ordenar ascendente)\n",
    "top_valores = df.sort_values('columna', ascending=True).tail(10)\n",
    "\n",
    "# Obtener los N valores más BAJOS (head después de ordenar ascendente)\n",
    "bottom_valores = df.sort_values('columna', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. VISUALIZACIÓN APROPIADA SEGÚN TIPO DE DATO\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para elegir el gráfico correcto según el tipo de variable.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Evitar gráficos inapropiados (ej: time series cuando no hay tiempo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEGIR VISUALIZACIÓN APROPIADA\n",
    "\n",
    "# GRÁFICO DE LÍNEAS: Solo para series temporales o datos ordenados\n",
    "# ✅ CORRECTO: Para datos sin orden temporal, usar histograma\n",
    "df['variable_numerica'].plot(kind='hist')\n",
    "\n",
    "# HISTOGRAMA: Para distribuciones de variables continuas\n",
    "df['variable_numerica'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "\n",
    "# BOXPLOT: Para detectar outliers y ver distribución\n",
    "df['variable'].plot(kind='box')\n",
    "\n",
    "# BARPLOT: Para conteos de categorías\n",
    "df['categoria'].value_counts().plot(kind='bar')\n",
    "\n",
    "# SCATTER: Para relación entre dos variables\n",
    "df.plot(kind='scatter', x='var1', y='var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. FILTRADO CON QUERY Y MÚLTIPLES MÉTODOS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para filtrar datos según condiciones.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Conocer las diferentes formas de filtrar filas en pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MÉTODOS DE FILTRADO EN PANDAS\n",
    "\n",
    "# MÉTODO 1: Indexación booleana (más común)\n",
    "df_filtrado = df[df['columna'] < 1000]\n",
    "\n",
    "# MÉTODO 2: .loc con condición\n",
    "df_filtrado = df.loc[df['columna'] < 1000]\n",
    "\n",
    "# MÉTODO 3: .query() - Más legible para condiciones complejas\n",
    "df_filtrado = df.query('columna < 1000')\n",
    "\n",
    "# Query con múltiples condiciones\n",
    "df_filtrado = df.query('columna1 < 1000 and columna2 > 50')\n",
    "df_filtrado = df.query('columna1 < 1000 or columna2 > 50')\n",
    "\n",
    "# Query con variables externas (usar @)\n",
    "umbral = 1000\n",
    "df_filtrado = df.query('columna < @umbral')\n",
    "\n",
    "# Query con operadores in\n",
    "categorias = ['cat1', 'cat2', 'cat3']\n",
    "df_filtrado = df.query('categoria in @categorias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. CONVERSIÓN Y MANEJO DE FECHAS (DATETIME)\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando se trabaja con fechas en diferentes formatos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Convertir timestamps, strings o formatos especiales a datetime de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSIÓN A DATETIME\n",
    "\n",
    "# Desde string con formato estándar\n",
    "df['fecha'] = pd.to_datetime(df['fecha_string'])\n",
    "\n",
    "# Desde UNIX timestamp (segundos desde 1970-01-01)\n",
    "df['fecha'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "# Verificar tipo de dato\n",
    "print(df['fecha_procesada'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. EXTRACCIÓN DE COMPONENTES DE FECHAS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para extraer año, mes, día, etc. de columnas datetime.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Crear variables derivadas desde fechas para análisis temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACCIÓN DE COMPONENTES CON .dt\n",
    "# IMPORTANTE: Solo funciona si la columna es tipo datetime\n",
    "\n",
    "# Extraer año\n",
    "df['año'] = df['fecha'].dt.year\n",
    "\n",
    "# Extraer mes (1-12)\n",
    "df['mes'] = df['fecha'].dt.month\n",
    "\n",
    "# Extraer día del mes (1-31)\n",
    "df['dia'] = df['fecha'].dt.day\n",
    "\n",
    "# Extraer hora, minuto, segundo\n",
    "df['hora'] = df['fecha'].dt.hour\n",
    "df['minuto'] = df['fecha'].dt.minute\n",
    "df['segundo'] = df['fecha'].dt.second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. MUESTREO ALEATORIO CON SAMPLE\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para inspeccionar datos aleatoriamente o crear subconjuntos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Obtener muestras aleatorias para verificación o análisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUESTREO ALEATORIO\n",
    "\n",
    "# Obtener N filas aleatorias\n",
    "muestra = df.sample(10)  # 10 filas aleatorias\n",
    "\n",
    "# Muestra aleatoria de columnas específicas\n",
    "muestra = df[['col1', 'col2', 'col3']].sample(5)\n",
    "\n",
    "# Muestra con reemplazo (puede repetir filas)\n",
    "muestra = df.sample(100, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. BÚSQUEDA DE PATRONES EN STRINGS CON .str.contains()\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para filtrar o verificar presencia de texto en columnas de strings.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Detectar si un patrón o palabra está presente en valores de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÚSQUEDA DE PATRONES EN STRINGS\n",
    "\n",
    "# Verificar si una palabra está en cada valor\n",
    "contiene_palabra = df['columna_texto'].str.contains('palabra')\n",
    "# Devuelve Serie booleana (True/False)\n",
    "\n",
    "# Filtrar filas que contienen la palabra\n",
    "df_filtrado = df[df['columna_texto'].str.contains('palabra')]\n",
    "\n",
    "# Case-insensitive (ignorar mayúsculas/minúsculas)\n",
    "df['columna'].str.contains('palabra', case=False)\n",
    "\n",
    "# Búsqueda con expresiones regulares\n",
    "df['columna'].str.contains('patron.*regex', regex=True)\n",
    "\n",
    "# EJEMPLO PRÁCTICO: Verificar completitud de datos\n",
    "# ¿Todas las filas tienen cierta palabra en sus ratings?\n",
    "tiene_rating_funny = df['ratings'].str.contains('Funny').all()\n",
    "\n",
    "if tiene_rating_funny:\n",
    "    print(\"✓ Todas las filas tienen rating 'Funny'\")\n",
    "else:\n",
    "    print(\"✗ Algunas filas no tienen rating 'Funny'\")\n",
    "    n_sin_funny = (~df['ratings'].str.contains('Funny')).sum()\n",
    "    print(f\"Filas sin 'Funny': {n_sin_funny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. CONVERSIÓN DE STRINGS A ESTRUCTURAS DE DATOS CON ast.literal_eval\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Cuando se tienen strings que representan listas, diccionarios o estructuras de Python.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Convertir strings con formato de Python (\"[1, 2, 3]\", \"{' key': 'value'}\") a objetos reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSIÓN DE STRINGS A ESTRUCTURAS DE DATOS\n",
    "\n",
    "import ast\n",
    "# Ver tipo original\n",
    "print(type(df['columna_con_lista'][0]))  # <class 'str'>\n",
    "\n",
    "# Ejemplo simple\n",
    "string_lista = \"[1, 2, 3, 4, 5]\"\n",
    "lista_real = ast.literal_eval(string_lista)\n",
    "print(type(lista_real))  # <class 'list'>\n",
    "print(lista_real)  # [1, 2, 3, 4, 5]\n",
    "\n",
    "# APLICAR A TODA UNA COLUMNA con .apply()\n",
    "df['columna_convertida'] = df['columna_string'].apply(ast.literal_eval)\n",
    "\n",
    "# VERIFICAR CONVERSIÓN\n",
    "print(\"Antes:\")\n",
    "print(type(df['columna_string'][0]))  # str\n",
    "print(\"Después:\")\n",
    "print(type(df['columna_convertida'][0]))  # list o dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE III: PREPROCESAMIENTO AVANZADO Y CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. ANÁLISIS DE FRECUENCIAS Y ORDENAMIENTO CON VALUE_COUNTS\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para contar ocurrencias de valores únicos en una columna.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Obtener distribuciones de frecuencias y ordenarlas apropiadamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE_COUNTS Y ORDENAMIENTO\n",
    "\n",
    "# Conteo básico de frecuencias\n",
    "conteos = df['columna_categorica'].value_counts()\n",
    "print(conteos)\n",
    "\n",
    "# Con proporciones (porcentajes)\n",
    "proporciones = df['columna_categorica'].value_counts(normalize=True)\n",
    "porcentajes = df['columna_categorica'].value_counts(normalize=True) * 100\n",
    "\n",
    "# VISUALIZAR VALUE_COUNTS\n",
    "# Gráfico de barras por frecuencia\n",
    "df['categoria'].value_counts().plot(kind='bar')\n",
    "plt.title('Frecuencia por categoría')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de líneas temporal (después de sort_index)\n",
    "df['año'].value_counts().sort_index().plot()\n",
    "plt.title('Evolución temporal')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.show()\n",
    "\n",
    "# IMPORTANTE: Diferencia entre sort_values y sort_index\n",
    "# sort_values() → ordena por los valores (frecuencias)\n",
    "# sort_index() → ordena por el índice (las categorías/años)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. AGREGACIONES MÚLTIPLES CON AGG\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para calcular múltiples estadísticas simultáneamente sobre grupos.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Obtener varias métricas (count, mean, sum, etc.) en una sola operación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGACIONES MÚLTIPLES CON .agg()\n",
    "\n",
    "# CASO 1: Varias funciones sobre UNA columna\n",
    "resultado = df.groupby('categoria')['valor'].agg(['count', 'mean', 'sum'])\n",
    "print(resultado)\n",
    "\n",
    "# Funciones disponibles como strings:\n",
    "# 'count', 'sum', 'mean', 'median', 'std', 'var', 'min', 'max', 'first', 'last'\n",
    "\n",
    "# CASO 2: Diferentes funciones sobre DIFERENTES columnas\n",
    "resultado = df.groupby('categoria').agg({\n",
    "    'columna1': 'mean',\n",
    "    'columna2': 'sum',\n",
    "    'columna3': ['min', 'max'],\n",
    "    'columna4': 'count'\n",
    "})\n",
    "\n",
    "# ORDENAR RESULTADOS\n",
    "resultado = (df.groupby('evento')['visualizaciones']\n",
    "             .agg(['count', 'mean', 'sum'])\n",
    "             .sort_values('mean', ascending=False))\n",
    "\n",
    "# RESETEAR ÍNDICE para mejor manipulación\n",
    "resultado = (df.groupby('evento')['visualizaciones']\n",
    "             .agg(['count', 'mean', 'sum'])\n",
    "             .reset_index()\n",
    "             .sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. FILTRADO CON .isin() PARA MÚLTIPLES VALORES\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para filtrar filas donde una columna contiene uno de varios valores posibles.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Seleccionar subconjuntos basados en listas de valores permitidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTRADO CON .isin()\n",
    "\n",
    "# Definir lista de valores permitidos\n",
    "valores_deseados = ['valor1', 'valor2', 'valor3']\n",
    "\n",
    "# Filtrar filas que tienen alguno de esos valores\n",
    "df_filtrado = df[df['columna'].isin(valores_deseados)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. ANÁLISIS DE COLUMNAS CATEGÓRICAS CON describe()\n",
    "\n",
    "\n",
    "\n",
    "**Cuándo usar:** Para entender distribuciones de variables categóricas.\n",
    "\n",
    "\n",
    "\n",
    "**Propósito:** Obtener estadísticas sobre textos: valores únicos, más frecuente, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTADÍSTICAS DE VARIABLES CATEGÓRICAS\n",
    "\n",
    "# Para variables numéricas (default)\n",
    "df.describe()\n",
    "\n",
    "# Para variables categóricas (tipo object/string)\n",
    "df.describe(include='object')\n",
    "\n",
    "# Describe de una columna específica\n",
    "df['columna_categorica'].describe()\n",
    "\n",
    "# Información que proporciona describe() para categóricas:\n",
    "# - count: Número de valores no-NaN\n",
    "# - unique: Número de valores únicos\n",
    "# - top: Valor más frecuente\n",
    "# - freq: Frecuencia del valor más frecuente\n",
    "\n",
    "# EJEMPLO\n",
    "stats = df['ocupacion'].describe()\n",
    "print(f\"Total registros: {stats['count']}\")\n",
    "print(f\"Ocupaciones únicas: {stats['unique']}\")\n",
    "print(f\"Ocupación más común: {stats['top']}\")\n",
    "print(f\"Aparece {stats['freq']} veces\")\n",
    "\n",
    "# MÉTODOS COMPLEMENTARIOS\n",
    "# Número de valores únicos\n",
    "n_unicos = df['columna'].nunique()\n",
    "\n",
    "# Lista de valores únicos\n",
    "valores = df['columna'].unique()\n",
    "\n",
    "# Valor más frecuente\n",
    "mas_frecuente = df['columna'].mode()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
