{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f14322",
   "metadata": {},
   "source": [
    "# Cheat Sheet: Clasificacion de Teratogenicidad con Regresion Logistica\n",
    "\n",
    "Este cuaderno expande el ejercicio original incorporando explicaciones paso a paso, mejores practicas y codigo comentado para replicar un flujo completo de machine learning supervisado en un problema de clasificacion binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d784fd",
   "metadata": {},
   "source": [
    "## Indice de Acciones\n",
    "- [0. Contexto del Problema](#0-contexto-del-problema)\n",
    "- [1. Preparacion del Entorno](#1-preparacion-del-entorno)\n",
    "  - [1.1. Importacion de Librerias](#11-importacion-de-librerias)\n",
    "  - [1.2. Configuracion General](#12-configuracion-general)\n",
    "- [2. Comprension y Limpieza de Datos](#2-comprension-y-limpieza-de-datos)\n",
    "  - [2.1. Carga y Exploracion Inicial](#21-carga-y-exploracion-inicial)\n",
    "  - [2.2. Preparacion de Matrices X e y](#22-preparacion-de-matrices-x-e-y)\n",
    "- [3. Preprocesamiento](#3-preprocesamiento)\n",
    "  - [3.1. Variance Threshold](#31-variance-threshold)\n",
    "  - [3.2. Estandarizacion](#32-estandarizacion)\n",
    "- [4. Split de Entrenamiento y Test](#4-split-de-entrenamiento-y-test)\n",
    "- [5. Modelo 1: Logistic Regression con Lasso](#5-modelo-1-logistic-regression-con-lasso)\n",
    "- [6. Modelo 2: Logistic Regression con Elastic Net](#6-modelo-2-logistic-regression-con-elastic-net)\n",
    "- [7. Seleccion de Features con SelectKBest](#7-seleccion-de-features-con-selectkbest)\n",
    "- [8. Comparativa y Siguientes Pasos](#8-comparativa-y-siguientes-pasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a981e",
   "metadata": {},
   "source": [
    "## 0. Contexto del Problema\n",
    "\n",
    "El dataset `teratogenicity_data.txt` contiene medidas quimicas y una etiqueta binaria `Teratogenicity` que indica si una sustancia es teratogenica. El objetivo es construir modelos de clasificacion evaluados con AUC-ROC y accuracy, ademas de analizar la complejidad en terminos de features utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e6c5",
   "metadata": {},
   "source": [
    "## 1. Preparacion del Entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5249eec",
   "metadata": {},
   "source": [
    "### 1.1. Importacion de Librerias\n",
    "Incluimos librerias para manipulacion de datos, modelado y utilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22901e1f",
   "metadata": {},
   "source": [
    "### 1.2. Configuracion General\n",
    "Silenciamos warnings y fijamos semilla para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4f2f0",
   "metadata": {},
   "source": [
    "## 2. Comprension y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf4a4f",
   "metadata": {},
   "source": [
    "### 2.1. Carga y Exploracion Inicial\n",
    "Leemos el archivo, revisamos dimensiones y verificamos valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277daf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('.', 'data', 'teratogenicity_data.txt')\n",
    "df = pd.read_csv(DATA_PATH).set_index('ID')\n",
    "\n",
    "print(f'Forma del dataset: {df.shape}')\n",
    "print('\\nPrimeras filas:')\n",
    "display(df.head())\n",
    "\n",
    "print('\\nResumen estadistico:')\n",
    "display(df.describe())\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "print('\\nValores nulos por columna:')\n",
    "if missing.sum() == 0:\n",
    "    print('No se detectaron valores nulos')\n",
    "else:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf65c8",
   "metadata": {},
   "source": [
    "### 2.2. Preparacion de Matrices X e y\n",
    "Separamos features y target. La etiqueta usa valores {-1, 1}; la convertimos a {0, 1} para una clasificacion binaria estandar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Teratogenicity'].replace({-1: 0}).values\n",
    "feature_cols = [col for col in df.columns if col != 'Teratogenicity']\n",
    "X = df[feature_cols].values\n",
    "\n",
    "print(f'Shape X: {X.shape}')\n",
    "print(f'Shape y: {y.shape}')\n",
    "print(f'Clases en y: {np.unique(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff3d72",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6838c78",
   "metadata": {},
   "source": [
    "### 3.1. Variance Threshold\n",
    "Eliminamos columnas casi constantes porque aportan poca informacion discriminante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00991de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos un filtro básico para eliminar columnas casi constantes, lo que no aporta información al modelo\n",
    "var_selector = VarianceThreshold(threshold=0.5)\n",
    "var_selector.fit(X)\n",
    "\n",
    "variance_mask = var_selector.get_support()\n",
    "feature_names_reduced = np.array(feature_cols)[variance_mask]\n",
    "\n",
    "print(f'Features originales: {X.shape[1]}')\n",
    "print(f'Features tras filtrar por varianza: {feature_names_reduced.size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cf642",
   "metadata": {},
   "source": [
    "### 3.2. Estandarizacion\n",
    "La estandarización alinea las escalas de los predictores. Ajustaremos el `StandardScaler` únicamente con los datos de entrenamiento para evitar fuga de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "print('Scaler instanciado; se ajustará tras crear el conjunto de entrenamiento.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071dd83",
   "metadata": {},
   "source": [
    "## 4. Split de Entrenamiento y Test\n",
    "Dividimos el dataset al 50% manteniendo la proporción de clases (`stratify`). También aplicamos el filtro de varianza y el escalado usando únicamente información del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.5,\n",
    "    stratify=y,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "var_selector_train = VarianceThreshold(threshold=var_selector.threshold)\n",
    "var_selector_train.fit(X_train_raw)\n",
    "\n",
    "train_mask = var_selector_train.get_support()\n",
    "selected_features = np.array(feature_cols)[train_mask]\n",
    "\n",
    "X_train_filtered = var_selector_train.transform(X_train_raw)\n",
    "X_test_filtered = var_selector_train.transform(X_test_raw)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_filtered)\n",
    "X_test = scaler.transform(X_test_filtered)\n",
    "\n",
    "print(f\"Tamaño X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño X_test: {X_test.shape}\")\n",
    "print(f\"Features seleccionadas tras preprocesado: {selected_features.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db2e92",
   "metadata": {},
   "source": [
    "## 5. Modelo 1: Logistic Regression con Lasso\n",
    "Entrenamos una regresión logística con regularización L1 (`penalty='l1'`) para inducir sparsity. Utilizamos `LogisticRegressionCV` para seleccionar automáticamente la intensidad de regularización (`C`) optimizando AUC-ROC mediante validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f33225",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_clf = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    scoring='roc_auc',\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "lasso_train_proba = lasso_clf.predict_proba(X_train)[:, 1]\n",
    "lasso_test_proba = lasso_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "lasso_metrics = {\n",
    "    'cv_mean_auc': lasso_clf.scores_[1].mean(axis=0)[np.where(lasso_clf.Cs_ == lasso_clf.C_[0])[0][0]],\n",
    "    'train_auc': roc_auc_score(y_train, lasso_train_proba),\n",
    "    'test_auc': roc_auc_score(y_test, lasso_test_proba),\n",
    "    'train_accuracy': accuracy_score(y_train, lasso_clf.predict(X_train)),\n",
    "    'test_accuracy': accuracy_score(y_test, lasso_clf.predict(X_test)),\n",
    "    'non_zero_coef': np.count_nonzero(lasso_clf.coef_),\n",
    "}\n",
    "\n",
    "print('Resultados Lasso:')\n",
    "for k, v in lasso_metrics.items():\n",
    "    print(f'  {k}: {v:.4f}' if isinstance(v, float) else f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4604598",
   "metadata": {},
   "source": [
    "## 6. Modelo 2: Logistic Regression con Elastic Net\n",
    "El modelo Elastic Net combina penalizaciones L1 y L2. Evaluamos una rejilla de `l1_ratio` para equilibrar sparsity y estabilidad, de nuevo optimizando AUC-ROC con validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75812394",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_clf = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    l1_ratios=np.linspace(0.1, 0.9, 9),\n",
    "    scoring='roc_auc',\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "elastic_train_proba = elastic_clf.predict_proba(X_train)[:, 1]\n",
    "elastic_test_proba = elastic_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "elastic_scores = elastic_clf.scores_[1].mean(axis=0)\n",
    "best_c_idx = np.where(elastic_clf.Cs_ == elastic_clf.C_[0])[0][0]\n",
    "best_l1_idx = np.where(elastic_clf.l1_ratios_ == elastic_clf.l1_ratio_[0])[0][0]\n",
    "\n",
    "elastic_metrics = {\n",
    "    'cv_mean_auc': elastic_scores[best_c_idx, best_l1_idx],\n",
    "    'train_auc': roc_auc_score(y_train, elastic_train_proba),\n",
    "    'test_auc': roc_auc_score(y_test, elastic_test_proba),\n",
    "    'train_accuracy': accuracy_score(y_train, elastic_clf.predict(X_train)),\n",
    "    'test_accuracy': accuracy_score(y_test, elastic_clf.predict(X_test)),\n",
    "    'non_zero_coef': np.count_nonzero(elastic_clf.coef_),\n",
    "    'best_l1_ratio': elastic_clf.l1_ratio_[0],\n",
    "}\n",
    "\n",
    "print('Resultados Elastic Net:')\n",
    "for k, v in elastic_metrics.items():\n",
    "    print(f'  {k}: {v:.4f}' if isinstance(v, float) else f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fbd2a",
   "metadata": {},
   "source": [
    "## 7. Seleccion de Features con SelectKBest\n",
    "Probamos distintos valores de `k` con una prueba univariada ANOVA (`f_classif`). Para cada `k`, ajustamos una regresión logística, medimos el AUC-ROC medio en validación cruzada y seleccionamos la mejor combinación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88933baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = min(50, X_train.shape[1])\n",
    "k_grid = range(1, max_k + 1)\n",
    "\n",
    "cv_auc_per_k = []\n",
    "base_logreg = LogisticRegression(max_iter=5000, solver='lbfgs')\n",
    "\n",
    "for k in k_grid:\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_k = selector.fit_transform(X_train, y_train)\n",
    "    scores = cross_val_score(\n",
    "        base_logreg,\n",
    "        X_train_k,\n",
    "        y_train,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    cv_auc_per_k.append(scores.mean())\n",
    "\n",
    "best_idx = int(np.argmax(cv_auc_per_k))\n",
    "best_k = list(k_grid)[best_idx]\n",
    "\n",
    "print(f'Mejor k por validación cruzada: {best_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c68a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc_df = pd.DataFrame({'k': list(k_grid), 'cv_auc': cv_auc_per_k})\n",
    "display(cv_auc_df.head(10))\n",
    "display(cv_auc_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_selector = SelectKBest(score_func=f_classif, k=best_k)\n",
    "X_train_best = best_selector.fit_transform(X_train, y_train)\n",
    "X_test_best = best_selector.transform(X_test)\n",
    "\n",
    "selected_feature_names = selected_features[best_selector.get_support()]\n",
    "\n",
    "fs_clf = LogisticRegression(max_iter=5000, solver='lbfgs').fit(X_train_best, y_train)\n",
    "\n",
    "fs_train_proba = fs_clf.predict_proba(X_train_best)[:, 1]\n",
    "fs_test_proba = fs_clf.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "feature_selection_metrics = {\n",
    "    'cv_mean_auc': cv_auc_per_k[best_idx],\n",
    "    'train_auc': roc_auc_score(y_train, fs_train_proba),\n",
    "    'test_auc': roc_auc_score(y_test, fs_test_proba),\n",
    "    'train_accuracy': accuracy_score(y_train, fs_clf.predict(X_train_best)),\n",
    "    'test_accuracy': accuracy_score(y_test, fs_clf.predict(X_test_best)),\n",
    "    'selected_features': selected_feature_names.size,\n",
    "}\n",
    "\n",
    "print('Resultados Logistic Regression + SelectKBest:')\n",
    "for k, v in feature_selection_metrics.items():\n",
    "    print(f'  {k}: {v:.4f}' if isinstance(v, float) else f'  {k}: {v}')\n",
    "\n",
    "print('\\nFeatures retenidas:')\n",
    "print(list(selected_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07714fb",
   "metadata": {},
   "source": [
    "## 8. Comparativa y Siguientes Pasos\n",
    "Resumimos los resultados clave para contrastar regularizaciones y selección de variables. Este cuadro facilita identificar el compromiso entre desempeño, estabilidad y complejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'modelo': 'Logistic L1',\n",
    "        'cv_mean_auc': lasso_metrics['cv_mean_auc'],\n",
    "        'test_auc': lasso_metrics['test_auc'],\n",
    "        'test_accuracy': lasso_metrics['test_accuracy'],\n",
    "        'features_activas': lasso_metrics['non_zero_coef'],\n",
    "    },\n",
    "    {\n",
    "        'modelo': 'Logistic Elastic Net',\n",
    "        'cv_mean_auc': elastic_metrics['cv_mean_auc'],\n",
    "        'test_auc': elastic_metrics['test_auc'],\n",
    "        'test_accuracy': elastic_metrics['test_accuracy'],\n",
    "        'features_activas': elastic_metrics['non_zero_coef'],\n",
    "    },\n",
    "    {\n",
    "        'modelo': f'Logistic + SelectKBest (k={best_k})',\n",
    "        'cv_mean_auc': feature_selection_metrics['cv_mean_auc'],\n",
    "        'test_auc': feature_selection_metrics['test_auc'],\n",
    "        'test_accuracy': feature_selection_metrics['test_accuracy'],\n",
    "        'features_activas': feature_selection_metrics['selected_features'],\n",
    "    },\n",
    "])\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9af03",
   "metadata": {},
   "source": [
    "### Ideas para profundizar\n",
    "- Explorar `Pipeline` o `ColumnTransformer` para encapsular preprocesos y reducir fugas.\n",
    "- Revisar métricas adicionales (precisión, recall, matriz de confusión) según los requisitos del dominio.\n",
    "- Probar otras técnicas de selección o regularización (ej. `RecursiveFeatureElimination`, `XGBoost`)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
