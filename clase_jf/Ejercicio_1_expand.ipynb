{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ec60b7",
   "metadata": {},
   "source": [
    "# CHEAT SHEET - SelecciÃ³n de Variables y ValidaciÃ³n de Modelos\n",
    "\n",
    "## ðŸ“š ÃNDICE\n",
    "\n",
    "### 1. ConfiguraciÃ³n Inicial\n",
    "- [1.1. LibrerÃ­as Necesarias](#11-librerÃ­as-necesarias)\n",
    "- [1.2. Limpieza de Entorno](#12-limpieza-de-entorno)\n",
    "- [1.3. Carga de Datos](#13-carga-de-datos)\n",
    "\n",
    "### 2. PreparaciÃ³n de Datos\n",
    "- [2.1. SeparaciÃ³n X e Y](#21-separaciÃ³n-x-e-y)\n",
    "- [2.2. Train-Test Split](#22-train-test-split)\n",
    "\n",
    "### 3. Modelo Lineal Completo\n",
    "- [3.1. Ajuste del Modelo](#31-ajuste-del-modelo)\n",
    "- [3.2. EvaluaciÃ³n Train vs Test](#32-evaluaciÃ³n-train-vs-test)\n",
    "\n",
    "### 4. SelecciÃ³n Secuencial de Variables\n",
    "- [4.1. Forward Selection](#41-forward-selection)\n",
    "- [4.2. VisualizaciÃ³n de Errores](#42-visualizaciÃ³n-de-errores)\n",
    "\n",
    "### 5. Criterios de InformaciÃ³n\n",
    "- [5.1. AIC (Akaike Information Criterion)](#51-aic-akaike-information-criterion)\n",
    "- [5.2. BIC (Bayesian Information Criterion)](#52-bic-bayesian-information-criterion)\n",
    "\n",
    "### 6. ValidaciÃ³n Cruzada (Cross-Validation)\n",
    "- [6.1. K-Fold Cross-Validation](#61-k-fold-cross-validation)\n",
    "- [6.2. SelecciÃ³n del Modelo Ã“ptimo](#62-selecciÃ³n-del-modelo-Ã³ptimo)\n",
    "\n",
    "### 7. Tips y Mejores PrÃ¡cticas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be348db9",
   "metadata": {},
   "source": [
    "## 1. CONFIGURACIÃ“N INICIAL\n",
    "\n",
    "### 1.1. LibrerÃ­as Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LIBRERÃAS ESENCIALES PARA SELECCIÃ“N DE MODELOS\n",
    "# ============================================\n",
    "\n",
    "import os                          # Manejo de directorios y archivos\n",
    "import pandas as pd                # ManipulaciÃ³n de datos tabulares\n",
    "import numpy as np                 # Operaciones numÃ©ricas y arrays\n",
    "import random                      # GeneraciÃ³n de nÃºmeros aleatorios\n",
    "import pylab as pl                 # VisualizaciÃ³n (wrapper de matplotlib)\n",
    "\n",
    "# Statsmodels: modelos estadÃ­sticos clÃ¡sicos (OLS, AIC, BIC)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "# Scikit-learn: machine learning y validaciÃ³n\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"âœ… LibrerÃ­as cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf019",
   "metadata": {},
   "source": [
    "### 1.2. Limpieza de Entorno\n",
    "\n",
    "**PropÃ³sito:** Asegurar un entorno limpio sin variables previas que puedan interferir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb99b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LIMPIEZA DE ENTORNO (OPCIONAL EN JUPYTER)\n",
    "# ============================================\n",
    "# Ãštil cuando trabajas en sesiones largas o reutilizas notebooks\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# %reset -f: elimina todas las variables del workspace\n",
    "# %clear: limpia el output de celdas anteriores\n",
    "get_ipython().run_line_magic('reset', '-f')\n",
    "get_ipython().run_line_magic('clear', '-f')\n",
    "\n",
    "print(\"âœ… Entorno limpio y listo\")\n",
    "\n",
    "# ALTERNATIVA sin IPython (para scripts):\n",
    "# No es necesario limpiar, Python gestiona memoria automÃ¡ticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4139b15",
   "metadata": {},
   "source": [
    "### 1.3. Carga de Datos\n",
    "\n",
    "**Dataset:** `data_mvn.txt` - Datos de distribuciÃ³n multivariante normal  \n",
    "**Formato:** Delimitado por tabulaciones (`\\t`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b34efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DE DATOS\n",
    "# ============================================\n",
    "\n",
    "# Verificar directorio de trabajo actual\n",
    "print(f\"Directorio actual: {os.getcwd()}\")\n",
    "\n",
    "# Cargar datos desde archivo delimitado por tabulaciones\n",
    "df = pd.read_csv('data_mvn.txt', delimiter='\\t')\n",
    "\n",
    "# ExploraciÃ³n inicial del dataset\n",
    "print(f\"\\nShape del dataset: {df.shape}\")  # (filas, columnas)\n",
    "print(f\"\\nColumnas: {df.columns.tolist()}\")\n",
    "print(f\"\\nPrimeras filas:\\n{df.head()}\")\n",
    "print(f\"\\nInformaciÃ³n del dataset:\")\n",
    "print(df.info())\n",
    "print(f\"\\nEstadÃ­sticas descriptivas:\\n{df.describe()}\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(f\"\\nValores nulos por columna:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabd7a6",
   "metadata": {},
   "source": [
    "## 2. PREPARACIÃ“N DE DATOS\n",
    "\n",
    "### 2.1. SeparaciÃ³n X e Y\n",
    "\n",
    "**Concepto clave:**\n",
    "- **X (Features):** Variables predictoras/independientes\n",
    "- **Y (Target):** Variable objetivo/dependiente a predecir\n",
    "- **Intercepto:** TÃ©rmino constante Î²â‚€ en la regresiÃ³n Y = Î²â‚€ + Î²â‚Xâ‚ + ... + Î²â‚šXâ‚š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2277e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARACIÃ“N DE VARIABLES X e Y\n",
    "# ============================================\n",
    "\n",
    "# Separar features (X) de target (Y)\n",
    "X = df.drop(columns=['Y'], axis=1).values  # Todas las columnas excepto 'Y'\n",
    "Y = df['Y'].values                          # Solo columna 'Y'\n",
    "\n",
    "# Dimensiones del problema\n",
    "n, p_original = df.shape  # n = observaciones, p = variables totales\n",
    "p = p_original - 1        # p = nÃºmero de predictores (sin contar Y)\n",
    "\n",
    "print(f\"NÃºmero de observaciones (n): {n}\")\n",
    "print(f\"NÃºmero de predictores (p): {p}\")\n",
    "print(f\"Shape de X: {X.shape}\")  # (n, p)\n",
    "print(f\"Shape de Y: {Y.shape}\")  # (n,)\n",
    "\n",
    "# ============================================\n",
    "# AÃ‘ADIR INTERCEPTO (TÃ‰RMINO CONSTANTE)\n",
    "# ============================================\n",
    "# statsmodels NO aÃ±ade intercepto automÃ¡ticamente (sklearn SÃ)\n",
    "# sm.add_constant() aÃ±ade columna de unos al inicio\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "print(f\"\\nShape de X con intercepto: {X.shape}\")  # (n, p+1)\n",
    "\n",
    "# Ahora X tiene estructura: [1, X1, X2, ..., Xp]\n",
    "# donde la primera columna son unos (intercepto)\n",
    "\n",
    "print(f\"\\nPrimeras filas de X (con intercepto):\\n{X[:3, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4cee7",
   "metadata": {},
   "source": [
    "### 2.2. Train-Test Split\n",
    "\n",
    "**DivisiÃ³n estratÃ©gica de datos:**\n",
    "- **Training set (50%):** Para entrenar/ajustar el modelo\n",
    "- **Test set (50%):** Para evaluar rendimiento en datos no vistos\n",
    "- **Random seed:** Garantiza reproducibilidad de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIVISIÃ“N TRAIN-TEST\n",
    "# ============================================\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "# Cualquier ejecuciÃ³n con seed=100 darÃ¡ mismos resultados\n",
    "random.seed(100)\n",
    "np.random.seed(100)  # TambiÃ©n fijar para numpy\n",
    "\n",
    "# DivisiÃ³n 50-50 (puede ajustarse segÃºn necesidad)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.5,      # 50% para test\n",
    "    random_state=100    # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "print(f\"TamaÃ±o train: {X_train.shape[0]} observaciones\")\n",
    "print(f\"TamaÃ±o test:  {X_test.shape[0]} observaciones\")\n",
    "print(f\"\\nProporciÃ³n train/test: {X_train.shape[0]/n:.1%} / {X_test.shape[0]/n:.1%}\")\n",
    "\n",
    "# IMPORTANTE: \n",
    "# - Entrenar SOLO con datos de train\n",
    "# - Evaluar rendimiento final con datos de test\n",
    "# - Nunca usar test durante entrenamiento (data leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1074a49",
   "metadata": {},
   "source": [
    "## 3. MODELO LINEAL COMPLETO\n",
    "\n",
    "### 3.1. Ajuste del Modelo\n",
    "\n",
    "**Modelo:** Y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚šXâ‚š + Îµ  \n",
    "**MÃ©todo:** OLS (Ordinary Least Squares) - MÃ­nimos Cuadrados Ordinarios  \n",
    "**Objetivo:** Minimizar Î£(Y - Å¶)Â²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a6634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODELO LINEAL CON TODAS LAS VARIABLES\n",
    "# ============================================\n",
    "\n",
    "# Ajustar modelo OLS con statsmodels\n",
    "model_fit = sm.OLS(Y_train, X_train).fit()\n",
    "\n",
    "# Predicciones en conjunto de entrenamiento\n",
    "ypred_train = model_fit.predict(X_train)\n",
    "\n",
    "# Calcular RMSE (Root Mean Squared Error) en train\n",
    "rmse_train = rmse(Y_train, ypred_train)\n",
    "print(f\"RMSE Training: {rmse_train:.4f}\")\n",
    "\n",
    "# Ver resumen completo del modelo\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DEL MODELO COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "print(model_fit.summary())\n",
    "\n",
    "# InformaciÃ³n clave del resumen:\n",
    "# - R-squared: proporciÃ³n de varianza explicada (0-1, mayor es mejor)\n",
    "# - Adj. R-squared: RÂ² ajustado por nÃºmero de variables\n",
    "# - F-statistic: significancia global del modelo\n",
    "# - Coef: valores de los coeficientes Î²\n",
    "# - P>|t|: p-valores (< 0.05 indica significancia)\n",
    "# - AIC/BIC: criterios de informaciÃ³n (menor es mejor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6237839",
   "metadata": {},
   "source": [
    "### 3.2. EvaluaciÃ³n Train vs Test\n",
    "\n",
    "**Conceptos fundamentales:**\n",
    "- **Overfitting:** Modelo aprende ruido del train â†’ mal rendimiento en test\n",
    "- **Underfitting:** Modelo demasiado simple â†’ mal rendimiento en train y test\n",
    "- **Balance ideal:** RMSE similar en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163792a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUACIÃ“N EN TEST SET\n",
    "# ============================================\n",
    "\n",
    "# Predicciones en conjunto de test (datos NO vistos durante entrenamiento)\n",
    "ypred_test = model_fit.predict(X_test)\n",
    "\n",
    "# Calcular RMSE en test\n",
    "rmse_test = rmse(Y_test, ypred_test)\n",
    "\n",
    "# Comparar rendimiento train vs test\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACIÃ“N TRAIN vs TEST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"RMSE Training: {rmse_train:.4f}\")\n",
    "print(f\"RMSE Test:     {rmse_test:.4f}\")\n",
    "print(f\"Diferencia:    {abs(rmse_test - rmse_train):.4f}\")\n",
    "print(f\"Ratio Test/Train: {rmse_test/rmse_train:.2f}\")\n",
    "\n",
    "# INTERPRETACIÃ“N:\n",
    "if rmse_test > rmse_train * 1.2:\n",
    "    print(\"\\nâš ï¸  OVERFITTING: RMSE test >> RMSE train\")\n",
    "    print(\"   â†’ Modelo se ajusta demasiado a datos de entrenamiento\")\n",
    "    print(\"   â†’ Considerar regularizaciÃ³n o reducir variables\")\n",
    "elif rmse_test < rmse_train * 0.8:\n",
    "    print(\"\\nâš ï¸  INUSUAL: RMSE test < RMSE train\")\n",
    "    print(\"   â†’ Puede indicar datos de test mÃ¡s fÃ¡ciles de predecir\")\n",
    "else:\n",
    "    print(\"\\nâœ… BALANCE ADECUADO: RMSE similar en train y test\")\n",
    "    print(\"   â†’ Modelo generaliza bien a datos nuevos\")\n",
    "\n",
    "# VisualizaciÃ³n de predicciones\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# GrÃ¡fico Train\n",
    "ax1.scatter(Y_train, ypred_train, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "ax1.plot([Y_train.min(), Y_train.max()], [Y_train.min(), Y_train.max()], \n",
    "         'r--', lw=2, label='PredicciÃ³n Perfecta')\n",
    "ax1.set_xlabel('Y Real (Train)')\n",
    "ax1.set_ylabel('Y Predicho')\n",
    "ax1.set_title(f'Train Set (RMSE={rmse_train:.4f})')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GrÃ¡fico Test\n",
    "ax2.scatter(Y_test, ypred_test, alpha=0.6, edgecolors='k', linewidths=0.5, color='orange')\n",
    "ax2.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], \n",
    "         'r--', lw=2, label='PredicciÃ³n Perfecta')\n",
    "ax2.set_xlabel('Y Real (Test)')\n",
    "ax2.set_ylabel('Y Predicho')\n",
    "ax2.set_title(f'Test Set (RMSE={rmse_test:.4f})')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7cbad",
   "metadata": {},
   "source": [
    "## 4. SELECCIÃ“N SECUENCIAL DE VARIABLES\n",
    "\n",
    "### 4.1. Forward Selection\n",
    "\n",
    "**Estrategia:** AÃ±adir variables una por una en orden secuencial  \n",
    "**IteraciÃ³n k:** Usar primeras k variables (intercepto + Xâ‚ + Xâ‚‚ + ... + Xâ‚–)  \n",
    "**Objetivo:** Identificar cuÃ¡ntas variables son necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FORWARD SELECTION: MODELOS SECUENCIALES\n",
    "# ============================================\n",
    "\n",
    "# Inicializar arrays para almacenar RMSE\n",
    "rmse_train_array = np.zeros(p+1)  # +1 para incluir modelo solo con intercepto\n",
    "rmse_test_array = np.zeros(p+1)\n",
    "length = range(0, p+1)  # NÃºmero de variables: 0, 1, 2, ..., p\n",
    "\n",
    "print(\"Ajustando modelos con k=1, 2, ..., p variables...\\n\")\n",
    "\n",
    "# Iterar sobre nÃºmero de variables\n",
    "k = 1\n",
    "while k <= (p+1):\n",
    "    # Seleccionar primeras k columnas (intercepto + k-1 variables)\n",
    "    Xk_train = X_train[:, :k]\n",
    "    Xk_test = X_test[:, :k]\n",
    "    \n",
    "    # Ajustar modelo con k variables\n",
    "    model_L = sm.OLS(Y_train, Xk_train)\n",
    "    model_fit = model_L.fit()\n",
    "    \n",
    "    # Predicciones y RMSE en train\n",
    "    ypred_train = model_fit.predict(Xk_train)\n",
    "    rmse_train_array[k-1] = rmse(Y_train, ypred_train)\n",
    "    \n",
    "    # Predicciones y RMSE en test\n",
    "    ypred_test = model_fit.predict(Xk_test)\n",
    "    rmse_test_array[k-1] = rmse(Y_test, ypred_test)\n",
    "    \n",
    "    # Mostrar progreso cada 5 iteraciones\n",
    "    if k % 5 == 0 or k <= 3:\n",
    "        print(f\"k={k:2d} variables: RMSE_train={rmse_train_array[k-1]:.4f}, \"\n",
    "              f\"RMSE_test={rmse_test_array[k-1]:.4f}\")\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "print(\"\\nâœ… Forward selection completado\")\n",
    "\n",
    "# Encontrar mejor k segÃºn test error\n",
    "best_k = np.argmin(rmse_test_array) + 1\n",
    "print(f\"\\nMejor nÃºmero de variables segÃºn test error: k={best_k}\")\n",
    "print(f\"RMSE test mÃ­nimo: {rmse_test_array[best_k-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a1c64",
   "metadata": {},
   "source": [
    "### 4.2. VisualizaciÃ³n de Errores\n",
    "\n",
    "**Curva de aprendizaje:**\n",
    "- **Train error:** Siempre decrece (mÃ¡s variables = mejor ajuste)\n",
    "- **Test error:** Forma de U (Ã³ptimo en el mÃ­nimo)\n",
    "- **Gap train-test:** Indica overfitting si crece mucho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÃ“N: CURVAS DE APRENDIZAJE\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot de errores\n",
    "plt.plot(length, rmse_train_array, 'b-o', label='Training Error', linewidth=2, markersize=6)\n",
    "plt.plot(length, rmse_test_array, 'r-s', label='Test Error', linewidth=2, markersize=6)\n",
    "\n",
    "# Marcar punto Ã³ptimo\n",
    "plt.axvline(x=best_k, color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Ã“ptimo (k={best_k})')\n",
    "plt.scatter([best_k], [rmse_test_array[best_k-1]], \n",
    "           color='green', s=200, zorder=5, marker='*')\n",
    "\n",
    "plt.xlabel('NÃºmero de Variables (k)', fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=12)\n",
    "plt.title('Forward Selection: Train vs Test Error', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# INTERPRETACIÃ“N:\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACIÃ“N DE LA CURVA\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“‰ TRAIN ERROR (azul):\")\n",
    "print(\"   - Decrece monotÃ³nicamente al aÃ±adir variables\")\n",
    "print(\"   - MÃ¡s variables â†’ mejor ajuste a datos de entrenamiento\")\n",
    "print(\"   - NO es buen criterio para selecciÃ³n (puede overfit)\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ TEST ERROR (rojo):\")\n",
    "print(\"   - Forma de U: decrece y luego aumenta\")\n",
    "print(\"   - MÃ­nimo indica nÃºmero Ã³ptimo de variables\")\n",
    "print(\"   - DespuÃ©s del mÃ­nimo: overfitting\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ PUNTO Ã“PTIMO (verde):\")\n",
    "print(f\"   - k={best_k} variables minimiza error en test\")\n",
    "print(f\"   - Balance entre bias (underfitting) y variance (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b04b0b",
   "metadata": {},
   "source": [
    "## 5. CRITERIOS DE INFORMACIÃ“N\n",
    "\n",
    "### 5.1. AIC (Akaike Information Criterion)\n",
    "\n",
    "**FÃ³rmula:** AIC = 2k - 2ln(L)  \n",
    "**Donde:**\n",
    "- k = nÃºmero de parÃ¡metros\n",
    "- L = likelihood (verosimilitud del modelo)\n",
    "\n",
    "**InterpretaciÃ³n:** Menor AIC = mejor modelo  \n",
    "**PenalizaciÃ³n:** Leve por nÃºmero de parÃ¡metros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9142d3",
   "metadata": {},
   "source": [
    "### 5.2. BIC (Bayesian Information Criterion)\n",
    "\n",
    "**FÃ³rmula:** BIC = kÂ·ln(n) - 2ln(L)  \n",
    "**Donde:**\n",
    "- k = nÃºmero de parÃ¡metros\n",
    "- n = nÃºmero de observaciones\n",
    "- L = likelihood\n",
    "\n",
    "**InterpretaciÃ³n:** Menor BIC = mejor modelo  \n",
    "**PenalizaciÃ³n:** MÃ¡s fuerte que AIC (prefiere modelos mÃ¡s simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3041f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CRITERIOS DE INFORMACIÃ“N: AIC y BIC\n",
    "# ============================================\n",
    "\n",
    "# Inicializar arrays\n",
    "AIC = np.zeros(p+1)\n",
    "BIC = np.zeros(p+1)\n",
    "\n",
    "print(\"Calculando AIC y BIC para k=1, 2, ..., p variables...\\n\")\n",
    "\n",
    "# IMPORTANTE: Usar TODOS los datos (X, Y) no solo train\n",
    "# AIC/BIC son criterios para selecciÃ³n de modelo, no para evaluaciÃ³n\n",
    "k = 1\n",
    "while k <= (p+1):\n",
    "    # Seleccionar primeras k variables\n",
    "    Xk = X[:, :k]\n",
    "    \n",
    "    # Ajustar modelo\n",
    "    model_L = sm.OLS(Y, Xk)\n",
    "    model_fit = model_L.fit()\n",
    "    \n",
    "    # Extraer AIC y BIC\n",
    "    AIC[k-1] = model_fit.aic\n",
    "    BIC[k-1] = model_fit.bic\n",
    "    \n",
    "    if k % 5 == 0 or k <= 3:\n",
    "        print(f\"k={k:2d}: AIC={AIC[k-1]:.2f}, BIC={BIC[k-1]:.2f}\")\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "# Encontrar Ã³ptimos\n",
    "best_k_aic = np.argmin(AIC) + 1\n",
    "best_k_bic = np.argmin(BIC) + 1\n",
    "\n",
    "print(f\"\\nâœ… Mejor k segÃºn AIC: {best_k_aic} (AIC={AIC[best_k_aic-1]:.2f})\")\n",
    "print(f\"âœ… Mejor k segÃºn BIC: {best_k_bic} (BIC={BIC[best_k_bic-1]:.2f})\")\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(length, AIC, 'b-o', label='AIC', linewidth=2, markersize=6)\n",
    "plt.plot(length, BIC, 'r-s', label='BIC', linewidth=2, markersize=6)\n",
    "\n",
    "# Marcar Ã³ptimos\n",
    "plt.axvline(x=best_k_aic, color='blue', linestyle='--', alpha=0.5, \n",
    "           label=f'Ã“ptimo AIC (k={best_k_aic})')\n",
    "plt.axvline(x=best_k_bic, color='red', linestyle='--', alpha=0.5, \n",
    "           label=f'Ã“ptimo BIC (k={best_k_bic})')\n",
    "\n",
    "plt.xlabel('NÃºmero de Variables (k)', fontsize=12)\n",
    "plt.ylabel('Valor del Criterio', fontsize=12)\n",
    "plt.title('Criterios de InformaciÃ³n: AIC vs BIC', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# COMPARACIÃ“N\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACIÃ“N AIC vs BIC\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“Š AIC (Akaike Information Criterion):\")\n",
    "print(f\"   - Ã“ptimo: k={best_k_aic} variables\")\n",
    "print(\"   - PenalizaciÃ³n leve por complejidad\")\n",
    "print(\"   - Prefiere modelos mÃ¡s complejos que BIC\")\n",
    "print(\"   - Mejor para predicciÃ³n\")\n",
    "\n",
    "print(\"\\nðŸ“Š BIC (Bayesian Information Criterion):\")\n",
    "print(f\"   - Ã“ptimo: k={best_k_bic} variables\")\n",
    "print(\"   - PenalizaciÃ³n fuerte por complejidad\")\n",
    "print(\"   - Prefiere modelos mÃ¡s simples (parsimonia)\")\n",
    "print(\"   - Mejor para interpretaciÃ³n\")\n",
    "\n",
    "if best_k_bic < best_k_aic:\n",
    "    print(\"\\nðŸ’¡ BIC selecciona modelo mÃ¡s simple (usual)\")\n",
    "elif best_k_bic > best_k_aic:\n",
    "    print(\"\\nðŸ’¡ AIC selecciona modelo mÃ¡s simple (inusual)\")\n",
    "else:\n",
    "    print(\"\\nðŸ’¡ AIC y BIC coinciden en el modelo Ã³ptimo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d74a81",
   "metadata": {},
   "source": [
    "## 6. VALIDACIÃ“N CRUZADA (CROSS-VALIDATION)\n",
    "\n",
    "### 6.1. K-Fold Cross-Validation\n",
    "\n",
    "**Concepto:** Dividir datos en K folds (subconjuntos)  \n",
    "**Proceso:**\n",
    "1. Entrenar con K-1 folds\n",
    "2. Validar con el fold restante\n",
    "3. Repetir K veces (cada fold como validaciÃ³n una vez)\n",
    "4. Promediar resultados\n",
    "\n",
    "**Ventajas:**\n",
    "- Usa todos los datos para train y validaciÃ³n\n",
    "- EstimaciÃ³n mÃ¡s robusta del error\n",
    "- Reduce varianza de la estimaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CROSS-VALIDATION: 5-FOLD CV\n",
    "# ============================================\n",
    "\n",
    "# Inicializar\n",
    "CV = np.zeros(p+1)\n",
    "lm = LinearRegression()  # Usar sklearn para compatibilidad con cross_val_score\n",
    "\n",
    "print(\"Ejecutando 5-Fold Cross-Validation...\\n\")\n",
    "print(\"Esto puede tardar un momento...\\n\")\n",
    "\n",
    "k = 1\n",
    "while k <= (p+1):\n",
    "    # Seleccionar primeras k variables\n",
    "    Xk = X_train[:, :k]\n",
    "    \n",
    "    # Ejecutar cross-validation con 5 folds\n",
    "    # scoring='neg_root_mean_squared_error': devuelve -RMSE (negativo para maximizar)\n",
    "    scores = -cross_val_score(\n",
    "        lm, Xk, Y_train,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,              # 5 folds\n",
    "        n_jobs=-1          # paralelizar\n",
    "    )\n",
    "    \n",
    "    # Promedio de RMSE en los 5 folds\n",
    "    CV[k-1] = np.mean(scores)\n",
    "    \n",
    "    if k % 5 == 0 or k <= 3:\n",
    "        print(f\"k={k:2d}: CV-RMSE={CV[k-1]:.4f} (std={np.std(scores):.4f})\")\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "print(\"\\nâœ… Cross-validation completado\")\n",
    "\n",
    "# Encontrar mejor k segÃºn CV\n",
    "best_k_cv = np.argmin(CV) + 1\n",
    "print(f\"\\nMejor k segÃºn Cross-Validation: {best_k_cv}\")\n",
    "print(f\"CV-RMSE mÃ­nimo: {CV[best_k_cv-1]:.4f}\")\n",
    "\n",
    "# VisualizaciÃ³n comparativa\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(length, rmse_train_array, 'b-o', label='Training Error', \n",
    "         linewidth=2, markersize=6, alpha=0.7)\n",
    "plt.plot(length, rmse_test_array, 'r-s', label='Test Error', \n",
    "         linewidth=2, markersize=6, alpha=0.7)\n",
    "plt.plot(length, CV, 'g-^', label='CV Error (5-Fold)', \n",
    "         linewidth=2, markersize=6)\n",
    "\n",
    "# Marcar Ã³ptimo CV\n",
    "plt.axvline(x=best_k_cv, color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Ã“ptimo CV (k={best_k_cv})')\n",
    "plt.scatter([best_k_cv], [CV[best_k_cv-1]], \n",
    "           color='green', s=200, zorder=5, marker='*')\n",
    "\n",
    "plt.xlabel('NÃºmero de Variables (k)', fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=12)\n",
    "plt.title('ComparaciÃ³n: Train, Test y Cross-Validation Error', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACIÃ“N DE CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… VENTAJAS:\")\n",
    "print(\"   - Usa todos los datos de train eficientemente\")\n",
    "print(\"   - EstimaciÃ³n mÃ¡s robusta que single train-test split\")\n",
    "print(\"   - Reduce varianza de la estimaciÃ³n del error\")\n",
    "print(\"   - Mejor generalizaciÃ³n a datos nuevos\")\n",
    "\n",
    "print(\"\\nðŸ’¡ CV Error suele estar entre Train y Test Error\")\n",
    "print(f\"   Train: {rmse_train_array[best_k_cv-1]:.4f}\")\n",
    "print(f\"   CV:    {CV[best_k_cv-1]:.4f}\")\n",
    "print(f\"   Test:  {rmse_test_array[best_k_cv-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156da2bf",
   "metadata": {},
   "source": [
    "### 6.2. SelecciÃ³n del Modelo Ã“ptimo\n",
    "\n",
    "**Modelo final:** Usar k Ã³ptimo determinado por CV  \n",
    "**Entrenamiento final:** Con todos los datos de train  \n",
    "**EvaluaciÃ³n final:** Con datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODELO FINAL CON K Ã“PTIMO\n",
    "# ============================================\n",
    "\n",
    "# Extraer k Ã³ptimo de CV\n",
    "min_value = min(CV)\n",
    "min_index = np.where(CV == min_value)\n",
    "K = np.ndarray.item(min_index[0]) + 1  # +1 porque Ã­ndices empiezan en 0\n",
    "\n",
    "print(f\"NÃºmero Ã³ptimo de variables: K = {K}\")\n",
    "print(f\"CV-RMSE Ã³ptimo: {min_value:.4f}\\n\")\n",
    "\n",
    "# Preparar datos con K variables\n",
    "X_train_k = X_train[:, :K]\n",
    "X_test_k = X_test[:, :K]\n",
    "\n",
    "# Entrenar modelo final con K variables\n",
    "model_L = sm.OLS(Y_train, X_train_k)\n",
    "model_fit_final = model_L.fit()\n",
    "\n",
    "# Predicciones\n",
    "ypred_train_final = model_fit_final.predict(X_train_k)\n",
    "ypred_test_final = model_fit_final.predict(X_test_k)\n",
    "\n",
    "# Calcular RMSE\n",
    "rmse_train_final = rmse(Y_train, ypred_train_final)\n",
    "rmse_test_final = rmse(Y_test, ypred_test_final)\n",
    "\n",
    "# Resultados finales\n",
    "print(\"=\"*70)\n",
    "print(\"RESULTADOS DEL MODELO FINAL OPTIMIZADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNÃºmero de variables seleccionadas: {K}\")\n",
    "print(f\"RMSE Training:   {rmse_train_final:.4f}\")\n",
    "print(f\"RMSE Test:       {rmse_test_final:.4f}\")\n",
    "print(f\"RMSE CV (5-fold): {CV[K-1]:.4f}\")\n",
    "\n",
    "# ComparaciÃ³n con modelo completo\n",
    "print(f\"\\nðŸ“Š COMPARACIÃ“N CON MODELO COMPLETO (p={p} variables):\")\n",
    "print(f\"   Modelo completo - RMSE Test: {rmse_test:.4f}\")\n",
    "print(f\"   Modelo Ã³ptimo   - RMSE Test: {rmse_test_final:.4f}\")\n",
    "print(f\"   Mejora: {(rmse_test - rmse_test_final)/rmse_test * 100:.2f}%\")\n",
    "print(f\"   ReducciÃ³n de variables: {p} â†’ {K} ({(p-K)/p*100:.1f}% menos)\")\n",
    "\n",
    "# Resumen del modelo\n",
    "print(f\"\\n{model_fit_final.summary()}\")\n",
    "\n",
    "# VisualizaciÃ³n final\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train\n",
    "axes[0].scatter(Y_train, ypred_train_final, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
    "axes[0].plot([Y_train.min(), Y_train.max()], [Y_train.min(), Y_train.max()], \n",
    "            'r--', lw=2, label='PredicciÃ³n Perfecta')\n",
    "axes[0].set_xlabel('Y Real (Train)', fontsize=11)\n",
    "axes[0].set_ylabel('Y Predicho', fontsize=11)\n",
    "axes[0].set_title(f'Modelo Final - Train (K={K}, RMSE={rmse_train_final:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test\n",
    "axes[1].scatter(Y_test, ypred_test_final, alpha=0.6, edgecolors='k', \n",
    "               linewidths=0.5, color='orange')\n",
    "axes[1].plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], \n",
    "            'r--', lw=2, label='PredicciÃ³n Perfecta')\n",
    "axes[1].set_xlabel('Y Real (Test)', fontsize=11)\n",
    "axes[1].set_ylabel('Y Predicho', fontsize=11)\n",
    "axes[1].set_title(f'Modelo Final - Test (K={K}, RMSE={rmse_test_final:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f588a",
   "metadata": {},
   "source": [
    "## 7. TIPS Y MEJORES PRÃCTICAS\n",
    "\n",
    "### ðŸ“Š Resumen de MÃ©todos de SelecciÃ³n\n",
    "\n",
    "| MÃ©todo | Ventajas | Desventajas | CuÃ¡ndo usar |\n",
    "|--------|----------|-------------|-------------|\n",
    "| **Test Error** | Simple, directo | Depende de split especÃ­fico | Cuando hay suficientes datos |\n",
    "| **AIC** | RÃ¡pido, matemÃ¡tico | Puede sobreajustar | PredicciÃ³n, muchos datos |\n",
    "| **BIC** | Parsimonia, robusto | Puede subajustar | InterpretaciÃ³n, explicaciÃ³n |\n",
    "| **Cross-Validation** | Robusto, confiable | Computacionalmente costoso | Datos limitados, mejor prÃ¡ctica |\n",
    "\n",
    "### âœ… HACER (Best Practices):\n",
    "\n",
    "1. **Siempre dividir datos:** Train-Test split ANTES de cualquier anÃ¡lisis\n",
    "2. **Nunca usar test en entrenamiento:** Test solo para evaluaciÃ³n final\n",
    "3. **Fijar random seed:** Para reproducibilidad (`random.seed(100)`)\n",
    "4. **Usar CV para selecciÃ³n:** MÃ¡s robusto que single split\n",
    "5. **AÃ±adir intercepto con statsmodels:** `sm.add_constant(X)`\n",
    "6. **Comparar mÃºltiples mÃ©todos:** AIC, BIC, CV pueden no coincidir\n",
    "7. **Visualizar curvas de aprendizaje:** Identificar overfitting/underfitting\n",
    "8. **Revisar significancia de variables:** P-valores en resumen del modelo\n",
    "9. **Verificar supuestos de regresiÃ³n:** Residuos normales, homocedasticidad\n",
    "10. **Documentar decisiones:** Justificar elecciÃ³n del modelo final\n",
    "\n",
    "### âŒ EVITAR (Common Mistakes):\n",
    "\n",
    "1. âŒ Usar test error para selecciÃ³n de modelo â†’ Data leakage\n",
    "2. âŒ No fijar seed â†’ Resultados no reproducibles\n",
    "3. âŒ Olvidar intercepto â†’ Modelo sesgado\n",
    "4. âŒ Confiar solo en RÂ² â†’ No indica generalizaciÃ³n\n",
    "5. âŒ Elegir modelo mÃ¡s complejo automÃ¡ticamente â†’ Overfitting\n",
    "6. âŒ Ignorar p-valores â†’ Variables no significativas\n",
    "7. âŒ No validar supuestos â†’ Conclusiones errÃ³neas\n",
    "8. âŒ Comparar RMSE de datasets diferentes â†’ No comparables\n",
    "\n",
    "### ðŸŽ¯ Criterios de DecisiÃ³n:\n",
    "\n",
    "**Si BIC y CV coinciden â†’ USAR ESE MODELO** (mÃ¡s confiable)\n",
    "\n",
    "**Si difieren:**\n",
    "- BIC < CV: Modelo mÃ¡s simple (mejor interpretaciÃ³n)\n",
    "- CV < BIC: Modelo mÃ¡s complejo (mejor predicciÃ³n)\n",
    "- DecisiÃ³n depende del objetivo del anÃ¡lisis\n",
    "\n",
    "**Objetivo PredicciÃ³n:** Usar CV (generalizaciÃ³n)\n",
    "\n",
    "**Objetivo InterpretaciÃ³n:** Usar BIC (parsimonia)\n",
    "\n",
    "### ðŸ” Checklist de ValidaciÃ³n:\n",
    "\n",
    "- [ ] Train-test split realizado correctamente\n",
    "- [ ] Seed fijado para reproducibilidad\n",
    "- [ ] Intercepto aÃ±adido si usa statsmodels\n",
    "- [ ] Forward selection ejecutado\n",
    "- [ ] AIC/BIC calculados\n",
    "- [ ] Cross-validation ejecutado\n",
    "- [ ] Modelo final entrenado con k Ã³ptimo\n",
    "- [ ] RMSE train, test y CV reportados\n",
    "- [ ] GrÃ¡ficos de predicciÃ³n vs real generados\n",
    "- [ ] Residuos verificados (opcional pero recomendado)\n",
    "\n",
    "### ðŸ“š Recursos Adicionales:\n",
    "\n",
    "**Statsmodels:** https://www.statsmodels.org/\n",
    "\n",
    "**Scikit-learn:** https://scikit-learn.org/\n",
    "\n",
    "**Cross-Validation:** https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "**Model Selection:** https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ CONCLUSIÃ“N\n",
    "\n",
    "Este notebook cubre el **flujo completo de selecciÃ³n de variables** en regresiÃ³n lineal:\n",
    "\n",
    "1. âœ… PreparaciÃ³n y split de datos\n",
    "2. âœ… Modelo completo como baseline\n",
    "3. âœ… Forward selection para explorar complejidad\n",
    "4. âœ… AIC/BIC para selecciÃ³n matemÃ¡tica\n",
    "5. âœ… Cross-validation para selecciÃ³n robusta\n",
    "6. âœ… Modelo final optimizado\n",
    "\n",
    "**Recuerda:** No existe un Ãºnico \"mejor\" mÃ©todo. La elecciÃ³n depende de:\n",
    "- TamaÃ±o del dataset\n",
    "- Objetivo (predicciÃ³n vs interpretaciÃ³n)\n",
    "- Recursos computacionales\n",
    "- Contexto del problema\n",
    "\n",
    "**Best Practice:** Usar mÃºltiples mÃ©todos y comparar resultados.\n",
    "\n",
    "---\n",
    "\n",
    "*Cheat sheet creado con fines educativos - Data Science Master*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
